{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-S5: A/B 테스트 기초\n",
    "\n",
    "FDS에서 새 모델을 안전하게 검증하는 A/B 테스트를 학습합니다.\n",
    "\n",
    "## 학습 목표\n",
    "1. A/B 테스트란? (일반 vs FDS)\n",
    "2. **Shadow 모드** (FDS 특수성)\n",
    "3. **통계적 유의성 (p-value)** (면접 필수!)\n",
    "4. Champion 승격 조건 4가지\n",
    "\n",
    "## 예상 시간\n",
    "- 약 1.5시간\n",
    "\n",
    "## 선수 조건\n",
    "- 2-S4 비용 최적화 + CI/CD 완료\n",
    "- 기본 통계 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"라이브러리 로드 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. A/B 테스트란?\n",
    "\n",
    "### 왜 A/B 테스트가 필요한가?\n",
    "\n",
    "```\n",
    "질문: 새 모델을 바로 배포하면 안 되나요?\n",
    "\n",
    "문제점:\n",
    "1. 테스트 데이터에서 좋았어도 실제 트래픽에선 다를 수 있음\n",
    "2. 전체 사용자에게 한 번에 적용 -> 리스크 큼\n",
    "3. 문제 발생 시 원인 파악 어려움\n",
    "\n",
    "해결책: A/B 테스트\n",
    "-> 일부 트래픽에만 새 모델 적용\n",
    "-> 성능 비교 후 결정\n",
    "```\n",
    "\n",
    "### 일반 웹서비스 A/B 테스트\n",
    "\n",
    "```\n",
    "일반 웹서비스 (버튼 색상 테스트)\n",
    "\n",
    "      사용자 요청\n",
    "          |\n",
    "    +-----+-----+\n",
    "    |  50%    50%|\n",
    "    v           v\n",
    " [버전 A]    [버전 B]\n",
    " 파란 버튼   빨간 버튼\n",
    "    |           |\n",
    "    v           v\n",
    " 응답 반환   응답 반환\n",
    "\n",
    "-> 클릭률 비교: A=3.2%, B=4.1% -> B 채택!\n",
    "```\n",
    "\n",
    "### FDS A/B 테스트의 특수성\n",
    "\n",
    "```\n",
    "FDS에서 일반 A/B는 위험!\n",
    "\n",
    "문제: 새 모델이 사기를 놓치면?\n",
    "-> 실제 금전 피해 발생!\n",
    "-> 웹 버튼 색상과 차원이 다른 리스크\n",
    "\n",
    "해결: Shadow 모드 A/B 테스트\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Shadow 모드 (FDS 핵심!)\n",
    "\n",
    "### Shadow 모드 구조\n",
    "\n",
    "```\n",
    "Shadow 모드 A/B 테스트\n",
    "\n",
    "      거래 요청\n",
    "          |\n",
    "    +-----+-----------+\n",
    "    |                 |\n",
    "    v                 v\n",
    " [Champion]       [Challenger]\n",
    " 현재 모델         새 모델\n",
    "    |                 |\n",
    "    v                 v\n",
    " * 실제 응답 *      로깅만!\n",
    " (차단/승인)       (응답 안 함)\n",
    "\n",
    "-> Challenger는 \"그림자\"처럼 따라다니며 예측만 기록\n",
    "-> 실제 거래에 영향 없음 (리스크 제로)\n",
    "-> 나중에 로그 분석으로 성능 비교\n",
    "```\n",
    "\n",
    "### Shadow 모드 장점\n",
    "\n",
    "| 특성 | 일반 A/B | Shadow 모드 |\n",
    "|------|---------|-------------|\n",
    "| 실제 응답 | 두 모델 모두 | Champion만 |\n",
    "| 리스크 | 있음 | **없음** |\n",
    "| 트래픽 분할 | 필요 | 불필요 (100% 동시) |\n",
    "| 비교 방식 | 실시간 메트릭 | 사후 로그 분석 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제: Shadow 모드 구현 (의사 코드)\n",
    "\n",
    "shadow_mode_code = \"\"\"\n",
    "# Shadow 모드 A/B 테스트 핵심 로직\n",
    "\n",
    "class FDSService:\n",
    "    def __init__(self):\n",
    "        self.champion = load_model(\"models:/fds-model@champion\")\n",
    "        self.challenger = load_model(\"models:/fds-model@challenger\")\n",
    "    \n",
    "    def predict(self, transaction):\n",
    "        # 1. Champion으로 실제 예측 (응답에 사용)\n",
    "        champion_pred = self.champion.predict(transaction)\n",
    "        champion_proba = self.champion.predict_proba(transaction)\n",
    "        \n",
    "        # 2. Challenger는 Shadow 예측 (로깅만!)\n",
    "        challenger_pred = self.challenger.predict(transaction)\n",
    "        challenger_proba = self.challenger.predict_proba(transaction)\n",
    "        \n",
    "        # 3. 로깅 (나중에 비교 분석용)\n",
    "        log_prediction(\n",
    "            transaction_id=transaction.id,\n",
    "            champion_pred=champion_pred,\n",
    "            champion_proba=champion_proba,\n",
    "            challenger_pred=challenger_pred,  # 그림자 예측\n",
    "            challenger_proba=challenger_proba,\n",
    "            actual_label=None  # 나중에 채워짐\n",
    "        )\n",
    "        \n",
    "        # 4. Champion 결과만 반환 (Challenger는 응답 안 함!)\n",
    "        return champion_pred, champion_proba\n",
    "\"\"\"\n",
    "print(shadow_mode_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 1: Shadow 모드 이해\n",
    "\n",
    "shadow_mode_quiz = {\n",
    "    \"challenger_실제_응답\": False,     # Challenger가 실제 응답을 반환하나요?\n",
    "    \"champion_실제_응답\": True,        # Champion이 실제 응답을 반환하나요?\n",
    "    \"challenger_로깅_함\": True,        # Challenger 예측을 로깅하나요?\n",
    "    \"실시간_비교\": False,              # 실시간으로 성능을 비교하나요? (사후 분석)\n",
    "    \"리스크_제로\": True,               # 실제 거래에 영향이 없나요?\n",
    "}\n",
    "\n",
    "# 검증\n",
    "assert shadow_mode_quiz[\"challenger_실제_응답\"] == False, \"Challenger는 응답 안 함!\"\n",
    "assert shadow_mode_quiz[\"champion_실제_응답\"] == True, \"Champion만 응답!\"\n",
    "assert shadow_mode_quiz[\"challenger_로깅_함\"] == True, \"Challenger는 로깅만!\"\n",
    "assert shadow_mode_quiz[\"리스크_제로\"] == True, \"Shadow 모드는 리스크 없음!\"\n",
    "\n",
    "print(\"체크포인트 1 통과! Shadow 모드 이해 완료\")\n",
    "print(\"   -> Challenger는 '그림자'처럼 로깅만, 실제 응답은 Champion만!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 통계적 유의성 (면접 필수!)\n",
    "\n",
    "### 왜 통계 검정이 필요한가?\n",
    "\n",
    "```\n",
    "상황:\n",
    "\n",
    "Champion Recall: 85.0%\n",
    "Challenger Recall: 86.2%\n",
    "\n",
    "\"Challenger가 1.2%p 높으니까 바로 교체하면 되지 않나요?\"\n",
    "\n",
    "문제점:\n",
    "- 그 차이가 실제 성능 차이인지?\n",
    "- 아니면 우연히 발생한 차이인지?\n",
    "- 내일 측정하면 반대로 나올 수도 있지 않나?\n",
    "\n",
    "-> 통계적 검정으로 \"우연이 아닌지\" 확인해야 함!\n",
    "```\n",
    "\n",
    "### t-검정 (t-test) 기초\n",
    "\n",
    "```\n",
    "t-검정이란?\n",
    "\n",
    "\"두 그룹의 평균이 통계적으로 다른지\" 검정하는 방법\n",
    "\n",
    "예시:\n",
    "- Champion 일별 Recall: [85, 84, 86, 85, 84, 85, 86]\n",
    "- Challenger 일별 Recall: [87, 88, 87, 86, 88, 87, 88]\n",
    "\n",
    "t-검정 결과:\n",
    "- t-statistic: 두 그룹이 얼마나 다른지 (클수록 차이 큼)\n",
    "- p-value: 그 차이가 우연일 확률\n",
    "```\n",
    "\n",
    "### p-value 해석\n",
    "\n",
    "| p-value | 해석 | 결정 |\n",
    "|---------|------|------|\n",
    "| **< 0.01** | 매우 강한 증거 | 확실히 다름 |\n",
    "| **< 0.05** | 강한 증거 | 다르다고 결론 (현업 기준) |\n",
    "| **< 0.10** | 약한 증거 | 애매함, 더 데이터 필요 |\n",
    "| **>= 0.10** | 증거 없음 | 차이 없다고 결론 |\n",
    "\n",
    "> **현업 기준: p < 0.05** (5% 유의수준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제: scipy로 t-검정 수행\n",
    "\n",
    "# 두 모델의 일별 Recall (7일간 측정)\n",
    "champion_recalls = np.array([0.85, 0.84, 0.86, 0.85, 0.84, 0.85, 0.86])\n",
    "challenger_recalls = np.array([0.87, 0.88, 0.87, 0.86, 0.88, 0.87, 0.88])\n",
    "\n",
    "print(\"성능 비교\")\n",
    "print(f\"Champion 평균 Recall: {champion_recalls.mean():.2%}\")\n",
    "print(f\"Challenger 평균 Recall: {challenger_recalls.mean():.2%}\")\n",
    "print(f\"차이: {(challenger_recalls.mean() - champion_recalls.mean()):.2%}\")\n",
    "print()\n",
    "\n",
    "# t-검정 수행\n",
    "t_stat, p_value = stats.ttest_ind(challenger_recalls, champion_recalls)\n",
    "\n",
    "print(\"t-검정 결과\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print()\n",
    "\n",
    "# 결과 해석\n",
    "if p_value < 0.05:\n",
    "    print(\"통계적으로 유의미한 차이가 있습니다 (p < 0.05)\")\n",
    "    print(\"   -> Challenger가 Champion보다 성능이 좋다고 결론 가능\")\n",
    "else:\n",
    "    print(\"통계적으로 유의미한 차이가 없습니다 (p >= 0.05)\")\n",
    "    print(\"   -> 차이가 우연일 수 있음, 더 많은 데이터 필요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습: t-검정 수행\n",
    "\n",
    "# 새로운 데이터: 14일간 측정한 Precision\n",
    "champion_precision = np.array([0.72, 0.73, 0.71, 0.72, 0.74, 0.73, 0.72,\n",
    "                               0.71, 0.73, 0.72, 0.74, 0.73, 0.72, 0.71])\n",
    "challenger_precision = np.array([0.75, 0.76, 0.74, 0.75, 0.77, 0.76, 0.75,\n",
    "                                  0.74, 0.76, 0.75, 0.77, 0.76, 0.75, 0.74])\n",
    "\n",
    "# 1. 평균 계산\n",
    "champion_mean = champion_precision.mean()\n",
    "challenger_mean = challenger_precision.mean()\n",
    "\n",
    "# 2. t-검정 수행\n",
    "t_stat, p_value = stats.ttest_ind(challenger_precision, champion_precision)\n",
    "\n",
    "# 3. 결과 출력\n",
    "print(f\"Champion 평균 Precision: {champion_mean:.2%}\")\n",
    "print(f\"Challenger 평균 Precision: {challenger_mean:.2%}\")\n",
    "print(f\"개선: {challenger_mean - champion_mean:.2%}\")\n",
    "print(f\"\\np-value: {p_value:.6f}\")\n",
    "\n",
    "# 4. 판단\n",
    "is_significant = p_value < 0.05\n",
    "\n",
    "# 체크포인트\n",
    "assert is_significant == True, \"p-value가 0.05보다 작아야 합니다\"\n",
    "print(f\"\\n체크포인트 2 통과! p={p_value:.6f} < 0.05 -> 통계적으로 유의미\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 면접에서 p-value 설명하기\n",
    "\n",
    "```\n",
    "면접관: \"p-value < 0.05가 의미하는 것은?\"\n",
    "\n",
    "모범 답변:\n",
    "\"p-value는 귀무가설(두 모델에 차이가 없다)이 참일 때,\n",
    "현재 관측한 결과가 나올 확률입니다.\n",
    "\n",
    "p < 0.05는 이 확률이 5% 미만이라는 뜻으로,\n",
    "'차이가 없다'는 가정 하에 이런 결과가 나올 확률이 매우 낮다는 의미입니다.\n",
    "\n",
    "따라서 귀무가설을 기각하고,\n",
    "'두 모델에 통계적으로 유의미한 차이가 있다'고 결론 내립니다.\n",
    "현업에서는 5% 유의수준을 기준으로 사용합니다.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Champion 승격 조건\n",
    "\n",
    "### Challenger -> Champion 승격 기준\n",
    "\n",
    "```\n",
    "승격 조건 4가지 (모두 충족해야 함)\n",
    "\n",
    "1. 성능 향상 >= 1%p\n",
    "   - 의미 있는 개선인지 (노이즈 vs 실제 개선)\n",
    "\n",
    "2. 통계적 유의성 (p < 0.05)\n",
    "   - 우연이 아닌지 검증\n",
    "\n",
    "3. 비용 개선 (또는 동등)\n",
    "   - 추론 비용, 인프라 비용\n",
    "\n",
    "4. 최소 관측 기간 >= 2주\n",
    "   - 충분한 데이터로 판단\n",
    "   - 주말/평일 패턴, 월초/월말 패턴 반영\n",
    "```\n",
    "\n",
    "### 왜 4가지 모두 필요한가?\n",
    "\n",
    "| 조건 | 빠지면 발생하는 문제 |\n",
    "|------|---------------------|\n",
    "| 성능 향상 | 0.1%p 개선에 교체 비용 들임 |\n",
    "| 통계적 유의성 | 우연한 차이로 잘못된 결정 |\n",
    "| 비용 개선 | 성능은 좋지만 비용 10배 |\n",
    "| 관측 기간 | 3일 데이터로 성급한 결정 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Champion 승격 판단 함수\n",
    "\n",
    "def should_promote_challenger(\n",
    "    champion_metrics: list,\n",
    "    challenger_metrics: list,\n",
    "    champion_cost: float,\n",
    "    challenger_cost: float,\n",
    "    observation_days: int,\n",
    "    min_improvement: float = 0.01,  # 1%p\n",
    "    p_threshold: float = 0.05,\n",
    "    min_days: int = 14\n",
    "):\n",
    "    \"\"\"Challenger를 Champion으로 승격할지 판단\n",
    "    \n",
    "    Args:\n",
    "        champion_metrics: Champion 일별 성능 (예: Recall)\n",
    "        challenger_metrics: Challenger 일별 성능\n",
    "        champion_cost: Champion 일 운영 비용\n",
    "        challenger_cost: Challenger 일 운영 비용\n",
    "        observation_days: 관측 기간 (일)\n",
    "        \n",
    "    Returns:\n",
    "        (승격 여부, 조건별 결과, 상세 정보)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 평균 성능 비교\n",
    "    champion_mean = np.mean(champion_metrics)\n",
    "    challenger_mean = np.mean(challenger_metrics)\n",
    "    improvement = challenger_mean - champion_mean\n",
    "    \n",
    "    # 2. 통계적 유의성\n",
    "    _, p_value = stats.ttest_ind(challenger_metrics, champion_metrics)\n",
    "    \n",
    "    # 3. 승격 조건 체크\n",
    "    conditions = {\n",
    "        f\"성능 향상 >= {min_improvement:.0%}\": improvement >= min_improvement,\n",
    "        f\"p-value < {p_threshold}\": p_value < p_threshold,\n",
    "        \"비용 개선\": challenger_cost <= champion_cost,\n",
    "        f\"관측 기간 >= {min_days}일\": observation_days >= min_days,\n",
    "    }\n",
    "    \n",
    "    # 4. 상세 정보\n",
    "    details = {\n",
    "        \"champion_mean\": champion_mean,\n",
    "        \"challenger_mean\": challenger_mean,\n",
    "        \"improvement\": improvement,\n",
    "        \"p_value\": p_value,\n",
    "        \"cost_saving\": champion_cost - challenger_cost,\n",
    "        \"observation_days\": observation_days,\n",
    "    }\n",
    "    \n",
    "    all_passed = all(conditions.values())\n",
    "    \n",
    "    return all_passed, conditions, details\n",
    "\n",
    "print(\"Champion 승격 판단 함수 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제: 승격 판단 테스트\n",
    "\n",
    "# 14일간 Recall 데이터\n",
    "champion_recall = np.array([0.85, 0.84, 0.86, 0.85, 0.84, 0.85, 0.86,\n",
    "                            0.85, 0.84, 0.86, 0.85, 0.84, 0.85, 0.86])\n",
    "challenger_recall = np.array([0.87, 0.88, 0.87, 0.86, 0.88, 0.87, 0.88,\n",
    "                              0.87, 0.88, 0.87, 0.86, 0.88, 0.87, 0.88])\n",
    "\n",
    "# 승격 판단\n",
    "should_promote, conditions, details = should_promote_challenger(\n",
    "    champion_metrics=champion_recall,\n",
    "    challenger_metrics=challenger_recall,\n",
    "    champion_cost=100.0,    # $100/일\n",
    "    challenger_cost=95.0,   # $95/일 (더 효율적)\n",
    "    observation_days=14\n",
    ")\n",
    "\n",
    "print(\"승격 조건 검사 결과\")\n",
    "print(\"=\" * 50)\n",
    "for condition, passed in conditions.items():\n",
    "    status = \"Pass\" if passed else \"Fail\"\n",
    "    print(f\"[{status}] {condition}\")\n",
    "\n",
    "print(\"\\n상세 정보\")\n",
    "print(f\"   Champion 평균: {details['champion_mean']:.2%}\")\n",
    "print(f\"   Challenger 평균: {details['challenger_mean']:.2%}\")\n",
    "print(f\"   개선: {details['improvement']:.2%}\")\n",
    "print(f\"   p-value: {details['p_value']:.6f}\")\n",
    "print(f\"   비용 절감: ${details['cost_saving']:.2f}/일\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if should_promote:\n",
    "    print(\"결론: Challenger를 Champion으로 승격!\")\n",
    "else:\n",
    "    print(\"결론: 승격 보류, 조건 미충족\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습: 승격 여부 판단\n",
    "\n",
    "# 시나리오: 새 모델 테스트 결과\n",
    "scenario = {\n",
    "    \"champion_recall\": np.array([0.80, 0.81, 0.79, 0.80, 0.82, 0.80, 0.81]),\n",
    "    \"challenger_recall\": np.array([0.82, 0.83, 0.81, 0.82, 0.84, 0.82, 0.83]),\n",
    "    \"champion_cost\": 120.0,\n",
    "    \"challenger_cost\": 150.0,  # 더 비쌈!\n",
    "    \"observation_days\": 7,     # 7일만 관측\n",
    "}\n",
    "\n",
    "should_promote, conditions, details = should_promote_challenger(\n",
    "    champion_metrics=scenario[\"champion_recall\"],\n",
    "    challenger_metrics=scenario[\"challenger_recall\"],\n",
    "    champion_cost=scenario[\"champion_cost\"],\n",
    "    challenger_cost=scenario[\"challenger_cost\"],\n",
    "    observation_days=scenario[\"observation_days\"]\n",
    ")\n",
    "\n",
    "print(\"승격 조건 검사 결과\")\n",
    "print(\"=\" * 50)\n",
    "failed_conditions = []\n",
    "for condition, passed in conditions.items():\n",
    "    status = \"Pass\" if passed else \"Fail\"\n",
    "    print(f\"[{status}] {condition}\")\n",
    "    if not passed:\n",
    "        failed_conditions.append(condition)\n",
    "\n",
    "print(f\"\\n실패한 조건: {failed_conditions}\")\n",
    "\n",
    "# 체크포인트\n",
    "assert should_promote == False, \"승격하면 안 됩니다!\"\n",
    "assert len(failed_conditions) == 2, \"실패 조건은 2개여야 합니다\"\n",
    "print(\"\\n체크포인트 3 통과!\")\n",
    "print(\"   -> 비용 증가 + 관측 기간 부족으로 승격 보류!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. 최종 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  2-S5 완료: A/B 테스트 기초\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"배운 것:\")\n",
    "print()\n",
    "print(\"1. A/B 테스트 vs Shadow 모드\")\n",
    "print(\"   - FDS는 Shadow 모드 사용 (리스크 제로)\")\n",
    "print(\"   - Challenger는 로깅만, 응답은 Champion만\")\n",
    "print()\n",
    "print(\"2. 통계적 유의성 (면접 필수!)\")\n",
    "print(\"   - p < 0.05: 통계적으로 유의미한 차이\")\n",
    "print(\"   - t-검정으로 두 모델 비교\")\n",
    "print()\n",
    "print(\"3. Champion 승격 조건 4가지\")\n",
    "print(\"   - 성능 향상 >= 1%p\")\n",
    "print(\"   - p-value < 0.05\")\n",
    "print(\"   - 비용 개선\")\n",
    "print(\"   - 관측 기간 >= 2주\")\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"Phase 2 학습 완료! 구현 노트북으로!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 체크리스트\n",
    "\n",
    "| 항목 | 이해도 |\n",
    "|------|--------|\n",
    "| Shadow 모드 (FDS 특수성) | |\n",
    "| **p-value < 0.05 (면접!)** | |\n",
    "| Champion 승격 조건 4가지 | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 면접 예상 질문\n",
    "\n",
    "**1. \"FDS에서 Shadow 모드 A/B를 사용하는 이유는?\"**\n",
    "\n",
    "답변:\n",
    "- FDS는 실제 거래에 영향을 미치므로 일반 A/B 테스트의 리스크가 큼\n",
    "- 새 모델이 사기를 놓치면 **금전 피해 발생**\n",
    "- Shadow 모드는 Challenger 모델이 '그림자'처럼 **예측만 로깅**하고, 실제 응답은 검증된 Champion 모델만 함\n",
    "- 실제 거래에 영향 없이 안전하게 새 모델 성능 평가 가능\n",
    "\n",
    "---\n",
    "\n",
    "**2. \"p-value < 0.05가 의미하는 것은?\"**\n",
    "\n",
    "답변:\n",
    "- p-value는 **귀무가설(두 모델에 차이가 없다)이 참일 때, 현재 관측한 결과가 나올 확률**\n",
    "- p < 0.05는 이 확률이 5% 미만이라는 뜻\n",
    "- '차이가 없다'는 가정 하에 이런 결과가 나올 확률이 매우 낮음\n",
    "- 따라서 **귀무가설을 기각**하고, '두 모델에 통계적으로 유의미한 차이가 있다'고 결론\n",
    "- 현업에서는 **5% 유의수준**을 표준으로 사용\n",
    "\n",
    "---\n",
    "\n",
    "**3. \"Champion 승격 조건은?\"**\n",
    "\n",
    "답변:\n",
    "1. **성능 향상 >= 1%p**: 의미 있는 개선인지\n",
    "2. **통계적 유의성 (p < 0.05)**: 우연이 아닌지\n",
    "3. **비용 개선**: 추론 비용, 인프라 비용\n",
    "4. **관측 기간 >= 2주**: 충분한 데이터로 판단"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
