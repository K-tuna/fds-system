{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-S4: LSTM ê¸°ì´ˆ\n",
    "\n",
    "ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” LSTMì˜ ê°œë…ê³¼ PyTorch ê¸°ë³¸ ì‚¬ìš©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "1. **RNN ê°œë…** - ìˆœì°¨ ë°ì´í„° ì²˜ë¦¬ ì›ë¦¬\n",
    "2. **LSTM êµ¬ì¡°** - ì¥ê¸° ê¸°ì–µì„ ìœ„í•œ ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜\n",
    "3. **PyTorch ê¸°ë³¸** - Tensor, nn.Module\n",
    "4. **LSTM ëª¨ë¸ êµ¬í˜„** - ê°„ë‹¨í•œ ë¶„ë¥˜ê¸° ë§Œë“¤ê¸°\n",
    "5. **FDS ì ìš©** - ê±°ë˜ ì‹œí€€ìŠ¤ í•™ìŠµ ê°œë…\n",
    "\n",
    "## ì˜ˆìƒ ì‹œê°„\n",
    "ì•½ 30ë¶„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# í•œê¸€ í°íŠ¸\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# GPU í™•ì¸\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"ì‚¬ìš© ì¥ì¹˜: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. RNN ê°œë…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. ì™œ RNNì´ í•„ìš”í•œê°€?\n",
    "\n",
    "**ì¼ë°˜ ì‹ ê²½ë§ì˜ í•œê³„**\n",
    "- ì…ë ¥ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬\n",
    "- \"ìˆœì„œ\" ì •ë³´ë¥¼ í™œìš©í•˜ì§€ ëª»í•¨\n",
    "\n",
    "**ì˜ˆì‹œ: ë¬¸ì¥ ë¶„ë¥˜**\n",
    "```\n",
    "\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆë‹¤\"\n",
    "\"ë¨¹ì—ˆë‹¤ ë°¥ì„ ë‚˜ëŠ”\"\n",
    "```\n",
    "â†’ ì¼ë°˜ ì‹ ê²½ë§ì€ ê°™ì€ ì…ë ¥ìœ¼ë¡œ ì²˜ë¦¬\n",
    "â†’ í•˜ì§€ë§Œ ì˜ë¯¸ê°€ ë‹¤ë¦„!\n",
    "\n",
    "**RNNì˜ í•´ê²°ì±…**\n",
    "- ì´ì „ ìƒíƒœ(hidden state)ë¥¼ ë‹¤ìŒ ìŠ¤í…ì— ì „ë‹¬\n",
    "- ìˆœì„œ ì •ë³´ë¥¼ \"ê¸°ì–µ\"í•˜ë©° ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. RNN êµ¬ì¡°\n",
    "\n",
    "```\n",
    "ì…ë ¥ x1 â†’ [RNN Cell] â†’ h1 (hidden state)\n",
    "              â†“\n",
    "ì…ë ¥ x2 â†’ [RNN Cell] â†’ h2 (ì´ì „ h1 ì‚¬ìš©)\n",
    "              â†“\n",
    "ì…ë ¥ x3 â†’ [RNN Cell] â†’ h3 (ì´ì „ h2 ì‚¬ìš©)\n",
    "              â†“\n",
    "           ìµœì¢… ì¶œë ¥\n",
    "```\n",
    "\n",
    "**ìˆ˜ì‹**\n",
    "```\n",
    "h_t = tanh(W_h * h_{t-1} + W_x * x_t + b)\n",
    "```\n",
    "\n",
    "- `h_t`: í˜„ì¬ hidden state\n",
    "- `h_{t-1}`: ì´ì „ hidden state\n",
    "- `x_t`: í˜„ì¬ ì…ë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š RNNì˜ ë™ì‘ ì›ë¦¬ ì‹œê°í™”\n",
    "# ê°„ë‹¨í•œ ì‹œí€€ìŠ¤: [1, 2, 3, 4, 5]\n",
    "\n",
    "sequence = [1, 2, 3, 4, 5]\n",
    "hidden = 0  # ì´ˆê¸° hidden state\n",
    "W_h, W_x = 0.5, 0.3  # ê°€ì¤‘ì¹˜ (ì˜ˆì‹œ)\n",
    "\n",
    "print(\"RNN ë™ì‘ ì‹œë®¬ë ˆì´ì…˜\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for t, x in enumerate(sequence):\n",
    "    new_hidden = np.tanh(W_h * hidden + W_x * x)\n",
    "    print(f\"t={t}: x={x}, h_prev={hidden:.3f} â†’ h_new={new_hidden:.3f}\")\n",
    "    hidden = new_hidden\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"ìµœì¢… hidden state: {hidden:.3f}\")\n",
    "print(\"â†’ ì‹œí€€ìŠ¤ ì „ì²´ ì •ë³´ê°€ ì¶•ì ë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. RNNì˜ í•œê³„: Vanishing Gradient\n",
    "\n",
    "**ë¬¸ì œ**\n",
    "- ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§€ë©´ ì´ˆë°˜ ì •ë³´ê°€ ì‚¬ë¼ì§\n",
    "- ì—­ì „íŒŒ ì‹œ ê·¸ë˜ë””ì–¸íŠ¸ê°€ 0ì— ìˆ˜ë ´\n",
    "\n",
    "```\n",
    "x1 â†’ x2 â†’ x3 â†’ ... â†’ x100 â†’ ì¶œë ¥\n",
    "â†‘                           â†“\n",
    "x1ì˜ ì˜í–¥ì´ ê±°ì˜ ì—†ì–´ì§ (vanishing)\n",
    "```\n",
    "\n",
    "**í•´ê²°ì±…: LSTM!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. LSTM êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. LSTM (Long Short-Term Memory)\n",
    "\n",
    "**í•µì‹¬ ì•„ì´ë””ì–´**\n",
    "- Cell State: ì¥ê¸° ê¸°ì–µ ì €ì¥ì†Œ (ê³ ì†ë„ë¡œì²˜ëŸ¼ ì •ë³´ ì „ë‹¬)\n",
    "- Gate: ì •ë³´ì˜ íë¦„ì„ ì œì–´í•˜ëŠ” ë°¸ë¸Œ\n",
    "\n",
    "### 2-2. 3ê°€ì§€ Gate\n",
    "\n",
    "| Gate | ì—­í•  | ë¹„ìœ  |\n",
    "|------|------|------|\n",
    "| **Forget Gate** | ê¸°ì¡´ ê¸°ì–µ ì¤‘ ë²„ë¦´ ê²ƒ ê²°ì • | ìŠì„ ë‚´ìš© ì„ íƒ |\n",
    "| **Input Gate** | ìƒˆ ì •ë³´ ì¤‘ ì €ì¥í•  ê²ƒ ê²°ì • | ê¸°ì–µí•  ë‚´ìš© ì„ íƒ |\n",
    "| **Output Gate** | ì¶œë ¥í•  ì •ë³´ ê²°ì • | ë§í•  ë‚´ìš© ì„ íƒ |\n",
    "\n",
    "```\n",
    "                    Cell State (ì¥ê¸° ê¸°ì–µ)\n",
    "     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Ã— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ + â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’\n",
    "                        â†‘               â†‘\n",
    "                   Forget Gate     Input Gate\n",
    "                   (ë²„ë¦´ ê²ƒ)        (ì €ì¥í•  ê²ƒ)\n",
    "                        â†‘               â†‘\n",
    "     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’\n",
    "                     Hidden State\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. LSTM vs RNN ë¹„êµ\n",
    "\n",
    "| íŠ¹ì„± | RNN | LSTM |\n",
    "|------|-----|------|\n",
    "| êµ¬ì¡° | ë‹¨ìˆœ | ë³µì¡ (ê²Œì´íŠ¸ 3ê°œ) |\n",
    "| ì¥ê¸° ê¸°ì–µ | âŒ ì–´ë ¤ì›€ | âœ… Cell State |\n",
    "| í•™ìŠµ ì†ë„ | ë¹ ë¦„ | ëŠë¦¼ |\n",
    "| íŒŒë¼ë¯¸í„° ìˆ˜ | ì ìŒ | 4ë°° ë§ìŒ |\n",
    "| ì‹¤ë¬´ ì‚¬ìš© | ê±°ì˜ ì•ˆí•¨ | ë§ì´ ì‚¬ìš© |\n",
    "\n",
    "**GRU (Gated Recurrent Unit)**\n",
    "- LSTMì˜ ê°„ì†Œí™” ë²„ì „\n",
    "- Gate 2ê°œ (Reset, Update)\n",
    "- ì„±ëŠ¥ ë¹„ìŠ·, ì†ë„ ë¹ ë¦„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. PyTorch ê¸°ë³¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Tensor\n",
    "\n",
    "PyTorchì˜ ê¸°ë³¸ ë°ì´í„° íƒ€ì… (NumPyì˜ ndarrayì™€ ìœ ì‚¬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š Tensor ê¸°ë³¸\n",
    "\n",
    "# ìƒì„±\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.zeros(3, 4)          # 0ìœ¼ë¡œ ì±„ìš´ 3x4\n",
    "c = torch.randn(3, 4)          # ì •ê·œë¶„í¬ ëœë¤\n",
    "\n",
    "print(\"í…ì„œ ìƒì„±\")\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b.shape = {b.shape}\")\n",
    "print(f\"c.shape = {c.shape}\")\n",
    "\n",
    "# NumPy ë³€í™˜\n",
    "np_arr = np.array([1, 2, 3])\n",
    "tensor_from_np = torch.from_numpy(np_arr)\n",
    "back_to_np = tensor_from_np.numpy()\n",
    "\n",
    "print(f\"\\nNumPy â†’ Tensor: {tensor_from_np}\")\n",
    "\n",
    "# GPUë¡œ ì´ë™\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tensor = c.to(device)\n",
    "    print(f\"\\nGPU í…ì„œ: {gpu_tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š Tensor ì—°ì‚°\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)  # ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì \n",
    "y = x ** 2\n",
    "z = y.sum()\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = xÂ² = {y}\")\n",
    "print(f\"z = sum(y) = {z}\")\n",
    "\n",
    "# ì—­ì „íŒŒ\n",
    "z.backward()\n",
    "print(f\"\\ndz/dx = 2x = {x.grad}\")  # [2, 4, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. nn.Module\n",
    "\n",
    "PyTorchì—ì„œ ëª¨ë¸ì„ ë§Œë“œëŠ” ê¸°ë³¸ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ê°„ë‹¨í•œ ì‹ ê²½ë§ ì˜ˆì œ\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "model = SimpleNet(input_size=10, hidden_size=5, output_size=1)\n",
    "print(model)\n",
    "\n",
    "# ìˆœì „íŒŒ\n",
    "sample_input = torch.randn(3, 10)  # ë°°ì¹˜ 3, ì…ë ¥ 10\n",
    "output = model(sample_input)\n",
    "print(f\"\\nì…ë ¥: {sample_input.shape}\")\n",
    "print(f\"ì¶œë ¥: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. í•™ìŠµ ë£¨í”„\n",
    "\n",
    "```python\n",
    "# 1. ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "model = MyModel()\n",
    "criterion = nn.BCELoss()           # Binary Cross Entropy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 2. í•™ìŠµ ë£¨í”„\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # ìˆœì „íŒŒ\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        \n",
    "        # ì—­ì „íŒŒ\n",
    "        optimizer.zero_grad()   # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "        loss.backward()         # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°\n",
    "        optimizer.step()        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. LSTM ëª¨ë¸ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. nn.LSTM ì‚¬ìš©ë²•\n",
    "\n",
    "```python\n",
    "lstm = nn.LSTM(\n",
    "    input_size=10,      # ì…ë ¥ í”¼ì²˜ ìˆ˜\n",
    "    hidden_size=64,     # hidden state í¬ê¸°\n",
    "    num_layers=2,       # LSTM ì¸µ ìˆ˜\n",
    "    batch_first=True,   # ì…ë ¥ í˜•íƒœ: (batch, seq, features)\n",
    "    dropout=0.2         # ë“œë¡­ì•„ì›ƒ (2ì¸µ ì´ìƒì¼ ë•Œ)\n",
    ")\n",
    "```\n",
    "\n",
    "**ì…ë ¥ í˜•íƒœ** (batch_first=True)\n",
    "- ì…ë ¥: `(batch_size, seq_len, input_size)`\n",
    "- ì¶œë ¥: `(batch_size, seq_len, hidden_size)`\n",
    "- hidden: `(num_layers, batch_size, hidden_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š LSTM ë ˆì´ì–´ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "# LSTM ì •ì˜\n",
    "lstm_layer = nn.LSTM(\n",
    "    input_size=10,      # ê° timestepì˜ í”¼ì²˜ ìˆ˜\n",
    "    hidden_size=64,     # hidden state í¬ê¸°\n",
    "    num_layers=1,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "# ê°€ìƒ ì…ë ¥: ë°°ì¹˜ 4, ì‹œí€€ìŠ¤ ê¸¸ì´ 5, í”¼ì²˜ 10\n",
    "sample_input = torch.randn(4, 5, 10)\n",
    "print(f\"ì…ë ¥ shape: {sample_input.shape}\")\n",
    "print(\"  â†’ (batch=4, seq_len=5, features=10)\")\n",
    "\n",
    "# LSTM í†µê³¼\n",
    "output, (hidden, cell) = lstm_layer(sample_input)\n",
    "\n",
    "print(f\"\\nì¶œë ¥ shape: {output.shape}\")\n",
    "print(\"  â†’ (batch=4, seq_len=5, hidden=64)\")\n",
    "print(f\"\\nhidden shape: {hidden.shape}\")\n",
    "print(\"  â†’ (num_layers=1, batch=4, hidden=64)\")\n",
    "print(f\"\\ncell shape: {cell.shape}\")\n",
    "print(\"  â†’ Cell State (ì¥ê¸° ê¸°ì–µ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š FDSìš© LSTM ë¶„ë¥˜ê¸°\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"ê±°ë˜ ì‹œí€€ìŠ¤ë¡œ ì‚¬ê¸° íƒì§€í•˜ëŠ” LSTM ëª¨ë¸\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int,    # ê±°ë˜ë‹¹ í”¼ì²˜ ìˆ˜\n",
    "        hidden_size: int,   # LSTM hidden í¬ê¸°\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 1)  # ì´ì§„ ë¶„ë¥˜\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, features) í˜•íƒœì˜ ì‹œí€€ìŠ¤\n",
    "        Returns:\n",
    "            ì‚¬ê¸° í™•ë¥  (batch, 1)\n",
    "        \"\"\"\n",
    "        # LSTM í†µê³¼\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ hidden state ì‚¬ìš© (ë§ˆì§€ë§‰ layer)\n",
    "        last_hidden = hidden[-1]  # (batch, hidden_size)\n",
    "        \n",
    "        # ë¶„ë¥˜\n",
    "        out = self.fc(last_hidden)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "model = LSTMClassifier(\n",
    "    input_size=15,      # ê±°ë˜ë‹¹ 15ê°œ í”¼ì²˜\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nì´ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ê°„ë‹¨í•œ í•™ìŠµ ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ê°€ìƒ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±\n",
    "# ì‹œí€€ìŠ¤ì˜ í•©ì´ íŠ¹ì • ê°’ ì´ìƒì´ë©´ ì‚¬ê¸°(1)ë¡œ ë¶„ë¥˜\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 1000\n",
    "seq_len = 10\n",
    "n_features = 5\n",
    "\n",
    "# ëœë¤ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "X = np.random.randn(n_samples, seq_len, n_features).astype(np.float32)\n",
    "\n",
    "# ë¼ë²¨: ì‹œí€€ìŠ¤ì˜ ì²« ë²ˆì§¸ í”¼ì²˜ í‰ê· ì´ 0.3 ì´ìƒì´ë©´ ì‚¬ê¸°\n",
    "y = (X[:, :, 0].mean(axis=1) > 0.3).astype(np.float32)\n",
    "\n",
    "print(f\"ë°ì´í„° shape: {X.shape}\")\n",
    "print(f\"  â†’ {n_samples}ê°œ ì‹œí€€ìŠ¤, ê° {seq_len} timestep, {n_features} í”¼ì²˜\")\n",
    "print(f\"\\nì‚¬ê¸° ë¹„ìœ¨: {y.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ë°ì´í„° ë¶„í•  ë° DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tensor ë³€í™˜\n",
    "X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "y_train_t = torch.FloatTensor(y_train).reshape(-1, 1).to(device)\n",
    "X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "y_test_t = torch.FloatTensor(y_test).reshape(-1, 1).to(device)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š LSTM í•™ìŠµ\n",
    "\n",
    "# ëª¨ë¸\n",
    "model = LSTMClassifier(\n",
    "    input_size=n_features,\n",
    "    hidden_size=32,\n",
    "    num_layers=1\n",
    ").to(device)\n",
    "\n",
    "# ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# í•™ìŠµ\n",
    "n_epochs = 20\n",
    "history = {'loss': [], 'acc': []}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # ìˆœì „íŒŒ\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        \n",
    "        # ì—­ì „íŒŒ\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # ì—í¬í¬ í‰ê°€\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    history['loss'].append(avg_loss)\n",
    "    \n",
    "    # ì •í™•ë„ ê³„ì‚°\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_test_t)\n",
    "        acc = ((pred > 0.5) == y_test_t).float().mean().item()\n",
    "        history['acc'].append(acc)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}: Loss={avg_loss:.4f}, Test Acc={acc:.1%}\")\n",
    "\n",
    "print(\"\\ní•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š í•™ìŠµ ê³¼ì • ì‹œê°í™”\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['loss'])\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['acc'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Test Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. FDSì—ì„œ LSTM ì ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1. ì™œ FDSì—ì„œ LSTM?\n",
    "\n",
    "**ê±°ë˜ ë°ì´í„°ì˜ íŠ¹ì„±**\n",
    "- ë™ì¼ ì‚¬ìš©ìì˜ ì—°ì† ê±°ë˜ëŠ” íŒ¨í„´ì´ ìˆìŒ\n",
    "- ê°‘ìê¸° í‰ì†Œì™€ ë‹¤ë¥¸ íŒ¨í„´ = ì´ìƒ ì‹ í˜¸\n",
    "\n",
    "**ì˜ˆì‹œ: ì •ìƒ vs ì‚¬ê¸° íŒ¨í„´**\n",
    "```\n",
    "ì •ìƒ ì‚¬ìš©ì:\n",
    "  ê±°ë˜1: í¸ì˜ì  5ì²œì›\n",
    "  ê±°ë˜2: ì§€í•˜ì²  1,400ì›\n",
    "  ê±°ë˜3: ì¹´í˜ 4,500ì›\n",
    "  ê±°ë˜4: ë§ˆíŠ¸ 3ë§Œì›\n",
    "  â†’ ì¼ìƒì  ì†Œì•¡ íŒ¨í„´\n",
    "\n",
    "ì‚¬ê¸° ë°œìƒ ì‹œ:\n",
    "  ê±°ë˜1: í¸ì˜ì  5ì²œì›\n",
    "  ê±°ë˜2: í•´ì™¸ ë©´ì„¸ì  200ë§Œì›  â† ê°‘ìê¸°?\n",
    "  ê±°ë˜3: ë‹¤ë¥¸ í•´ì™¸ ë©´ì„¸ì  150ë§Œì›  â† ì—°ì†?\n",
    "  ê±°ë˜4: ìƒˆë²½ 3ì‹œ ì˜¨ë¼ì¸ ê²°ì œ 80ë§Œì›  â† ì´ìƒ!\n",
    "  â†’ LSTMì´ ì´ íŒ¨í„´ ë³€í™”ë¥¼ ê°ì§€\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. ì‹œí€€ìŠ¤ ë°ì´í„° êµ¬ì„± ë°©ë²•\n",
    "\n",
    "**Phase 1ì—ì„œ ì‚¬ìš©í•  ë°©ì‹**\n",
    "\n",
    "```python\n",
    "# 1. ì‚¬ìš©ìë³„ë¡œ ê±°ë˜ ì •ë ¬\n",
    "df_sorted = df.sort_values(['user_id', 'timestamp'])\n",
    "\n",
    "# 2. ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "SEQ_LEN = 10  # ìµœê·¼ 10ê±´ì˜ ê±°ë˜ë¥¼ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ\n",
    "\n",
    "sequences = []\n",
    "for user_id in df['user_id'].unique():\n",
    "    user_txns = df[df['user_id'] == user_id]\n",
    "    for i in range(len(user_txns) - SEQ_LEN):\n",
    "        seq = user_txns.iloc[i:i+SEQ_LEN][features]\n",
    "        label = user_txns.iloc[i+SEQ_LEN]['is_fraud']\n",
    "        sequences.append((seq, label))\n",
    "```\n",
    "\n",
    "**ì‹œí€€ìŠ¤ í˜•íƒœ**\n",
    "```\n",
    "ì…ë ¥: (batch, 10, 15)\n",
    "  â†’ ê° ë°°ì¹˜ë§ˆë‹¤ 10ê±´ì˜ ê±°ë˜, ê±°ë˜ë‹¹ 15ê°œ í”¼ì²˜\n",
    "ì¶œë ¥: (batch, 1)\n",
    "  â†’ ë‹¤ìŒ ê±°ë˜ê°€ ì‚¬ê¸°ì¼ í™•ë¥ \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-3. XGBoost vs LSTM ì—­í•  ë¶„ë‹´\n",
    "\n",
    "| ê´€ì  | XGBoost | LSTM |\n",
    "|------|---------|------|\n",
    "| ì…ë ¥ | ë‹¨ì¼ ê±°ë˜ | ê±°ë˜ ì‹œí€€ìŠ¤ |\n",
    "| ê°•ì  | ì •ì  í”¼ì²˜ (ê¸ˆì•¡, ì‹œê°„ëŒ€, ì¹´ë“œì¢…ë¥˜) | ë™ì  íŒ¨í„´ (ì—°ì†ì„±, ë³€í™”) |\n",
    "| íŠ¹ì„± | ë¹ ë¥¸ í•™ìŠµ/ì¶”ë¡  | ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ |\n",
    "\n",
    "**ì•™ìƒë¸” ì „ëµ (Phase 1)**\n",
    "```python\n",
    "# XGBoost: ê°œë³„ ê±°ë˜ íŠ¹ì„±ìœ¼ë¡œ ì˜ˆì¸¡\n",
    "xgb_prob = xgb_model.predict_proba(single_txn)\n",
    "\n",
    "# LSTM: ê±°ë˜ ì‹œí€€ìŠ¤ë¡œ ì˜ˆì¸¡\n",
    "lstm_prob = lstm_model(txn_sequence)\n",
    "\n",
    "# ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”\n",
    "final_prob = 0.6 * xgb_prob + 0.4 * lstm_prob\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ë©´ì ‘ Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: \"ì™œ LSTMì„ ì‚¬ìš©í–ˆë‚˜ìš”?\"\n",
    "\n",
    "> \"FDSì—ì„œ ê°œë³„ ê±°ë˜ë¿ë§Œ ì•„ë‹ˆë¼ ê±°ë˜ íŒ¨í„´ì˜ ë³€í™”ë¥¼ ê°ì§€í•˜ê³  ì‹¶ì—ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í‰ì†Œ ì†Œì•¡ ê²°ì œë§Œ í•˜ë˜ ì‚¬ìš©ìê°€ ê°‘ìê¸° í•´ì™¸ì—ì„œ ê³ ì•¡ ê²°ì œë¥¼ í•˜ë©´, ê°œë³„ ê±°ë˜ë¡œëŠ” ì •ìƒì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ ì‹œí€€ìŠ¤ë¡œ ë³´ë©´ ì´ìƒ íŒ¨í„´ì…ë‹ˆë‹¤. LSTMì€ ì´ëŸ° ì‹œê³„ì—´ íŒ¨í„´ì„ ì˜ í•™ìŠµí•©ë‹ˆë‹¤.\"\n",
    "\n",
    "### Q: \"RNN ëŒ€ì‹  LSTMì„ ì„ íƒí•œ ì´ìœ ëŠ”?\"\n",
    "\n",
    "> \"RNNì€ ì¥ê¸° ì˜ì¡´ì„±(long-term dependency) ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. 10ê±´ ì´ìƒì˜ ê±°ë˜ ì‹œí€€ìŠ¤ì—ì„œ ì´ˆë°˜ ê±°ë˜ ì •ë³´ê°€ ìœ ì‹¤ë˜ëŠ” vanishing gradient ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. LSTMì€ Cell Stateì™€ Gate ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì´ë¥¼ í•´ê²°í•´ì„œ ê¸´ ì‹œí€€ìŠ¤ì—ì„œë„ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "### Q: \"Transformer ëŒ€ì‹  LSTMì„ ì“´ ì´ìœ ëŠ”?\"\n",
    "\n",
    "> \"ë‘ ê°€ì§€ ì´ìœ ì…ë‹ˆë‹¤. ì²«ì§¸, ê±°ë˜ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 10~20ê±´ ì •ë„ë¡œ ì§§ì•„ì„œ Transformerì˜ ì¥ì ì¸ ë³‘ë ¬ ì²˜ë¦¬ì™€ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ê°€ í¬ê²Œ í•„ìš”í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‘˜ì§¸, LSTMì´ ë” ì ì€ íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµì´ ë¹ ë¥´ê³ , ì‹¤ì‹œê°„ ì¶”ë¡ ì— ìœ ë¦¬í•©ë‹ˆë‹¤. ë¬¼ë¡  ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜ í›„ ìµœì¢… ê²°ì •í–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "### Q: \"ì‹œí€€ìŠ¤ ê¸¸ì´(seq_len)ëŠ” ì–´ë–»ê²Œ ì •í–ˆë‚˜ìš”?\"\n",
    "\n",
    "> \"10ê±´ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. ë„ˆë¬´ ì§§ìœ¼ë©´ íŒ¨í„´ì„ ëª» ì¡ê³ , ë„ˆë¬´ ê¸¸ë©´ ê´€ë ¨ ì—†ëŠ” ì˜¤ë˜ëœ ê±°ë˜ê°€ í¬í•¨ë©ë‹ˆë‹¤. 5, 10, 20ê±´ìœ¼ë¡œ ì‹¤í—˜í•œ ê²°ê³¼ 10ê±´ì—ì„œ AUCê°€ ê°€ì¥ ë†’ì•˜ìŠµë‹ˆë‹¤. ë˜í•œ ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì—ì„œ ìµœê·¼ 10ê±´ì€ í•­ìƒ ìºì‹±í•´ë‘˜ ìˆ˜ ìˆëŠ” í˜„ì‹¤ì ì¸ ìˆ˜ì¹˜ì…ë‹ˆë‹¤.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ìµœì¢… ì²´í¬í¬ì¸íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  1-S4 ì™„ë£Œ: LSTM ê¸°ì´ˆ\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"ë°°ìš´ ê²ƒ:\")\n",
    "print()\n",
    "print(\"1. RNN ê°œë…\")\n",
    "print(\"   - ìˆœì°¨ ë°ì´í„° ì²˜ë¦¬, hidden state ì „ë‹¬\")\n",
    "print(\"   - í•œê³„: Vanishing Gradient\")\n",
    "print()\n",
    "print(\"2. LSTM êµ¬ì¡°\")\n",
    "print(\"   - Cell State: ì¥ê¸° ê¸°ì–µ ì €ì¥ì†Œ\")\n",
    "print(\"   - 3ê°€ì§€ Gate: Forget, Input, Output\")\n",
    "print()\n",
    "print(\"3. PyTorch ê¸°ë³¸\")\n",
    "print(\"   - Tensor, nn.Module, í•™ìŠµ ë£¨í”„\")\n",
    "print(\"   - forward(), backward(), optimizer.step()\")\n",
    "print()\n",
    "print(\"4. FDS ì ìš©\")\n",
    "print(\"   - ê±°ë˜ ì‹œí€€ìŠ¤ë¡œ íŒ¨í„´ ë³€í™” ê°ì§€\")\n",
    "print(\"   - XGBoost(ì •ì ) + LSTM(ë™ì ) ì•™ìƒë¸”\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"ë‹¤ìŒ: 1-S5 ì•™ìƒë¸” ê°œë…\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
