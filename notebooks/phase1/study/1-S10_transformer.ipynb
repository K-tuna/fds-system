{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-S10: Transformer for Tabular Data\n",
    "\n",
    "정형(Tabular) 데이터에 Transformer를 적용하는 방법을 학습합니다.\n",
    "\n",
    "## 학습 목표\n",
    "1. **Self-Attention** - Query, Key, Value 개념\n",
    "2. **정형 데이터에서 Transformer** - NLP와의 차이점\n",
    "3. **TabTransformer** - 범주형 피처에 Transformer 적용\n",
    "4. **FT-Transformer** - 모든 피처에 Transformer 적용\n",
    "5. **트리 vs Transformer** - 언제 Transformer가 유리한가?\n",
    "\n",
    "## 왜 Transformer인가?\n",
    "\n",
    "| 모델 | 장점 | 단점 |\n",
    "|------|------|------|\n",
    "| 트리 (XGBoost) | 빠름, 해석 가능 | 피처 간 상호작용 제한 |\n",
    "| **Transformer** | 피처 간 관계 학습 | 느림, 데이터 많이 필요 |\n",
    "\n",
    "## 예상 시간\n",
    "약 30분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"패키지 로드 완료!\")\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"GPU 사용 가능: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Self-Attention 기초\n",
    "\n",
    "### 1-1. Attention이란?\n",
    "\n",
    "**핵심 아이디어:** \"어떤 정보에 집중할지 학습\"\n",
    "\n",
    "```\n",
    "입력: [거래금액, 거래시간, 카드종류, 위치]\n",
    "      ↓\n",
    "Attention: \"이 거래에서 '위치'와 '카드종류'가 중요하다\"\n",
    "      ↓\n",
    "가중치: [0.1, 0.2, 0.3, 0.4]  # 위치에 높은 가중치\n",
    "```\n",
    "\n",
    "### 1-2. Query, Key, Value (Q, K, V)\n",
    "\n",
    "**비유: 검색 엔진**\n",
    "\n",
    "| 요소 | 역할 | 비유 |\n",
    "|------|------|------|\n",
    "| **Query (Q)** | \"무엇을 찾고 있는가?\" | 검색어 |\n",
    "| **Key (K)** | \"무엇이 있는가?\" | 문서 제목 |\n",
    "| **Value (V)** | \"실제 정보\" | 문서 내용 |\n",
    "\n",
    "### 1-3. Self-Attention 수식\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q·K^T / √d_k) · V\n",
    "```\n",
    "\n",
    "1. Q와 K의 내적 → 유사도 계산\n",
    "2. √d_k로 나눔 → 스케일 조정\n",
    "3. softmax → 확률로 변환 (합이 1)\n",
    "4. V와 곱함 → 가중합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention 직접 구현\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \n",
    "    Args:\n",
    "        Q: Query (batch, seq_len, d_k)\n",
    "        K: Key (batch, seq_len, d_k)\n",
    "        V: Value (batch, seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output\n",
    "        attention_weights: 어디에 집중했는지\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # 1. Q·K^T (유사도 계산)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # 2. Scale (기울기 안정화)\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # 3. Softmax (확률로 변환)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 4. V와 곱함 (가중합)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "print(\"Self-Attention 함수 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention 예제 (4개 피처)\n",
    "\n",
    "# 4개 피처: [거래금액, 거래시간, 카드종류, 위치]\n",
    "# 각 피처를 4차원 임베딩으로 표현\n",
    "seq_len = 4  # 피처 수\n",
    "d_model = 4  # 임베딩 차원\n",
    "\n",
    "# 임의의 입력 (1개 샘플)\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Q, K, V 생성 (실제로는 학습되는 가중치로 변환)\n",
    "# 여기서는 간단히 X를 그대로 사용\n",
    "Q, K, V = X, X, X\n",
    "\n",
    "# Self-Attention 적용\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"입력 shape:\", X.shape)\n",
    "print(\"출력 shape:\", output.shape)\n",
    "print(\"\\nAttention Weights (어디에 집중했는지):\")\n",
    "print(attention_weights[0].numpy().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Weights 시각화\n",
    "\n",
    "feature_names = ['거래금액', '거래시간', '카드종류', '위치']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(attention_weights[0].detach().numpy(), cmap='Blues')\n",
    "\n",
    "ax.set_xticks(range(len(feature_names)))\n",
    "ax.set_yticks(range(len(feature_names)))\n",
    "ax.set_xticklabels(feature_names)\n",
    "ax.set_yticklabels(feature_names)\n",
    "\n",
    "ax.set_xlabel('Key (정보 제공)')\n",
    "ax.set_ylabel('Query (정보 요청)')\n",
    "ax.set_title('Self-Attention Weights\\n(각 피처가 다른 피처에 얼마나 집중하는가)')\n",
    "\n",
    "# 값 표시\n",
    "for i in range(len(feature_names)):\n",
    "    for j in range(len(feature_names)):\n",
    "        val = attention_weights[0, i, j].item()\n",
    "        ax.text(j, i, f'{val:.2f}', ha='center', va='center', \n",
    "                color='white' if val > 0.3 else 'black')\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n해석: 각 행은 Query 피처가 Key 피처들에 얼마나 집중하는지 보여줌\")\n",
    "print(\"      대각선이 높으면 자기 자신에 집중 (Self-Attention)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. Multi-Head Attention\n",
    "\n",
    "**문제:** 단일 Attention은 하나의 관점만 학습\n",
    "\n",
    "**해결:** 여러 개의 Attention을 병렬로 (Multi-Head)\n",
    "\n",
    "```\n",
    "Head 1: \"거래금액과 위치의 관계\" 학습\n",
    "Head 2: \"거래시간과 카드종류의 관계\" 학습\n",
    "Head 3: \"전체적인 패턴\" 학습\n",
    "...\n",
    "→ 모든 Head 결과를 concat → 풍부한 표현\n",
    "```\n",
    "\n",
    "```python\n",
    "# PyTorch에서\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=64, num_heads=8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 정형 데이터에서 Transformer\n",
    "\n",
    "### 2-1. NLP vs 정형 데이터\n",
    "\n",
    "| 특성 | NLP (텍스트) | Tabular (정형) |\n",
    "|------|-------------|----------------|\n",
    "| 입력 | 단어 시퀀스 | 피처 집합 |\n",
    "| **순서** | 중요 (문장 순서) | **순서 없음** |\n",
    "| 토큰 | 단어/서브워드 | 피처 값 |\n",
    "| Positional Encoding | 필수 | **불필요** |\n",
    "\n",
    "### 2-2. Column Embedding\n",
    "\n",
    "**아이디어:** 각 피처(컬럼)를 하나의 \"토큰\"으로 취급\n",
    "\n",
    "```\n",
    "NLP:     [\"I\", \"love\", \"AI\"]  → [embed(\"I\"), embed(\"love\"), embed(\"AI\")]\n",
    "Tabular: [100, 0.5, \"visa\"]   → [embed(금액), embed(시간), embed(카드)]\n",
    "```\n",
    "\n",
    "### 2-3. 수치형 vs 범주형 처리\n",
    "\n",
    "| 타입 | 처리 방법 |\n",
    "|------|----------|\n",
    "| **수치형** | Linear projection → 임베딩 |\n",
    "| **범주형** | Embedding lookup (NLP와 동일) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. TabTransformer (2020)\n",
    "\n",
    "**논문:** \"TabTransformer: Tabular Data Modeling Using Contextual Embeddings\"\n",
    "\n",
    "### 3-1. 핵심 아이디어\n",
    "\n",
    "**범주형 피처만** Transformer로 처리\n",
    "\n",
    "```\n",
    "입력: [수치형 피처들] + [범주형 피처들]\n",
    "              ↓                ↓\n",
    "           그대로         Transformer\n",
    "              ↓                ↓\n",
    "           Concat ← ← ← ← ← ←\n",
    "              ↓\n",
    "            MLP\n",
    "              ↓\n",
    "            예측\n",
    "```\n",
    "\n",
    "### 3-2. 왜 범주형만?\n",
    "\n",
    "- 범주형 피처 간 상호작용이 중요\n",
    "- 예: \"카드종류=visa\" + \"국가=해외\" → 사기 확률 상승\n",
    "- Transformer가 이런 조합을 학습\n",
    "\n",
    "### 3-3. 구조\n",
    "\n",
    "```\n",
    "1. Column Embedding: 각 범주형 피처를 d차원 벡터로\n",
    "2. Transformer Layers: 피처 간 관계 학습 (N개 레이어)\n",
    "3. Concatenation: Transformer 출력 + 수치형 피처\n",
    "4. MLP: 최종 분류/회귀\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabTransformer 간단 구현\n",
    "\n",
    "class TabTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    TabTransformer: 범주형 피처에 Transformer 적용\n",
    "    \n",
    "    Args:\n",
    "        num_categories: 각 범주형 피처의 카테고리 수 리스트\n",
    "        num_continuous: 수치형 피처 수\n",
    "        d_model: 임베딩 차원\n",
    "        n_heads: Multi-Head Attention 헤드 수\n",
    "        n_layers: Transformer 레이어 수\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_categories, num_continuous, d_model=32, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_categories = num_categories\n",
    "        self.num_continuous = num_continuous\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 범주형 임베딩 (각 피처별로)\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_cat, d_model) for num_cat in num_categories\n",
    "        ])\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # MLP (Transformer 출력 + 수치형 피처 → 예측)\n",
    "        mlp_input_dim = len(num_categories) * d_model + num_continuous\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(mlp_input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_cat: 범주형 피처 (batch, num_cat_features) - 정수 인덱스\n",
    "            x_cont: 수치형 피처 (batch, num_cont_features) - 실수\n",
    "        \"\"\"\n",
    "        # 1. 범주형 임베딩\n",
    "        cat_embeds = []\n",
    "        for i, embed in enumerate(self.embeddings):\n",
    "            cat_embeds.append(embed(x_cat[:, i]))\n",
    "        \n",
    "        # (batch, num_cat_features, d_model)\n",
    "        cat_embeds = torch.stack(cat_embeds, dim=1)\n",
    "        \n",
    "        # 2. Transformer (범주형 피처 간 관계 학습)\n",
    "        transformed = self.transformer(cat_embeds)\n",
    "        \n",
    "        # 3. Flatten\n",
    "        transformed = transformed.flatten(start_dim=1)\n",
    "        \n",
    "        # 4. Concat (Transformer 출력 + 수치형)\n",
    "        combined = torch.cat([transformed, x_cont], dim=1)\n",
    "        \n",
    "        # 5. MLP\n",
    "        out = self.mlp(combined)\n",
    "        \n",
    "        return out.squeeze(-1)\n",
    "\n",
    "print(\"TabTransformer 클래스 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabTransformer 테스트\n",
    "\n",
    "# 예시: 3개 범주형 피처, 5개 수치형 피처\n",
    "num_categories = [10, 5, 8]  # 각 범주형 피처의 카테고리 수\n",
    "num_continuous = 5\n",
    "\n",
    "model = TabTransformer(\n",
    "    num_categories=num_categories,\n",
    "    num_continuous=num_continuous,\n",
    "    d_model=32,\n",
    "    n_heads=4,\n",
    "    n_layers=2\n",
    ")\n",
    "\n",
    "# 더미 데이터\n",
    "batch_size = 4\n",
    "x_cat = torch.randint(0, 5, (batch_size, len(num_categories)))  # 범주형 인덱스\n",
    "x_cont = torch.randn(batch_size, num_continuous)  # 수치형\n",
    "\n",
    "# Forward\n",
    "output = model(x_cat, x_cont)\n",
    "\n",
    "print(f\"범주형 입력: {x_cat.shape}\")\n",
    "print(f\"수치형 입력: {x_cont.shape}\")\n",
    "print(f\"출력: {output.shape}\")\n",
    "print(f\"예측 확률: {output.detach().numpy().round(3)}\")\n",
    "\n",
    "# 파라미터 수\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n총 파라미터 수: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. FT-Transformer (2021)\n",
    "\n",
    "**논문:** \"Revisiting Deep Learning Models for Tabular Data\"\n",
    "\n",
    "### 4-1. 핵심 아이디어\n",
    "\n",
    "**모든 피처**를 Transformer로 처리 (수치형 포함)\n",
    "\n",
    "```\n",
    "TabTransformer: 범주형만 Transformer\n",
    "FT-Transformer: 모든 피처 Transformer\n",
    "```\n",
    "\n",
    "### 4-2. Feature Tokenizer\n",
    "\n",
    "수치형 피처도 \"토큰\"으로 변환:\n",
    "\n",
    "```\n",
    "수치형 피처 x → Linear(x) + bias → d차원 임베딩\n",
    "범주형 피처 c → Embedding(c) → d차원 임베딩\n",
    "```\n",
    "\n",
    "### 4-3. [CLS] 토큰\n",
    "\n",
    "BERT처럼 특별한 [CLS] 토큰 추가:\n",
    "\n",
    "```\n",
    "입력: [[CLS], 피처1, 피처2, ..., 피처N]\n",
    "       ↓\n",
    "Transformer\n",
    "       ↓\n",
    "[CLS]의 출력 → 예측에 사용\n",
    "```\n",
    "\n",
    "### 4-4. 구조 비교\n",
    "\n",
    "| 요소 | TabTransformer | FT-Transformer |\n",
    "|------|----------------|----------------|\n",
    "| 수치형 처리 | MLP에서 | **Transformer에서** |\n",
    "| 범주형 처리 | Transformer | Transformer |\n",
    "| 출력 방식 | Flatten | **[CLS] 토큰** |\n",
    "| 성능 | 좋음 | **더 좋음** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FT-Transformer 간단 구현\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    FT-Transformer: 모든 피처에 Transformer 적용\n",
    "    \n",
    "    Args:\n",
    "        num_categories: 각 범주형 피처의 카테고리 수 리스트\n",
    "        num_continuous: 수치형 피처 수\n",
    "        d_model: 임베딩 차원\n",
    "        n_heads: Multi-Head Attention 헤드 수\n",
    "        n_layers: Transformer 레이어 수\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_categories, num_continuous, d_model=32, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_categories = num_categories\n",
    "        self.num_continuous = num_continuous\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 범주형 임베딩\n",
    "        self.cat_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_cat, d_model) for num_cat in num_categories\n",
    "        ])\n",
    "        \n",
    "        # 수치형 Feature Tokenizer (각 피처별 Linear)\n",
    "        self.cont_embeddings = nn.ModuleList([\n",
    "            nn.Linear(1, d_model) for _ in range(num_continuous)\n",
    "        ])\n",
    "        \n",
    "        # [CLS] 토큰\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # 출력 레이어 ([CLS] 토큰 → 예측)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_cat: 범주형 피처 (batch, num_cat_features)\n",
    "            x_cont: 수치형 피처 (batch, num_cont_features)\n",
    "        \"\"\"\n",
    "        batch_size = x_cat.size(0)\n",
    "        \n",
    "        # 1. 범주형 임베딩\n",
    "        cat_embeds = [embed(x_cat[:, i]) for i, embed in enumerate(self.cat_embeddings)]\n",
    "        \n",
    "        # 2. 수치형 임베딩 (Feature Tokenizer)\n",
    "        cont_embeds = [\n",
    "            embed(x_cont[:, i:i+1]) for i, embed in enumerate(self.cont_embeddings)\n",
    "        ]\n",
    "        \n",
    "        # 3. 모든 임베딩 결합\n",
    "        all_embeds = cat_embeds + cont_embeds\n",
    "        tokens = torch.stack(all_embeds, dim=1)  # (batch, num_features, d_model)\n",
    "        \n",
    "        # 4. [CLS] 토큰 추가\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        tokens = torch.cat([cls_tokens, tokens], dim=1)  # (batch, 1+num_features, d_model)\n",
    "        \n",
    "        # 5. Transformer\n",
    "        transformed = self.transformer(tokens)\n",
    "        \n",
    "        # 6. [CLS] 토큰 출력만 사용\n",
    "        cls_output = transformed[:, 0]  # (batch, d_model)\n",
    "        \n",
    "        # 7. 예측\n",
    "        out = self.output(cls_output)\n",
    "        \n",
    "        return out.squeeze(-1)\n",
    "\n",
    "print(\"FT-Transformer 클래스 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FT-Transformer 테스트\n",
    "\n",
    "model_ft = FTTransformer(\n",
    "    num_categories=num_categories,\n",
    "    num_continuous=num_continuous,\n",
    "    d_model=32,\n",
    "    n_heads=4,\n",
    "    n_layers=2\n",
    ")\n",
    "\n",
    "# Forward\n",
    "output_ft = model_ft(x_cat, x_cont)\n",
    "\n",
    "print(f\"출력: {output_ft.shape}\")\n",
    "print(f\"예측 확률: {output_ft.detach().numpy().round(3)}\")\n",
    "\n",
    "# 파라미터 수 비교\n",
    "params_tab = sum(p.numel() for p in model.parameters())\n",
    "params_ft = sum(p.numel() for p in model_ft.parameters())\n",
    "\n",
    "print(f\"\\nTabTransformer 파라미터: {params_tab:,}\")\n",
    "print(f\"FT-Transformer 파라미터: {params_ft:,}\")\n",
    "print(f\"→ FT-Transformer가 수치형 임베딩으로 인해 더 많음\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. 트리 모델 vs Transformer\n",
    "\n",
    "### 5-1. 2024-2025 벤치마크 결과\n",
    "\n",
    "**\"Why do tree-based models still outperform deep learning on typical tabular data?\"**\n",
    "\n",
    "| 데이터 크기 | 승자 | 이유 |\n",
    "|------------|------|------|\n",
    "| **작은 데이터 (<10K)** | 트리 | DL은 과적합 |\n",
    "| **중간 데이터 (10K-100K)** | 트리 ≈ DL | 비슷 |\n",
    "| **큰 데이터 (>100K)** | DL 유리 | 충분한 학습 |\n",
    "\n",
    "### 5-2. 언제 Transformer가 유리한가?\n",
    "\n",
    "1. **대규모 데이터** (100K+ 샘플)\n",
    "2. **피처 간 복잡한 상호작용**이 있을 때\n",
    "3. **범주형 피처가 많을 때**\n",
    "4. **Semi-supervised Learning** 가능할 때\n",
    "\n",
    "### 5-3. 트리가 유리한 경우\n",
    "\n",
    "1. **작은/중간 데이터**\n",
    "2. **해석 가능성** 중요\n",
    "3. **빠른 추론** 필요\n",
    "4. **튜닝 시간** 제한\n",
    "\n",
    "### 5-4. FDS에서의 선택\n",
    "\n",
    "| 요소 | 분석 | 결론 |\n",
    "|------|------|------|\n",
    "| 데이터 크기 | 590K | Transformer 가능 |\n",
    "| 추론 속도 | 실시간 필요 | 트리 유리 |\n",
    "| 해석 가능성 | 규제 요구 | 트리 유리 (SHAP) |\n",
    "| **최종** | - | **트리 스태킹 + Transformer 실험** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트리 vs Transformer 비교 시각화\n",
    "\n",
    "comparison_data = {\n",
    "    '모델': ['XGBoost', 'LightGBM', 'CatBoost', 'TabTransformer', 'FT-Transformer'],\n",
    "    '학습 속도': [5, 5, 4, 2, 2],\n",
    "    '추론 속도': [5, 5, 4, 3, 3],\n",
    "    '해석 가능성': [5, 5, 5, 2, 2],\n",
    "    '대규모 데이터': [4, 4, 4, 5, 5],\n",
    "    '피처 상호작용': [3, 3, 3, 4, 5]\n",
    "}\n",
    "\n",
    "df_compare = pd.DataFrame(comparison_data)\n",
    "df_compare = df_compare.set_index('모델')\n",
    "\n",
    "# Radar Chart\n",
    "categories = list(df_compare.columns)\n",
    "N = len(categories)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]  # 닫기\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12']\n",
    "\n",
    "for idx, model in enumerate(df_compare.index):\n",
    "    values = df_compare.loc[model].values.flatten().tolist()\n",
    "    values += values[:1]  # 닫기\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.1, color=colors[idx])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=11)\n",
    "ax.set_ylim(0, 5)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.set_title('트리 vs Transformer 비교', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n결론:\")\n",
    "print(\"  - 트리: 속도, 해석 가능성에서 우위\")\n",
    "print(\"  - Transformer: 대규모 데이터, 피처 상호작용에서 우위\")\n",
    "print(\"  - FDS에서는 트리 스태킹이 현실적 선택 (실시간 + SHAP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 면접 Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: \"Self-Attention이 뭔가요?\"\n",
    "\n",
    "> \"입력 시퀀스의 각 요소가 다른 모든 요소와의 관계를 학습하는 메커니즘입니다.\n",
    "> Query, Key, Value를 통해 '어디에 집중할지'를 학습합니다.\n",
    "> 정형 데이터에서는 각 피처가 다른 피처와의 상호작용을 학습합니다.\"\n",
    "\n",
    "### Q: \"TabTransformer와 FT-Transformer의 차이는?\"\n",
    "\n",
    "> \"TabTransformer는 **범주형 피처만** Transformer로 처리하고, 수치형은 MLP에서 처리합니다.\n",
    "> FT-Transformer는 **모든 피처**를 Transformer로 처리합니다.\n",
    "> FT-Transformer가 성능이 더 좋지만, 파라미터가 더 많습니다.\"\n",
    "\n",
    "### Q: \"정형 데이터에서 Transformer가 트리보다 항상 좋나요?\"\n",
    "\n",
    "> \"아닙니다. 2024-2025 벤치마크에서 **작은/중간 데이터에서는 트리가 여전히 우위**입니다.\n",
    "> Transformer는 100K+ 대규모 데이터, 복잡한 피처 상호작용이 있을 때 유리합니다.\n",
    "> 또한 트리는 추론 속도가 빠르고 SHAP으로 해석 가능해서 실시간 FDS에 적합합니다.\"\n",
    "\n",
    "### Q: \"FDS에서 Transformer를 왜 안 썼나요?\"\n",
    "\n",
    "> \"세 가지 이유입니다:\n",
    "> 1. **추론 속도**: 트리(5ms) vs Transformer(50ms+) - 실시간 필요\n",
    "> 2. **해석 가능성**: 금융 규제로 SHAP 설명 필요 - 트리가 적합\n",
    "> 3. **성능**: 59만 건에서 트리 스태킹(F1 0.99)이 충분히 높음\n",
    ">\n",
    "> 다만 Transformer 실험(1-10)으로 비교는 해볼 계획입니다.\"\n",
    "\n",
    "### Q: \"Positional Encoding이 정형 데이터에서 왜 불필요한가요?\"\n",
    "\n",
    "> \"NLP에서 Positional Encoding은 **단어의 순서** 정보를 제공합니다.\n",
    "> 정형 데이터에서 피처는 **순서가 없습니다** - [금액, 시간] ≡ [시간, 금액]\n",
    "> 따라서 Positional Encoding을 추가하면 오히려 불필요한 바이어스가 생깁니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 최종 체크포인트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  1-S10 완료: Transformer for Tabular Data\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"배운 것:\")\n",
    "print()\n",
    "print(\"1. Self-Attention\")\n",
    "print(\"   - Query, Key, Value로 '어디에 집중할지' 학습\")\n",
    "print(\"   - Multi-Head: 여러 관점에서 관계 학습\")\n",
    "print()\n",
    "print(\"2. 정형 데이터에서 Transformer\")\n",
    "print(\"   - Column Embedding: 피처를 토큰처럼 취급\")\n",
    "print(\"   - Positional Encoding 불필요 (순서 없음)\")\n",
    "print()\n",
    "print(\"3. TabTransformer\")\n",
    "print(\"   - 범주형 피처만 Transformer 적용\")\n",
    "print(\"   - 수치형은 MLP에서 처리\")\n",
    "print()\n",
    "print(\"4. FT-Transformer\")\n",
    "print(\"   - 모든 피처를 Transformer로 처리\")\n",
    "print(\"   - [CLS] 토큰으로 예측\")\n",
    "print()\n",
    "print(\"5. 트리 vs Transformer\")\n",
    "print(\"   - 작은/중간 데이터: 트리 우위\")\n",
    "print(\"   - 대규모 데이터: Transformer 유리\")\n",
    "print(\"   - FDS: 트리 스태킹 선택 (속도 + 해석)\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"다음: 1-10 Transformer 구현 (선택)\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
