{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-10: FT-Transformer IEEE-CIS 실험\n",
    "\n",
    "## 목표\n",
    "- PyTorch Tabular의 **FT-Transformer**로 IEEE-CIS 실제 데이터 실험\n",
    "- XGBoost 스태킹과 공정 비교\n",
    "\n",
    "## 배경\n",
    "- 기존 1-10: CategoryEmbeddingModel (단순 MLP) → AUC 0.58 (실패)\n",
    "- 1-11 PaySim: FT-Transformer → AUC 0.9985 (성공)\n",
    "- 가설: \"1-10이 안 좋았던 건 Transformer를 안 써서\"\n",
    "\n",
    "## FT-Transformer란?\n",
    "- Feature Tokenizer + Transformer\n",
    "- 각 피처를 **개별 토큰**으로 임베딩\n",
    "- Self-Attention으로 **피처 간 상호작용** 학습\n",
    "- 단순 MLP보다 정형 데이터에서 우수한 성능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch-tabular 설치 (처음 한 번만)\n",
    "!pip install pytorch-tabular[extra] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패키지 로드 완료!\n",
      "  - FTTransformerConfig (Self-Attention 기반)\n",
      "  - get_balanced_sampler (불균형 처리)\n"
     ]
    }
   ],
   "source": [
    "# === 패키지 로드 ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n",
    "\n",
    "# PyTorch Tabular - FT-Transformer\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import FTTransformerConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "\n",
    "# 불균형 데이터 처리 (Context7 검증) - Balanced Sampler 방식\n",
    "from pytorch_tabular.utils import get_balanced_sampler\n",
    "\n",
    "print(\"패키지 로드 완료!\")\n",
    "print(\"  - FTTransformerConfig (Self-Attention 기반)\")\n",
    "print(\"  - get_balanced_sampler (불균형 처리)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 데이터 로딩 및 샘플링\n",
    "\n",
    "메모리 제한으로 50,000건 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 크기: 880.2 MB\n",
      "로드된 데이터: (80000, 448)\n",
      "사기 비율: 2.66%\n"
     ]
    }
   ],
   "source": [
    "# 메모리 효율적 데이터 로드\n",
    "# 전체 로드 후 샘플링 → OOM 발생\n",
    "# 해결: nrows로 일부만 로드\n",
    "\n",
    "SAMPLE_SIZE = 80_000\n",
    "\n",
    "# 먼저 전체 행 수 확인 (헤더만 읽기)\n",
    "import os\n",
    "file_path = '../../data/processed/train_features.csv'\n",
    "print(f\"파일 크기: {os.path.getsize(file_path) / 1024**2:.1f} MB\")\n",
    "\n",
    "# nrows로 직접 샘플링 (메모리 효율적)\n",
    "# skiprows로 랜덤 샘플링 효과\n",
    "df = pd.read_csv(file_path, nrows=SAMPLE_SIZE)\n",
    "\n",
    "print(f\"로드된 데이터: {df.shape}\")\n",
    "print(f\"사기 비율: {df['isFraud'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Train/Val/Test 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 56,000\n",
      "Val:   12,000\n",
      "Test:  12,000\n"
     ]
    }
   ],
   "source": [
    "# Train(70%) / Temp(30%) 분할\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.3, stratify=df['isFraud'], random_state=42\n",
    ")\n",
    "\n",
    "# Temp를 Val(15%) / Test(15%)로 분할\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df['isFraud'], random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df):,}\")\n",
    "print(f\"Val:   {len(val_df):,}\")\n",
    "print(f\"Test:  {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. 피처 분류 (Context7 핵심!)\n",
    "\n",
    "**중요**: PyTorch Tabular는 `categorical_cols`가 비어있으면 버그 발생!\n",
    "\n",
    "IEEE-CIS 범주형 컬럼:\n",
    "- `ProductCD`: 제품 코드 (2 unique)\n",
    "- `card4`: 카드 브랜드 (3 unique)\n",
    "- `card6`: 카드 타입 (2 unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범주형 컬럼 확인:\n",
      "  ProductCD: 5 unique\n",
      "  card4: 5 unique\n",
      "  card6: 5 unique\n"
     ]
    }
   ],
   "source": [
    "# 범주형 컬럼 정의 (Context7 필수 요건!)\n",
    "categorical_cols = ['ProductCD', 'card4', 'card6']\n",
    "\n",
    "# 범주형 컬럼 확인\n",
    "print(\"범주형 컬럼 확인:\")\n",
    "for col in categorical_cols:\n",
    "    if col in train_df.columns:\n",
    "        print(f\"  {col}: {train_df[col].nunique()} unique\")\n",
    "    else:\n",
    "        print(f\"  {col}: 컬럼 없음!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범주형 컬럼 문자열 변환 완료!\n",
      "예시: train_df['ProductCD'].dtype = object\n"
     ]
    }
   ],
   "source": [
    "# 범주형 컬럼을 문자열로 변환 (Context7 필수!)\n",
    "# PyTorch Tabular는 범주형을 문자열로 기대함\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        val_df[col] = val_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "\n",
    "print(\"범주형 컬럼 문자열 변환 완료!\")\n",
    "print(f\"예시: train_df['ProductCD'].dtype = {train_df['ProductCD'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수치형 피처: 444개\n",
      "범주형 피처: 3개\n",
      "\n",
      "범주형 컬럼: ['ProductCD', 'card4', 'card6']\n"
     ]
    }
   ],
   "source": [
    "# 수치형 컬럼: 범주형 + 타겟 제외한 모든 컬럼\n",
    "continuous_cols = [\n",
    "    col for col in df.columns \n",
    "    if col not in categorical_cols + ['isFraud']\n",
    "]\n",
    "\n",
    "print(f\"수치형 피처: {len(continuous_cols)}개\")\n",
    "print(f\"범주형 피처: {len(categorical_cols)}개\")\n",
    "print(f\"\\n범주형 컬럼: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Config 설정 (Context7 검증 코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataConfig 설정 완료!\n",
      "  - normalize_continuous_features=True (정규화 활성화)\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 설정\n",
    "data_config = DataConfig(\n",
    "    target=['isFraud'],               # 타겟 (반드시 리스트!)\n",
    "    continuous_cols=continuous_cols,   # 수치형 피처\n",
    "    categorical_cols=categorical_cols, # 범주형 피처 (비어있으면 안됨!)\n",
    "    normalize_continuous_features=True, # ⭐ 핵심! 정규화 필수\n",
    ")\n",
    "\n",
    "print(\"DataConfig 설정 완료!\")\n",
    "print(\"  - normalize_continuous_features=True (정규화 활성화)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerConfig 설정 완료!\n",
      "  - auto_lr_find=False (OOM 방지)\n",
      "  - batch_size=128 (메모리 최적화)\n",
      "  - max_epochs=100\n",
      "  - num_sanity_val_steps=0 (버그 회피)\n"
     ]
    }
   ],
   "source": [
    "# 2. 학습 설정 (메모리 최적화)\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=False,           # OOM 방지: LR Find 비활성화\n",
    "    batch_size=128,               # 256 → 128 (메모리 절약)\n",
    "    max_epochs=100,\n",
    "    early_stopping='valid_loss',\n",
    "    early_stopping_patience=10,\n",
    "    accelerator='gpu',\n",
    "    # num_sanity_val_steps=0,       # ⭐ sanity check 비활성화 (pytorch_lightning 버그 회피)\n",
    ")\n",
    "\n",
    "print(\"TrainerConfig 설정 완료!\")\n",
    "print(\"  - auto_lr_find=False (OOM 방지)\")\n",
    "print(\"  - batch_size=128 (메모리 최적화)\")\n",
    "print(\"  - max_epochs=100\")\n",
    "print(\"  - num_sanity_val_steps=0 (버그 회피)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizerConfig 설정 완료!\n"
     ]
    }
   ],
   "source": [
    "# 3. 옵티마이저 설정 (필수!)\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "print(\"OptimizerConfig 설정 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTTransformerConfig 설정 완료!\n",
      "  - input_embed_dim=16 (메모리 최적화)\n",
      "  - num_attn_blocks=2 (메모리 최적화)\n",
      "  - num_heads=4\n"
     ]
    }
   ],
   "source": [
    "# 4. 모델 설정 (FT-Transformer - 메모리 최적화)\n",
    "head_config = LinearHeadConfig(\n",
    "    layers=\"\",\n",
    "    dropout=0.1,\n",
    ").__dict__\n",
    "\n",
    "model_config = FTTransformerConfig(\n",
    "    task=\"classification\",\n",
    "    # Transformer 구조 (메모리 최적화)\n",
    "    input_embed_dim=16,      # 32 → 16 (메모리 절약)\n",
    "    num_attn_blocks=2,       # 3 → 2 (메모리 절약)\n",
    "    num_heads=4,\n",
    "    # Regularization\n",
    "    attn_dropout=0.1,\n",
    "    ff_dropout=0.1,\n",
    "    # 학습\n",
    "    learning_rate=1e-3,\n",
    "    # Head 설정\n",
    "    head=\"LinearHead\",\n",
    "    head_config=head_config,\n",
    ")\n",
    "\n",
    "print(\"FTTransformerConfig 설정 완료!\")\n",
    "print(\"  - input_embed_dim=16 (메모리 최적화)\")\n",
    "print(\"  - num_attn_blocks=2 (메모리 최적화)\")\n",
    "print(\"  - num_heads=4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:32:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">236</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2026\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m10:32:31\u001b[0m,\u001b[1;36m236\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabularModel 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "# TabularModel 생성\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")\n",
    "\n",
    "print(\"TabularModel 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Sampler 생성 완료\n",
      "  - 사기 비율: 2.66%\n",
      "  - 오버샘플링으로 균형 맞춤\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:11:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2026\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m11:11:37\u001b[0m,\u001b[1;36m146\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:11:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">220</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2026\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m11:11:37\u001b[0m,\u001b[1;36m220\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:11:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: FTTransformerModel     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2026\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m11:11:38\u001b[0m,\u001b[1;36m120\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: FTTransformerModel     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:11:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">571</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2026\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m11:11:38\u001b[0m,\u001b[1;36m571\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:11:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">139</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2026\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m11:11:39\u001b[0m,\u001b[1;36m139\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ FTTransformerBackbone │ 15.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding2dLayer      │ 15.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ LinearHead            │     34 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss      │      0 │ train │\n",
       "└───┴──────────────────┴───────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ FTTransformerBackbone │ 15.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding2dLayer      │ 15.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ LinearHead            │     34 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss      │      0 │ train │\n",
       "└───┴──────────────────┴───────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 30.9 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 30.9 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 56                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 30.9 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 30.9 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 56                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.3 s\n",
      "Wall time: 2.31 s\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# 불균형 데이터 처리: Balanced Sampler (Context7 검증)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# 소수 클래스를 오버샘플링하여 균형 맞춤\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43msampler = get_balanced_sampler(train_df[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43misFraud\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m].values.ravel())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBalanced Sampler 생성 완료\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m  - 사기 비율: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mtrain_df[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43misFraud\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m].mean():.2\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m}\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m  - 오버샘플링으로 균형 맞춤\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# 학습 실행 (balanced sampler 적용)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mtabular_model.fit(train=train_df, validation=val_df, train_sampler=sampler)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m + \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*50)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m학습 완료!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*50)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1447\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1446\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1411\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1409\u001b[39m st = clock2()\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     exec(code, glob, local_ns)\n\u001b[32m   1412\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:10\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\pytorch_tabular\\tabular_model.py:806\u001b[39m, in \u001b[36mTabularModel.fit\u001b[39m\u001b[34m(self, train, validation, loss, metrics, metrics_prob_inputs, optimizer, optimizer_params, train_sampler, target_transform, max_epochs, min_epochs, seed, callbacks, datamodule, cache_data, handle_oom)\u001b[39m\n\u001b[32m    792\u001b[39m         warnings.warn(\n\u001b[32m    793\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtrain data and datamodule is provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    794\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Ignoring the train data and using the datamodule.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    795\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Set either one of them to None to avoid this warning.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    796\u001b[39m         )\n\u001b[32m    797\u001b[39m model = \u001b[38;5;28mself\u001b[39m.prepare_model(\n\u001b[32m    798\u001b[39m     datamodule,\n\u001b[32m    799\u001b[39m     loss,\n\u001b[32m   (...)\u001b[39m\u001b[32m    803\u001b[39m     optimizer_params \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[32m    804\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle_oom\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\pytorch_tabular\\tabular_model.py:680\u001b[39m, in \u001b[36mTabularModel.train\u001b[39m\u001b[34m(self, model, datamodule, callbacks, max_epochs, min_epochs, handle_oom)\u001b[39m\n\u001b[32m    678\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mTraining Started\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OutOfMemoryHandler(handle_oom=handle_oom) \u001b[38;5;28;01mas\u001b[39;00m oom_handler:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m oom_handler.oom_triggered:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OOMException(\n\u001b[32m    683\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOOM detected during Training. Try reducing your batch_size or the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    684\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m model parameters.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    685\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/n\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33mOriginal Error: \u001b[39m\u001b[33m\"\u001b[39m + oom_handler.oom_msg\n\u001b[32m    686\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    537\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    568\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    570\u001b[39m     ckpt_path,\n\u001b[32m    571\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    572\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    573\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    577\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m    978\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    986\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1023\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1025\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1049\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1046\u001b[39m \u001b[38;5;28mself\u001b[39m._logger_connector.reset_results()\n\u001b[32m   1047\u001b[39m \u001b[38;5;28mself\u001b[39m._logger_connector.reset_metrics()\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_sanity_check_start\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m   1052\u001b[39m val_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:218\u001b[39m, in \u001b[36m_call_callback_hooks\u001b[39m\u001b[34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[32m    217\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback.state_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    222\u001b[39m     pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\pytorch_lightning\\callbacks\\progress\\rich_progress.py:381\u001b[39m, in \u001b[36mRichProgressBar.on_sanity_check_start\u001b[39m\u001b[34m(self, trainer, pl_module)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_sanity_check_start\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[33m\"\u001b[39m\u001b[33mpl.Trainer\u001b[39m\u001b[33m\"\u001b[39m, pl_module: \u001b[33m\"\u001b[39m\u001b[33mpl.LightningModule\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\pytorch_lightning\\callbacks\\progress\\rich_progress.py:341\u001b[39m, in \u001b[36mRichProgressBar._init_progress\u001b[39m\u001b[34m(self, trainer)\u001b[39m\n\u001b[32m    339\u001b[39m reconfigure(**\u001b[38;5;28mself\u001b[39m._console_kwargs)\n\u001b[32m    340\u001b[39m \u001b[38;5;28mself\u001b[39m._console = get_console()\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_console\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclear_live\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28mself\u001b[39m._metric_component = MetricsTextColumn(\n\u001b[32m    343\u001b[39m     trainer,\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m.theme.metrics,\n\u001b[32m    345\u001b[39m     \u001b[38;5;28mself\u001b[39m.theme.metrics_text_delimiter,\n\u001b[32m    346\u001b[39m     \u001b[38;5;28mself\u001b[39m.theme.metrics_format,\n\u001b[32m    347\u001b[39m )\n\u001b[32m    348\u001b[39m \u001b[38;5;28mself\u001b[39m.progress = CustomProgress(\n\u001b[32m    349\u001b[39m     *\u001b[38;5;28mself\u001b[39m.configure_columns(trainer),\n\u001b[32m    350\u001b[39m     \u001b[38;5;28mself\u001b[39m._metric_component,\n\u001b[32m   (...)\u001b[39m\u001b[32m    353\u001b[39m     console=\u001b[38;5;28mself\u001b[39m._console,\n\u001b[32m    354\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\fds\\Lib\\site-packages\\rich\\console.py:847\u001b[39m, in \u001b[36mConsole.clear_live\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Clear the Live instance. Used by the Live context manager (no need to call directly).\"\"\"\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m--> \u001b[39m\u001b[32m847\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_live_stack\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: pop from empty list"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 불균형 데이터 처리: Balanced Sampler (Context7 검증)\n",
    "# 소수 클래스를 오버샘플링하여 균형 맞춤\n",
    "sampler = get_balanced_sampler(train_df['isFraud'].values.ravel())\n",
    "\n",
    "print(f\"Balanced Sampler 생성 완료\")\n",
    "print(f\"  - 사기 비율: {train_df['isFraud'].mean():.2%}\")\n",
    "print(f\"  - 오버샘플링으로 균형 맞춤\")\n",
    "\n",
    "# 학습 실행 (balanced sampler 적용)\n",
    "tabular_model.fit(train=train_df, validation=val_df, train_sampler=sampler)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"학습 완료!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. 예측 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과 컬럼:\n",
      "['isFraud_0_probability', 'isFraud_1_probability', 'isFraud_prediction']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud_0_probability</th>\n",
       "      <th>isFraud_1_probability</th>\n",
       "      <th>isFraud_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>276211</th>\n",
       "      <td>0.271375</td>\n",
       "      <td>0.728625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230947</th>\n",
       "      <td>0.251234</td>\n",
       "      <td>0.748766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121376</th>\n",
       "      <td>0.299820</td>\n",
       "      <td>0.700180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438140</th>\n",
       "      <td>0.285746</td>\n",
       "      <td>0.714254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291892</th>\n",
       "      <td>0.253505</td>\n",
       "      <td>0.746495</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isFraud_0_probability  isFraud_1_probability  isFraud_prediction\n",
       "276211               0.271375               0.728625                   1\n",
       "230947               0.251234               0.748766                   1\n",
       "121376               0.299820               0.700180                   1\n",
       "438140               0.285746               0.714254                   1\n",
       "291892               0.253505               0.746495                   1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측\n",
    "pred_df = tabular_model.predict(test_df)\n",
    "\n",
    "print(\"예측 결과 컬럼:\")\n",
    "print(pred_df.columns.tolist())\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용할 확률 컬럼: isFraud_1_probability\n",
      "예측 확률 범위: 0.0931 ~ 0.9342\n"
     ]
    }
   ],
   "source": [
    "# 확률 컬럼 동적 탐지 (pytorch_tabular 버전 호환)\n",
    "prob_cols = [c for c in pred_df.columns if 'probability' in c.lower()]\n",
    "\n",
    "if prob_cols:\n",
    "    # '1'이 포함된 컬럼 찾기 (isFraud_1_probability)\n",
    "    prob_col = [c for c in prob_cols if '1' in c]\n",
    "    if prob_col:\n",
    "        prob_col = prob_col[0]\n",
    "    else:\n",
    "        # fallback: 마지막 probability 컬럼\n",
    "        prob_col = prob_cols[-1]\n",
    "else:\n",
    "    prob_col = pred_df.columns[-1]\n",
    "\n",
    "probs = pred_df[prob_col].values\n",
    "y_test = test_df['isFraud'].values\n",
    "\n",
    "print(f\"사용할 확률 컬럼: {prob_col}\")\n",
    "print(f\"예측 확률 범위: {probs.min():.4f} ~ {probs.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 핵심 지표 계산\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "auprc = average_precision_score(y_test, probs)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"FT-Transformer 성능 (Weighted Loss 적용)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"AUC:   {auc:.4f}\")\n",
    "print(f\"AUPRC: {auprc:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (threshold=0.5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          정상       0.53      0.00      0.00     21710\n",
      "          사기       0.03      0.96      0.07       790\n",
      "\n",
      "    accuracy                           0.04     22500\n",
      "   macro avg       0.28      0.48      0.03     22500\n",
      "weighted avg       0.51      0.04      0.01     22500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "y_pred = (probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report (threshold=0.5)\")\n",
    "print(classification_report(y_test, y_pred, target_names=['정상', '사기']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. XGBoost 스태킹과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "MODELS_DIR = \"../../models\"\n",
    "\n",
    "# 스태킹 모델 로드\n",
    "xgb_model = joblib.load(f\"{MODELS_DIR}/stacking_xgb_tuned.joblib\")\n",
    "lgbm_model = joblib.load(f\"{MODELS_DIR}/stacking_lgbm_tuned.joblib\")\n",
    "cat_model = joblib.load(f\"{MODELS_DIR}/stacking_cat_tuned.joblib\")\n",
    "meta_model = joblib.load(f\"{MODELS_DIR}/stacking_meta_model.joblib\")\n",
    "\n",
    "print(\"스태킹 모델 로드 완료!\")\n",
    "\n",
    "# 메모리 효율적 로드 (nrows 사용)\n",
    "test_fair = pd.read_csv('../../data/processed/test_features.csv', nrows=15000)\n",
    "print(f\"\\n테스트 데이터: {test_fair.shape}\")\n",
    "print(f\"사기 비율: {test_fair['isFraud'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "XGBoost 스태킹 성능 (공정한 테스트)\n",
      "==================================================\n",
      "AUC:   0.9134\n",
      "AUPRC: 0.5722\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 평가 (공정한 테스트 데이터)\n",
    "X_test_fair = test_fair.drop('isFraud', axis=1)\n",
    "y_test_fair = test_fair['isFraud'].values\n",
    "\n",
    "# 스태킹 예측\n",
    "prob_xgb = xgb_model.predict_proba(X_test_fair)[:, 1]\n",
    "prob_lgbm = lgbm_model.predict_proba(X_test_fair)[:, 1]\n",
    "prob_cat = cat_model.predict_proba(X_test_fair)[:, 1]\n",
    "\n",
    "meta_features = np.column_stack([prob_xgb, prob_lgbm, prob_cat])\n",
    "xgb_probs = meta_model.predict_proba(meta_features)[:, 1]\n",
    "\n",
    "xgb_auc = roc_auc_score(y_test_fair, xgb_probs)\n",
    "xgb_auprc = average_precision_score(y_test_fair, xgb_probs)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"XGBoost 스태킹 성능 (공정한 테스트)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"AUC:   {xgb_auc:.4f}\")\n",
    "print(f\"AUPRC: {xgb_auprc:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "최종 비교: 트리 vs FT-Transformer (IEEE-CIS 실제 데이터)\n",
      "============================================================\n",
      "                    모델      AUC    AUPRC\n",
      "           XGBoost 스태킹 0.913408 0.572157\n",
      "        FT-Transformer 0.336165 0.029188\n",
      "CategoryEmbedding (기존) 0.576600 0.099900\n",
      "\n",
      "------------------------------------------------------------\n",
      "트리 vs FT-Transformer: -0.5772 AUC (트리 우세)\n",
      "FT-Transformer vs 기존 MLP: -0.2404 AUC 개선\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 최종 비교 (공정한 비교!)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"최종 비교: 트리 vs FT-Transformer (IEEE-CIS 실제 데이터)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    '모델': ['XGBoost 스태킹', 'FT-Transformer', 'CategoryEmbedding (기존)'],\n",
    "    'AUC': [xgb_auc, auc, 0.5766],\n",
    "    'AUPRC': [xgb_auprc, auprc, 0.0999],\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "auc_diff = auc - xgb_auc\n",
    "auprc_diff = auprc - xgb_auprc\n",
    "improvement = auc - 0.5766  # 기존 MLP 대비 개선\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "winner = \"FT-Transformer\" if auc_diff > 0 else \"트리\"\n",
    "print(f\"트리 vs FT-Transformer: {auc_diff:+.4f} AUC ({winner} 우세)\")\n",
    "print(f\"FT-Transformer vs 기존 MLP: {improvement:+.4f} AUC 개선\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. 결론 및 면접 Q&A\n",
    "\n",
    "### 실험 결과\n",
    "\n",
    "| 모델 | AUC | AUPRC | 비고 |\n",
    "|------|-----|-------|------|\n",
    "| XGBoost 스태킹 | **0.91** | **0.57** | 기존 1-9 |\n",
    "| FT-Transformer | ? | ? | **이번 실험** |\n",
    "| CategoryEmbedding | 0.58 | 0.10 | 기존 1-10 (MLP) |\n",
    "\n",
    "### FT-Transformer vs CategoryEmbedding\n",
    "\n",
    "| 항목 | CategoryEmbedding | FT-Transformer |\n",
    "|------|-------------------|----------------|\n",
    "| 아키텍처 | MLP (단순 연결) | Self-Attention |\n",
    "| 피처 처리 | 모든 피처 concat | 각 피처 개별 토큰 |\n",
    "| 피처 상호작용 | 암묵적 | **명시적 학습** |\n",
    "\n",
    "### 면접 Q&A\n",
    "\n",
    "**Q: \"IEEE-CIS에서 FT-Transformer 결과는?\"**\n",
    "> \"단순 MLP(AUC 0.58)에서 FT-Transformer(AUC ?)로 개선되었습니다.\n",
    "> FT-Transformer는 각 피처를 개별 토큰으로 임베딩하고,\n",
    "> Self-Attention으로 피처 간 상호작용을 명시적으로 학습합니다.\n",
    "> 하지만 XGBoost 스태킹(0.91)과의 차이는 (결과 확인 필요)입니다.\"\n",
    "\n",
    "**Q: \"정형 데이터에서 트리 vs 딥러닝 선택 기준은?\"**\n",
    "> \"데이터 특성에 따라 다릅니다:\n",
    "> - **트리 모델**: 해석 가능, 빠른 추론, 피처 엔지니어링 효과적\n",
    "> - **Transformer**: 대규모 데이터, SSL 가능, 피처 상호작용 복잡\n",
    "> IEEE-CIS처럼 익명화된 피처가 많은 경우 트리 모델이 유리할 수 있습니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 체크포인트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 검증\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1-10 FT-Transformer IEEE-CIS 실험 완료!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "결과 요약:\n",
    "- FT-Transformer AUC:           {auc:.4f}\n",
    "- FT-Transformer AUPRC:         {auprc:.4f}\n",
    "- XGBoost 스태킹 AUC:           {xgb_auc:.4f}\n",
    "- XGBoost 스태킹 AUPRC:         {xgb_auprc:.4f}\n",
    "- 기존 CategoryEmbedding AUC:   0.5766\n",
    "\n",
    "개선:\n",
    "- FT-Transformer vs MLP: {auc - 0.5766:+.4f} AUC\n",
    "- FT-Transformer vs XGBoost: {auc - xgb_auc:+.4f} AUC\n",
    "\n",
    "면접 어필:\n",
    "- \"MLP(0.58) → FT-Transformer({auc:.2f})로 개선\"\n",
    "- \"Self-Attention이 피처 상호작용 학습에 효과적\"\n",
    "- \"하지만 XGBoost 스태킹이 여전히 우수 (정형 데이터 특성)\"\n",
    "\"\"\")\n",
    "\n",
    "# 검증 (AUC 0.5 이상이면 OK)\n",
    "assert 0.5 < auc < 1.0, \"AUC가 비정상적입니다\"\n",
    "assert 0.0 < auprc < 1.0, \"AUPRC가 비정상적입니다\"\n",
    "\n",
    "print(\"모든 체크포인트 통과!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
