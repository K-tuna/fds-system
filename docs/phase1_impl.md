# Phase 1: FDS + Ensemble + XAI - êµ¬í˜„ ìƒì„¸ (AIìš©)

> ë…¸íŠ¸ë¶/ì½”ë“œ ìƒì„±ì„ ìœ„í•œ ìƒì„¸ ìŠ¤í™

---

## âš ï¸ êµ¬í˜„ ë°©ì‹ (í•„ë…)

**ê° Day ë…¸íŠ¸ë¶ì€ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬í˜„í•´ì•¼ í•¨:**

### ë…¸íŠ¸ë¶ êµ¬ì¡°

```
[ë§ˆí¬ë‹¤ìš´] ì œëª© + í•™ìŠµ ëª©í‘œ
[ì½”ë“œ] íŒ¨í‚¤ì§€ ì„í¬íŠ¸
[ë§ˆí¬ë‹¤ìš´] ğŸ“š ê°œë… ì„¤ëª…
[ì½”ë“œ] ì˜ˆì œ ì½”ë“œ (ì™„ì„±ë³¸)
[ë§ˆí¬ë‹¤ìš´] ğŸ’» ì‹¤ìŠµ N ì„¤ëª…
[ì½”ë“œ] ì‹¤ìŠµ TODO (ë¹ˆì¹¸)
[ì½”ë“œ] âœ… ì‹¤ìŠµ ì •ë‹µ (ì±„ìš´ ë²„ì „)
[ì½”ë“œ] ì²´í¬í¬ì¸íŠ¸ (assert)
... (ë°˜ë³µ)
[ë§ˆí¬ë‹¤ìš´] ìµœì¢… ìš”ì•½ + ë©´ì ‘ í¬ì¸íŠ¸
```

### í¬í•¨ ìš”ì†Œ

| ìš”ì†Œ | ì„¤ëª… |
|------|------|
| ğŸ“š ê°œë… ì„¤ëª… | ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ê°œë…/ì´ë¡  ì„¤ëª… |
| ì˜ˆì œ ì½”ë“œ | ì™„ì„±ëœ ì˜ˆì œ (í•™ìŠµìš©) |
| ğŸ’» ì‹¤ìŠµ TODO | ë¹ˆì¹¸/TODO í¬í•¨ëœ ì‹¤ìŠµ ì½”ë“œ |
| âœ… ì‹¤ìŠµ ì •ë‹µ | TODO ì±„ìš´ ì •ë‹µ ì½”ë“œ |
| ì²´í¬í¬ì¸íŠ¸ | assertë¡œ ê²€ì¦ |
| ë©´ì ‘ í¬ì¸íŠ¸ | í•´ë‹¹ Day ê´€ë ¨ ë©´ì ‘ Q&A |

### src/ ëª¨ë“ˆí™”

1-2ë¶€í„° ê²€ì¦ëœ ì½”ë“œë¥¼ src/ë¡œ ëª¨ë“ˆí™”:

```
ë…¸íŠ¸ë¶ì—ì„œ ì½”ë“œ ì‘ì„± + ì‹¤í—˜
    â†“
ê²€ì¦ë˜ë©´ src/ë¡œ ëª¨ë“ˆí™”
    â†“
ë…¸íŠ¸ë¶ì—ì„œ ëª¨ë“ˆ importí•´ì„œ ì‚¬ìš©
```

---

## íŒŒì¼ êµ¬ì¡°

```
fds-system/
â”œâ”€â”€ notebooks/                   # ì‹¤í—˜ìš© ë…¸íŠ¸ë¶
â”‚   â””â”€â”€ phase1/
â”‚       â”œâ”€â”€ 1-1_data_eda.ipynb
â”‚       â”œâ”€â”€ 1-2_feature_engineering.ipynb
â”‚       â”œâ”€â”€ 1-3_xgboost.ipynb
â”‚       â”œâ”€â”€ 1-4_lstm.ipynb
â”‚       â”œâ”€â”€ 1-5_ensemble.ipynb
â”‚       â”œâ”€â”€ 1-6_shap.ipynb
â”‚       â”œâ”€â”€ 1-7_fastapi.ipynb
â”‚       â”œâ”€â”€ 1-8_react_admin.md   # React Admin ê°€ì´ë“œ
â”‚       â”œâ”€â”€ 1-9_tree_stacking.ipynb   # â­â­ íŠ¸ë¦¬ ìŠ¤íƒœí‚¹ (í•„ìˆ˜)
â”‚       â”œâ”€â”€ 1-10_transformer.ipynb    # Transformer (ì„ íƒ)
â”‚       â”œâ”€â”€ 1-11_hybrid.ipynb         # í•˜ì´ë¸Œë¦¬ë“œ DL+XGB (ì„ íƒ)
â”‚       â””â”€â”€ 1-12_paysim.ipynb         # PaySim ì‹œí€€ìŠ¤ ì‹¤í—˜ (ì„ íƒ)
â”‚
â”œâ”€â”€ src/                         # í”„ë¡œë•ì…˜ ì½”ë“œ
â”‚   â”œâ”€â”€ models/                  # â­ PyTorch í´ë˜ìŠ¤ ì •ì˜ (í•„ìˆ˜!)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lstm.py              # FraudLSTM í´ë˜ìŠ¤
â”‚   â”‚   â”œâ”€â”€ cnn_lstm.py          # CNN-LSTM í´ë˜ìŠ¤ (ì„ íƒ)
â”‚   â”‚   â””â”€â”€ fusion.py            # ìœµí•© ëª¨ë¸ í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ preprocessing.py     # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ train.py             # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ explainer/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ shap_explainer.py
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py
â”‚       â””â”€â”€ schemas.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # IEEE-CIS ì›ë³¸
â”‚   â””â”€â”€ processed/               # ì „ì²˜ë¦¬ ë°ì´í„°
â”‚
â”œâ”€â”€ models/                      # ì €ì¥ëœ ê°€ì¤‘ì¹˜
â”‚   â”œâ”€â”€ xgb_model.pkl            # XGBoost (joblib)
â”‚   â””â”€â”€ lstm_model.pt            # LSTM (torch.save)
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml              # í•˜ì´í¼íŒŒë¼ë¯¸í„°, í”¼ì²˜ ëª©ë¡
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

### âš ï¸ PyTorch ëª¨ë¸ ì €ì¥/ë¡œë“œ ë°©ì‹ (í˜„ì—… í•„ìˆ˜)

```python
# âŒ ë…¸íŠ¸ë¶ì—ì„œë§Œ í´ë˜ìŠ¤ ì •ì˜ â†’ APIì—ì„œ ë¡œë“œ ë¶ˆê°€
# torch.load()ëŠ” í´ë˜ìŠ¤ ì •ì˜ê°€ í•„ìš”í•¨

# âœ… í˜„ì—… ë°©ì‹: src/models/ì— í´ë˜ìŠ¤ ì •ì˜
# 1. src/models/lstm.pyì— FraudLSTM í´ë˜ìŠ¤ ì •ì˜
# 2. ë…¸íŠ¸ë¶ì—ì„œ from src.models.lstm import FraudLSTM
# 3. APIì—ì„œë„ ë™ì¼í•˜ê²Œ import

# ì €ì¥
torch.save(model.state_dict(), 'models/lstm_model.pt')

# ë¡œë“œ (API)
from src.models.lstm import FraudLSTM
model = FraudLSTM(input_size=35, hidden_size=64)
model.load_state_dict(torch.load('models/lstm_model.pt'))
```

---

## 1-1: ë°ì´í„° + EDA (Day 1)

### í•„ìš” íŒ¨í‚¤ì§€
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. IEEE-CIS ë°ì´í„°ì…‹**
- Kaggleì—ì„œ ë‹¤ìš´ë¡œë“œ
- Transaction í…Œì´ë¸”: ê±°ë˜ ì •ë³´ (ê¸ˆì•¡, ì‹œê°„, ì¹´ë“œ)
- Identity í…Œì´ë¸”: ê¸°ê¸°/ë¸Œë¼ìš°ì € ì •ë³´
- TransactionIDë¡œ LEFT JOIN

**2. ê¸°ë³¸ EDA**
- shape, dtypes, info()
- head(), tail()
- describe()

**3. ë¶ˆê· í˜• ë°ì´í„°**
- íƒ€ê²Ÿ(isFraud) ë¶„í¬ í™•ì¸
- ì‚¬ê¸° ë¹„ìœ¨ ~3.5%
- Accuracyê°€ ë¬´ì˜ë¯¸í•œ ì´ìœ 
- í‰ê°€ ì§€í‘œ: AUC-ROC, PR-AUC, F1

**4. ê²°ì¸¡ì¹˜ ë¶„ì„**
- ì»¬ëŸ¼ë³„ ê²°ì¸¡ ë¹„ìœ¨
- ê²°ì¸¡ 50% ì´ìƒ ì»¬ëŸ¼ ì²˜ë¦¬ ì „ëµ
- Identity í…Œì´ë¸” ê²°ì¸¡ (ë³‘í•©ìœ¼ë¡œ ì¸í•œ)

**5. í”¼ì²˜ íƒìƒ‰**
- ìˆ˜ì¹˜í˜•: TransactionAmt, card1~5
- ë²”ì£¼í˜•: ProductCD, card4, card6
- ì‹œê°„: TransactionDT

**6. íƒ€ê²Ÿë³„ ë¶„ì„**
- ì •ìƒ vs ì‚¬ê¸° ê¸ˆì•¡ ë¶„í¬
- ì‹œê°„ëŒ€ë³„ ì‚¬ê¸° ë¹„ìœ¨
- ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ê¸° ë¹„ìœ¨

**7. ì‹œê°„ ê¸°ë°˜ ë¶„í• **
- ì™œ ëœë¤ ë¶„í• ì´ ì•ˆ ë˜ëŠ”ì§€ (Data Leakage)
- TransactionDT ê¸°ì¤€ ì •ë ¬
- 80/20 ë¶„í• 

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: ë°ì´í„° ë¡œë“œ ë° ë³‘í•© (LEFT JOIN)
- ì‹¤ìŠµ 2: íƒ€ê²Ÿ ë¶ˆê· í˜• ì‹œê°í™”
- ì‹¤ìŠµ 3: ê²°ì¸¡ì¹˜ ë¶„ì„ ë° ì²˜ë¦¬ ì „ëµ
- ì‹¤ìŠµ 4: íƒ€ê²Ÿë³„ ê¸ˆì•¡ ë¶„í¬ ë¹„êµ
- ì‹¤ìŠµ 5: ì‹œê°„ ê¸°ë°˜ train/test ë¶„í• 

### í•µì‹¬ ì½”ë“œ

```python
# ë°ì´í„° ë³‘í•©
df = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')

# ì‹œê°„ ê¸°ë°˜ ë¶„í• 
df_sorted = df.sort_values('TransactionDT')
split_idx = int(len(df_sorted) * 0.8)
train_df = df_sorted.iloc[:split_idx]
test_df = df_sorted.iloc[split_idx:]

# ê²€ì¦: ì‹œê°„ ìˆœì„œ í™•ì¸
assert train_df['TransactionDT'].max() <= test_df['TransactionDT'].min()
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ ëœë¤ ë¶„í• ì´ ì•„ë‹Œ ì‹œê°„ ê¸°ë°˜ ë¶„í• ì„ í•˜ë‚˜ìš”?"
> "ì‹¤ì œ FDSëŠ” ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµí•´ì„œ ë¯¸ë˜ ê±°ë˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ëœë¤ ë¶„í• ì€ ë¯¸ë˜ ì •ë³´ê°€ í•™ìŠµì— í¬í•¨ë˜ì–´ Data Leakageê°€ ë°œìƒí•©ë‹ˆë‹¤. ì‹œê°„ ê¸°ë°˜ ë¶„í• ì´ ì‹¤ì œ ìš´ì˜ í™˜ê²½ì„ ë°˜ì˜í•©ë‹ˆë‹¤."

---

## 1-2: Feature Engineering (Day 2)

### í•„ìš” íŒ¨í‚¤ì§€
```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì •í˜• í”¼ì²˜ (XGBoostìš©)**
- ì‹œê°„ í”¼ì²˜: hour, dayofweek, is_weekend, is_night
- ê¸ˆì•¡ í”¼ì²˜: amt_log, amt_bin, amt_decimal
- ì§‘ê³„ í”¼ì²˜: card1ë³„ ê±°ë˜ íšŸìˆ˜, í‰ê·  ê¸ˆì•¡
- ë²”ì£¼í˜• ì¸ì½”ë”©: LabelEncoder

**2. ì‹œê³„ì—´ í”¼ì²˜ (LSTMìš©)** â­
- ì‚¬ìš©ìë³„ ìµœê·¼ Nê°œ ê±°ë˜ ì‹œí€€ìŠ¤ ì¶”ì¶œ
- ì‹œí€€ìŠ¤ í”¼ì²˜: [ê¸ˆì•¡, ì‹œê°„ì°¨, ì¹´í…Œê³ ë¦¬, ...]
- Padding: ê±°ë˜ ìˆ˜ê°€ N ë¯¸ë§Œì´ë©´ 0ìœ¼ë¡œ ì±„ì›€
- Scaling: MinMaxScalerë¡œ 0~1 ì •ê·œí™”

**3. ì‹œí€€ìŠ¤ ìƒì„± ë¡œì§**
```
ì‚¬ìš©ì Aì˜ ê±°ë˜: [t1, t2, t3, t4, t5]
ì‹œí€€ìŠ¤ ê¸¸ì´ N=3ì¼ ë•Œ:
- t3 ì˜ˆì¸¡ìš©: [t1, t2] â†’ ê¸¸ì´ ë¶€ì¡± â†’ [0, t1, t2]
- t4 ì˜ˆì¸¡ìš©: [t1, t2, t3]
- t5 ì˜ˆì¸¡ìš©: [t2, t3, t4]
```

**4. í”¼ì²˜ ì €ì¥**
- X_tabular: ì •í˜• í”¼ì²˜ (DataFrame)
- X_sequence: ì‹œê³„ì—´ í”¼ì²˜ (3D array: samples x seq_len x features)

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: ì‹œê°„ í”¼ì²˜ ìƒì„±
- ì‹¤ìŠµ 2: ê¸ˆì•¡ í”¼ì²˜ ìƒì„±
- ì‹¤ìŠµ 3: ì§‘ê³„ í”¼ì²˜ ìƒì„±
- ì‹¤ìŠµ 4: ë²”ì£¼í˜• ì¸ì½”ë”©
- ì‹¤ìŠµ 5: ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± â­

### í•µì‹¬ ì½”ë“œ: ì‹œí€€ìŠ¤ ìƒì„±

```python
def create_sequences(df, user_col, features, seq_len=10):
    """ì‚¬ìš©ìë³„ ì‹œí€€ìŠ¤ ìƒì„±"""
    sequences = []
    labels = []

    for user_id, group in df.groupby(user_col):
        group = group.sort_values('TransactionDT')

        for i in range(len(group)):
            # í˜„ì¬ ê±°ë˜ ì´ì „ seq_lenê°œ ê±°ë˜
            start_idx = max(0, i - seq_len)
            seq = group.iloc[start_idx:i][features].values

            # íŒ¨ë”© (ê¸¸ì´ ë¶€ì¡± ì‹œ)
            if len(seq) < seq_len:
                pad = np.zeros((seq_len - len(seq), len(features)))
                seq = np.vstack([pad, seq])

            sequences.append(seq)
            labels.append(group.iloc[i]['isFraud'])

    return np.array(sequences), np.array(labels)

# ì‹œí€€ìŠ¤ í”¼ì²˜
seq_features = ['TransactionAmt_scaled', 'hour_scaled', 'time_diff_scaled']
X_seq, y_seq = create_sequences(train_df, 'card1', seq_features, seq_len=10)
print(f"Sequence shape: {X_seq.shape}")  # (samples, 10, 3)
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì‹œí€€ìŠ¤ ê¸¸ì´ 10ì€ ì–´ë–»ê²Œ ì •í–ˆë‚˜ìš”?"
> "5, 10, 15, 20ìœ¼ë¡œ ì‹¤í—˜í–ˆìŠµë‹ˆë‹¤. 10ì´ AUCì™€ í•™ìŠµ ì‹œê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ì—ì„œ ìµœì ì´ì—ˆìŠµë‹ˆë‹¤. 5ëŠ” íŒ¨í„´ì„ ëª» ì¡ê³ , 20ì€ í•™ìŠµ ì‹œê°„ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒì´ ë¯¸ë¯¸í–ˆìŠµë‹ˆë‹¤."

---

## 1-3: XGBoost ëª¨ë¸ (Day 3) â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score, precision_recall_curve
import optuna
import joblib
import time
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ëª¨ë¸ ë¹„êµ ì‹¤í—˜** â­
- XGBoost, LightGBM, CatBoost
- ë™ì¼ ì¡°ê±´ (ë™ì¼ í”¼ì²˜, ë™ì¼ ë¶„í• )
- AUC, í•™ìŠµ ì‹œê°„ ì¸¡ì •
- ê²°ê³¼ í‘œë¡œ ì •ë¦¬

**2. XGBoost ì„ íƒ ì´ìœ **
- AUC ìµœê³ 
- SHAP TreeExplainer í˜¸í™˜ì„± ìµœìƒ
- GPU í•™ìŠµ ì§€ì›

**3. Threshold ìµœì í™”** â­
- FN:FP ë¹„ìš© ë¹„ìœ¨ ì •ì˜ (10:1)
- ë¹„ìš© í•¨ìˆ˜ë¡œ ìµœì  threshold ì°¾ê¸°
- Precision-Recall Curve

**4. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
- íƒìƒ‰ ê³µê°„ ì •ì˜
- objective í•¨ìˆ˜
- n_trials ì„¤ì •

**5. n_estimatorsì™€ Early Stopping** â­
- í˜„ì—… íŠ¸ë¦¬ ê°œìˆ˜: 100~500ê°œ (ìƒí™©ë³„ ìƒì´)
- Early Stoppingìœ¼ë¡œ ìµœì  ê°œìˆ˜ ìë™ íƒìƒ‰
- GPU vs CPU ì„ íƒ ê¸°ì¤€

| ìƒí™© | íŠ¸ë¦¬ ìˆ˜ | ì´ìœ  |
|------|---------|------|
| ë¹ ë¥¸ ì‹¤í—˜ | 100 | ë² ì´ìŠ¤ë¼ì¸ |
| í”„ë¡œë•ì…˜ FDS | 200~500 | ì„±ëŠ¥ vs ì†ë„ ê· í˜• |
| Kaggle ëŒ€íšŒ | 1000+ | ìµœê³  ì„±ëŠ¥ |
| ì‹¤ì‹œê°„ API | 50~200 | ì‘ë‹µì†ë„ ì¤‘ìš” |

**6. ëª¨ë¸ ì €ì¥**
- joblib.dump
- ë©”íƒ€ë°ì´í„° í•¨ê»˜ ì €ì¥

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: 3ê°œ ëª¨ë¸ ë¹„êµ ì‹¤í—˜ â†’ ê²°ê³¼ í‘œ
- ì‹¤ìŠµ 2: XGBoost í•™ìŠµ
- ì‹¤ìŠµ 3: Threshold ë¹„ìš© ë¶„ì„ â†’ ê·¸ë˜í”„
- ì‹¤ìŠµ 4: Optuna íŠœë‹
- ì‹¤ìŠµ 5: ëª¨ë¸ ì €ì¥

### í•µì‹¬ ì½”ë“œ: ëª¨ë¸ ë¹„êµ

```python
models = {
    'XGBoost': XGBClassifier(n_estimators=100, tree_method='hist', device='cuda'),
    'LightGBM': LGBMClassifier(n_estimators=100, device='gpu'),
    'CatBoost': CatBoostClassifier(n_estimators=100, task_type='GPU', verbose=0),
}

results = []
for name, model in models.items():
    start = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start

    y_prob = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_prob)

    results.append({'Model': name, 'AUC': auc, 'Time(s)': train_time})

results_df = pd.DataFrame(results)
print(results_df.to_markdown(index=False))
```

### í•µì‹¬ ì½”ë“œ: Threshold ìµœì í™”

```python
def calculate_cost(threshold, y_true, y_prob, fn_cost=10, fp_cost=1):
    """ë¹„ìš© í•¨ìˆ˜: FNì´ FPë³´ë‹¤ 10ë°° ë¹„ìŒˆ"""
    y_pred = (y_prob >= threshold).astype(int)
    fn = ((y_true == 1) & (y_pred == 0)).sum()
    fp = ((y_true == 0) & (y_pred == 1)).sum()
    return fn * fn_cost + fp * fp_cost

# ìµœì  threshold ì°¾ê¸°
thresholds = np.arange(0.1, 0.9, 0.05)
costs = [calculate_cost(t, y_test, y_prob) for t in thresholds]
optimal_threshold = thresholds[np.argmin(costs)]
print(f"ìµœì  Threshold: {optimal_threshold:.2f}")
```

### í•µì‹¬ ì½”ë“œ: Early Stopping

```python
# Early Stoppingìœ¼ë¡œ ìµœì  íŠ¸ë¦¬ ê°œìˆ˜ ì°¾ê¸°
model = XGBClassifier(
    n_estimators=1000,           # ì¼ë‹¨ ë§ì´
    early_stopping_rounds=50,    # 50ë²ˆ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
    eval_metric='auc',
    device='cuda',
    random_state=42
)

model.fit(
    X_train, y_train,
    eval_set=[(X_valid, y_valid)],
    verbose=100  # 100ë²ˆë§ˆë‹¤ ì¶œë ¥
)

print(f"ì‹¤ì œ ì‚¬ìš©ëœ íŠ¸ë¦¬ ìˆ˜: {model.best_iteration}")  # ì˜ˆ: 287
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ XGBoostë¥¼ ì„ íƒí–ˆë‚˜ìš”?"
> "3ê°œ ëª¨ë¸ì„ ë™ì¼ ì¡°ê±´ì—ì„œ ë¹„êµí–ˆìŠµë‹ˆë‹¤. XGBoostê°€ AUC 0.92ë¡œ ê°€ì¥ ë†’ì•˜ê³ , SHAP TreeExplainer í˜¸í™˜ì„±ë„ ìµœìƒì´ì—ˆìŠµë‹ˆë‹¤."

Q: "ThresholdëŠ” ì–´ë–»ê²Œ ì •í–ˆë‚˜ìš”?"
> "FDSì—ì„œ FN(ë†“ì¹œ ì‚¬ê¸°)ì´ FP(ì˜¤íƒ)ë³´ë‹¤ ë¹„ìš©ì´ í½ë‹ˆë‹¤. FN:FP = 10:1ë¡œ ë¹„ìš© í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ìµœì†Œí™”í•˜ëŠ” Threshold 0.18ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì´ Thresholdì—ì„œ Recall 90.55%ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤."

Q: "n_estimatorsëŠ” ì–´ë–»ê²Œ ì •í–ˆë‚˜ìš”?"
> "Early Stoppingì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. n_estimators=1000ìœ¼ë¡œ ì„¤ì •í•˜ê³ , validation AUCê°€ 50 epoch ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” ì•½ 300ê°œ íŠ¸ë¦¬ì—ì„œ ìˆ˜ë ´í–ˆìŠµë‹ˆë‹¤."

Q: "GPUë¥¼ ì•ˆ ì“°ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
> "28ë§Œê±´ ë°ì´í„°ì—ì„œëŠ” CPUë„ ì¶©ë¶„íˆ ë¹ ë¦…ë‹ˆë‹¤ (0.6ì´ˆ). GPU ì˜¤ë²„í—¤ë“œ ë•Œë¬¸ì— ì˜¤íˆë ¤ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆ˜ë°±ë§Œê±´ ì´ìƒì—ì„œ GPUê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤."

---

## 1-4: LSTM ëª¨ë¸ (Day 4) â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì™œ LSTMì´ í•„ìš”í•œê°€?**
- XGBoostëŠ” ë‹¨ì¼ ê±°ë˜ë§Œ ë´„
- ì‚¬ê¸° íŒ¨í„´: "ì†Œì•¡ â†’ ì†Œì•¡ â†’ ê³ ì•¡"
- ì‹œí€€ìŠ¤ íŒ¨í„´ì€ LSTMì´ í•™ìŠµ

**2. LSTM í”¼ì²˜ ì„ íƒ (í˜„ì—… ë°©ì‹)** â­

```python
# 1. Vì»¬ëŸ¼ ìƒìœ„ 20ê°œ (PCA ê¸°ë°˜, ë…¼ë¬¸ í‘œì¤€)
v_features = [f'V{i}' for i in range(1, 21)]  # V1~V20

# 2. XGBoost importance ìƒìœ„ 10ê°œ (V ì œì™¸)
# â†’ 1-3ì—ì„œ ì €ì¥í•œ feature_importance í™œìš©
xgb_importance = pd.read_csv('data/processed/xgb_importance.csv')
xgb_top = xgb_importance[~xgb_importance['feature'].str.startswith('V')].head(10)['feature'].tolist()

# 3. ì‹œê³„ì—´ í”¼ì²˜ (ì§ì ‘ ìƒì„±)
time_features = [
    'amt_log',              # ê¸ˆì•¡ ë¡œê·¸
    'hour',                 # ì‹œê°„
    'dayofweek',            # ìš”ì¼
    'time_since_last_tx',   # ì´ì „ ê±°ë˜ í›„ ê²½ê³¼ ì‹œê°„
    'rolling_avg_amt_5',    # ìµœê·¼ 5ê°œ í‰ê·  ê¸ˆì•¡
    'tx_count_24h',         # 24ì‹œê°„ ë‚´ ê±°ë˜ íšŸìˆ˜
]

# ì „ì²´ ì‹œí€€ìŠ¤ í”¼ì²˜ (~35ê°œ)
SEQ_FEATURES = v_features + xgb_top + time_features
print(f"ì´ í”¼ì²˜ ìˆ˜: {len(SEQ_FEATURES)}")  # ~35ê°œ
```

**3. LSTM êµ¬ì¡°**
- Input: (batch, seq_len, 35)  # 35ê°œ í”¼ì²˜
- Hidden: 64~128
- Output: 1 (sigmoid)
- pos_weight: ~27 (í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬)

**4. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬**

```python
# ì‚¬ê¸° ë¹„ìœ¨ 3.5% â†’ pos_weight ê³„ì‚°
n_pos = (y_train == 1).sum()
n_neg = (y_train == 0).sum()
pos_weight = n_neg / n_pos  # ~27

criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))
```

**5. PyTorch êµ¬í˜„**
- Dataset í´ë˜ìŠ¤
- DataLoader
- BCEWithLogitsLoss + Adam
- Early Stopping

**6. í•™ìŠµ ë£¨í”„**
- Train/Valid ë¶„ë¦¬
- Epochë³„ loss/AUC ì¶”ì 
- Best model ì €ì¥

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: í”¼ì²˜ ì„ íƒ (V1~V20 + XGBoost importance + ì‹œê³„ì—´)
- ì‹¤ìŠµ 2: Dataset í´ë˜ìŠ¤ êµ¬í˜„
- ì‹¤ìŠµ 3: LSTM ëª¨ë¸ ì •ì˜ (src/models/lstm.py)
- ì‹¤ìŠµ 4: í•™ìŠµ ë£¨í”„ + pos_weight
- ì‹¤ìŠµ 5: Optuna íŠœë‹
- ì‹¤ìŠµ 6: ëª¨ë¸ í‰ê°€ ë° ì €ì¥

### í•µì‹¬ ì½”ë“œ: LSTM ëª¨ë¸

```python
class FraudLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x: (batch, seq_len, features)
        _, (h_n, _) = self.lstm(x)
        # h_n: (num_layers, batch, hidden)
        out = self.fc(h_n[-1])
        return self.sigmoid(out)

# ëª¨ë¸ ì´ˆê¸°í™”
model = FraudLSTM(input_size=3, hidden_size=64)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

### í•µì‹¬ ì½”ë“œ: Dataset

```python
class FraudSequenceDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = torch.FloatTensor(sequences)
        self.labels = torch.FloatTensor(labels).unsqueeze(1)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]

# DataLoader
train_dataset = FraudSequenceDataset(X_seq_train, y_seq_train)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
```

### í•µì‹¬ ì½”ë“œ: í•™ìŠµ ë£¨í”„

```python
def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    for X, y in loader:
        X, y = X.to(device), y.to(device)

        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(loader)

# Early Stopping
best_auc = 0
patience = 5
counter = 0

for epoch in range(100):
    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
    val_auc = evaluate(model, val_loader, device)

    if val_auc > best_auc:
        best_auc = val_auc
        torch.save(model.state_dict(), 'models/lstm_best.pt')
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print(f"Early stopping at epoch {epoch}")
            break
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ LSTMì„ ì„ íƒí–ˆë‚˜ìš”?"
> "ê±°ë˜ ì‹œí€€ìŠ¤ì˜ ì‹œê°„ì  íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤. Transformerë„ ê³ ë ¤í–ˆì§€ë§Œ, ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 10~20ìœ¼ë¡œ ì§§ì•„ì„œ LSTMì´ ì¶©ë¶„í–ˆê³  í•™ìŠµë„ ë” ë¹ ë¦…ë‹ˆë‹¤."

Q: "ì™œ Transformerê°€ ì•„ë‹Œê°€ìš”?"
> "ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì§§ìŠµë‹ˆë‹¤. TransformerëŠ” ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ê°•ì ì´ ìˆì§€ë§Œ, 10~20ê°œ ì‹œí€€ìŠ¤ì—ì„œëŠ” LSTMê³¼ ì„±ëŠ¥ ì°¨ì´ê°€ ê±°ì˜ ì—†ê³  êµ¬í˜„ë„ ê°„ë‹¨í•©ë‹ˆë‹¤."

---

## 1-5: Ensemble ì‹¤í—˜ + í‰ê°€ (Day 5) â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
import numpy as np
from sklearn.metrics import roc_auc_score, classification_report
import matplotlib.pyplot as plt
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì•™ìƒë¸” ì‹¤í—˜ ê²°ê³¼**
- LSTM AUC 0.70ìœ¼ë¡œ ì˜ˆìƒë³´ë‹¤ ë‚®ìŒ
- ì•™ìƒë¸”í•´ë„ +0.12% í–¥ìƒì— ê·¸ì¹¨
- **ê²°ë¡ : XGBoost ë‹¨ë… ì±„íƒ**

**2. ì‹¤í—˜ ê²°ê³¼**
```
XGBoost ë‹¨ë…: AUC 0.9042 â†’ ì±„íƒ
LSTM ë‹¨ë…:    AUC 0.7054 â†’ ì„±ëŠ¥ ë‚®ìŒ
ì•™ìƒë¸”:       AUC 0.9054 â†’ +0.12% (íš¨ê³¼ ë¯¸ë¯¸)
ìµœì  ê°€ì¤‘ì¹˜:  XGBoost 90%, LSTM 10%
```

**3. ì±„íƒ ê·¼ê±°**
- +0.12% í–¥ìƒì€ LSTM ì„œë¹™ ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼ ì—†ìŒ
- ë³µì¡ë„ ì¦ê°€ (PyTorch, ì‹œí€€ìŠ¤ ìƒì„±) vs ì„±ëŠ¥ í–¥ìƒ trade-off
- XGBoost ë‹¨ë…ìœ¼ë¡œ Recall 90.55% ë‹¬ì„±

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: XGBoost ì˜ˆì¸¡
- ì‹¤ìŠµ 2: LSTM ì˜ˆì¸¡
- ì‹¤ìŠµ 3: ê°€ì¤‘ì¹˜ ìµœì í™” (Grid Search)
- ì‹¤ìŠµ 4: ì„±ëŠ¥ ë¹„êµ â†’ XGBoost ë‹¨ë… ì±„íƒ ê²°ë¡ 
- ì‹¤ìŠµ 5: ë³µì¡ë„ ëŒ€ë¹„ íš¨ê³¼ ë¶„ì„

### í•µì‹¬ ì½”ë“œ: ê°€ì¤‘ì¹˜ ìµœì í™”

```python
# XGBoost, LSTM ì˜ˆì¸¡
p_xgb = xgb_model.predict_proba(X_test_tabular)[:, 1]
p_lstm = lstm_model(X_test_seq).detach().cpu().numpy().flatten()

# ê°€ì¤‘ì¹˜ ìµœì í™”
best_auc = 0
best_weight = 0

for w_xgb in np.arange(0.3, 0.8, 0.1):
    w_lstm = 1 - w_xgb
    p_ensemble = w_xgb * p_xgb + w_lstm * p_lstm
    auc = roc_auc_score(y_test, p_ensemble)

    if auc > best_auc:
        best_auc = auc
        best_weight = w_xgb

print(f"ìµœì  ê°€ì¤‘ì¹˜: XGBoost {best_weight:.1f}, LSTM {1-best_weight:.1f}")
print(f"Ensemble AUC: {best_auc:.4f}")
```

### í•µì‹¬ ì½”ë“œ: ì„±ëŠ¥ ë¹„êµ

```python
# ì„±ëŠ¥ ë¹„êµ í‘œ
results = pd.DataFrame([
    {'Model': 'XGBoost', 'AUC': roc_auc_score(y_test, p_xgb)},
    {'Model': 'LSTM', 'AUC': roc_auc_score(y_test, p_lstm)},
    {'Model': 'Ensemble', 'AUC': best_auc},
])
print(results.to_markdown(index=False))

# ì‹œê°í™”
fig, ax = plt.subplots(figsize=(8, 5))
ax.bar(results['Model'], results['AUC'])
ax.set_ylabel('AUC')
ax.set_title('Model Comparison')
plt.show()
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì•™ìƒë¸”ë¡œ ì–¼ë§ˆë‚˜ ì„±ëŠ¥ì´ ì˜¬ëë‚˜ìš”?"
> "LSTM AUCê°€ 0.70ìœ¼ë¡œ ë‚®ì•„ì„œ, ì•™ìƒë¸”í•´ë„ +0.12% í–¥ìƒì— ê·¸ì³¤ìŠµë‹ˆë‹¤. LSTM ì„œë¹™ ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼ê°€ ì—†ì–´ì„œ XGBoost ë‹¨ë…ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤."

Q: "ì™œ LSTMì´ íš¨ê³¼ê°€ ì—†ì—ˆë‚˜ìš”?"
> "IEEE-CIS ë°ì´í„° íŠ¹ì„±ìƒ, ì‹œê³„ì—´ íŒ¨í„´ë³´ë‹¤ ì •í˜• í”¼ì²˜(ê¸ˆì•¡, ì‹œê°„, ì¹´ë“œì •ë³´)ê°€ ë” ê²°ì •ì ì´ì—ˆìŠµë‹ˆë‹¤. ëª¨ë“  ë¬¸ì œì— ë”¥ëŸ¬ë‹ì´ ìµœì„ ì€ ì•„ë‹™ë‹ˆë‹¤."

---

## 1-6: SHAP ì„¤ëª… (Day 6)

### í•„ìš” íŒ¨í‚¤ì§€
```python
import shap
import torch
import numpy as np
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. XGBoost ì„¤ëª…: TreeExplainer**
- ë¹ ë¥´ê³  ì •í™•
- ì „ì²´ í”¼ì²˜ ì¤‘ìš”ë„
- ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…

**2. ìì—°ì–´ ì„¤ëª… ìƒì„±**
- í”¼ì²˜ëª… â†’ ì„¤ëª… ë§¤í•‘
- ë°©í–¥ (ì¦ê°€/ê°ì†Œ) í¬í•¨
- Top 5 í”¼ì²˜ ì¶”ì¶œ

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: XGBoost SHAP ê³„ì‚°
- ì‹¤ìŠµ 2: SHAP Summary Plot
- ì‹¤ìŠµ 3: ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…
- ì‹¤ìŠµ 4: ìì—°ì–´ ì„¤ëª… ìƒì„±
- ì‹¤ìŠµ 5: API ì‘ë‹µ í˜•íƒœë¡œ ë³€í™˜

### í•µì‹¬ ì½”ë“œ: XGBoost SHAP

```python
# TreeExplainer
explainer_xgb = shap.TreeExplainer(xgb_model)
shap_values_xgb = explainer_xgb.shap_values(X_test_tabular)

# Summary Plot
shap.summary_plot(shap_values_xgb, X_test_tabular, max_display=10)
```

### í•µì‹¬ ì½”ë“œ: ìì—°ì–´ ì„¤ëª… ìƒì„±

```python
# í”¼ì²˜ëª… â†’ ì„¤ëª… ë§¤í•‘
FEATURE_DESC = {
    'TransactionAmt': 'ê±°ë˜ ê¸ˆì•¡',
    'hour': 'ê±°ë˜ ì‹œê°„',
    'card1_fraud_rate': 'ì¹´ë“œ ì‚¬ê¸° ì´ë ¥',
}

def to_natural_language(shap_values, feature_names, top_k=5):
    """SHAP ê°’ì„ ìì—°ì–´ ì„¤ëª…ìœ¼ë¡œ ë³€í™˜"""
    # Top K í”¼ì²˜ ì¶”ì¶œ
    importance = np.abs(shap_values)
    top_idx = np.argsort(importance)[-top_k:][::-1]

    lines = ["[ì‚¬ê¸° íŒë‹¨ ê·¼ê±°]"]
    for i in top_idx:
        name = FEATURE_DESC.get(feature_names[i], feature_names[i])
        direction = "ë†’ìŒ" if shap_values[i] > 0 else "ë‚®ìŒ"
        lines.append(f"- {name}: ì‚¬ê¸° í™•ë¥  {direction}")

    return "\n".join(lines)
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ SHAPì„ ì„ íƒí–ˆë‚˜ìš”?"
> "SHAPì€ ì´ë¡ ì  ê¸°ë°˜(Shapley Value)ì´ íƒ„íƒ„í•˜ê³ , XGBoostì˜ TreeExplainerë¡œ ë¹ ë¥´ê³  ì •í™•í•œ ì„¤ëª…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."

Q: "ì„¤ëª…ì€ ì–´ë–»ê²Œ ë³´ì—¬ì£¼ë‚˜ìš”?"
> "TreeExplainerë¡œ í”¼ì²˜ë³„ ê¸°ì—¬ë„ë¥¼ ê³„ì‚°í•˜ê³ , Top 5 í”¼ì²˜ë¥¼ ìì—°ì–´ë¡œ ë³€í™˜í•´ì„œ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆ: 'ê±°ë˜ ê¸ˆì•¡ì´ ë†’ìŒ â†’ ì‚¬ê¸° í™•ë¥  ì¦ê°€'"

---

## 1-7: FastAPI ë°°í¬ (Day 7)

### í•„ìš” íŒ¨í‚¤ì§€
```python
from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import torch
import uvicorn
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. API ì—”ë“œí¬ì¸íŠ¸**
- GET /health: í—¬ìŠ¤ì²´í¬
- POST /predict: ì‚¬ê¸° ì˜ˆì¸¡ + ì„¤ëª…

**2. Request/Response ìŠ¤í‚¤ë§ˆ**
- Pydantic BaseModel
- íƒ€ì… ê²€ì¦

**3. ëª¨ë¸ ë¡œë”©**
- ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë”©
- XGBoost + LSTM + SHAP Explainer

**4. Docker ì»¨í…Œì´ë„ˆí™”**
- Dockerfile ì‘ì„±
- docker-compose.yml
- ë‹¨ì¼ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰

**5. ì„±ëŠ¥ ìµœì í™”**
- ëª¨ë¸ ìºì‹±
- ì‘ë‹µ ì‹œê°„ < 200ms

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜
- ì‹¤ìŠµ 2: /health ì—”ë“œí¬ì¸íŠ¸
- ì‹¤ìŠµ 3: /predict ì—”ë“œí¬ì¸íŠ¸
- ì‹¤ìŠµ 4: Dockerfile ì‘ì„±
- ì‹¤ìŠµ 5: docker-compose ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

### í•µì‹¬ ì½”ë“œ: FastAPI

```python
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Dict
import joblib
import torch

app = FastAPI(title="FDS API", version="1.0")

# ëª¨ë¸ ë¡œë”© (ì‹œì‘ ì‹œ í•œ ë²ˆ)
xgb_model = joblib.load('models/xgb_model.pkl')
lstm_model = torch.load('models/lstm_model.pt')
lstm_model.eval()

class Transaction(BaseModel):
    transaction_id: int
    amount: float
    hour: int
    card1: int
    # ... ê¸°íƒ€ í”¼ì²˜
    recent_transactions: List[Dict]  # ìµœê·¼ ê±°ë˜ ì‹œí€€ìŠ¤

class PredictionResponse(BaseModel):
    fraud_probability: float
    model_scores: Dict[str, float]
    top_factors: List[Dict]
    is_fraud: bool

@app.get("/health")
def health():
    return {"status": "healthy"}

@app.post("/predict", response_model=PredictionResponse)
def predict(transaction: Transaction):
    # 1. Feature Engineering
    X_tabular = extract_tabular_features(transaction)
    X_sequence = extract_sequence_features(transaction.recent_transactions)

    # 2. ëª¨ë¸ ì˜ˆì¸¡
    p_xgb = xgb_model.predict_proba(X_tabular)[0, 1]
    with torch.no_grad():
        p_lstm = lstm_model(torch.FloatTensor(X_sequence).unsqueeze(0)).item()

    # 3. XGBoost ë‹¨ë… ì‚¬ìš© (ì•™ìƒë¸” íš¨ê³¼ ë¯¸ë¯¸í•˜ì—¬ ì±„íƒ)
    # ì°¸ê³ : ì•™ìƒë¸” ì‹¤í—˜ì—ì„œ p = 0.9*xgb + 0.1*lstm â†’ +0.12% í–¥ìƒì— ê·¸ì¹¨

    # 4. SHAP ì„¤ëª…
    explanation = generate_explanation(X_tabular)

    return PredictionResponse(
        fraud_probability=p_xgb,
        model_scores={"xgboost": p_xgb},
        top_factors=explanation,
        is_fraud=p_xgb > 0.18  # ìµœì í™”ëœ threshold
    )
```

### í•µì‹¬ ì½”ë“œ: Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY src/ ./src/
COPY models/ ./models/

CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### í•µì‹¬ ì½”ë“œ: docker-compose.yml

```yaml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "API ì‘ë‹µ ì‹œê°„ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?"
> "XGBoost ~10ms, LSTM ~50ms, SHAP ~100msë¡œ ì´ ì•½ 160msì…ë‹ˆë‹¤. ëª©í‘œì˜€ë˜ 200ms ì´í•˜ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤."

Q: "ëª¨ë¸ ì—…ë°ì´íŠ¸ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
> "í˜„ì¬ëŠ” Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ ë°©ì‹ì…ë‹ˆë‹¤. Phase 2ì—ì„œ MLflowë¡œ ëª¨ë¸ ë²„ì „ ê´€ë¦¬ë¥¼ ì¶”ê°€í•˜ë©´ ë¬´ì¤‘ë‹¨ ë°°í¬ê°€ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤."

---

## 1-8: React Admin (Day 8)

### í•„ìš” íŒ¨í‚¤ì§€
```bash
npx create-react-app fds-admin
npm install antd axios recharts
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. í”„ë¡œì íŠ¸ êµ¬ì¡°**
```
frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ TransactionTable.jsx   # ê±°ë˜ ëª©ë¡ í…Œì´ë¸”
â”‚   â”‚   â””â”€â”€ TransactionDetail.jsx  # ê±°ë˜ ìƒì„¸ + SHAP ìš”ì¸
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ Dashboard.jsx          # ë©”ì¸ ëŒ€ì‹œë³´ë“œ
â”‚   â”‚   â””â”€â”€ TransactionPage.jsx    # ê±°ë˜ ì¡°íšŒ í˜ì´ì§€
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ client.js              # Axios ì„¤ì •
â”‚   â””â”€â”€ App.jsx
â”œâ”€â”€ package.json
â””â”€â”€ Dockerfile
```

**2. í•µì‹¬ í™”ë©´ (2ê°œ)**
- **ê±°ë˜ ëª©ë¡**: í…Œì´ë¸” (ê±°ë˜ID, ê¸ˆì•¡, ì‚¬ê¸°í™•ë¥ , topìš”ì¸, ì¡°ì¹˜)
- **ê±°ë˜ ìƒì„¸**: SHAP ìš”ì¸ í‘œì‹œ, ìŠ¹ì¸/ì°¨ë‹¨ ë²„íŠ¼

**3. API ì—°ë™**
```javascript
// api/client.js
import axios from 'axios';

const api = axios.create({
  baseURL: 'http://localhost:8000',
});

export const getTransactions = () => api.get('/transactions');
export const getTransaction = (id) => api.get(`/transactions/${id}`);
export const predict = (data) => api.post('/predict', data);
```

**4. í˜„ì—… ìŠ¤íƒ€ì¼ (ë‹¨ìˆœí•¨ ìœ ì§€)**
- ì˜ˆìœ ê·¸ë˜í”„ X â†’ í…ìŠ¤íŠ¸/í…Œì´ë¸”ë§Œ
- SHAP waterfall X â†’ top 3 ìš”ì¸ í…ìŠ¤íŠ¸
- ë¶„ì„ê°€ìš© Admin ëŠë‚Œ

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: React í”„ë¡œì íŠ¸ ì„¤ì • + Ant Design
- ì‹¤ìŠµ 2: ê±°ë˜ ëª©ë¡ í…Œì´ë¸” (TransactionTable)
- ì‹¤ìŠµ 3: ê±°ë˜ ìƒì„¸ í˜ì´ì§€ (SHAP ìš”ì¸ í‘œì‹œ)
- ì‹¤ìŠµ 4: FastAPI ì—°ë™
- ì‹¤ìŠµ 5: Docker ì»¨í…Œì´ë„ˆí™”

### í•µì‹¬ ì½”ë“œ: ê±°ë˜ í…Œì´ë¸”

```jsx
// components/TransactionTable.jsx
import { Table, Tag } from 'antd';

const columns = [
  { title: 'ê±°ë˜ID', dataIndex: 'transaction_id', key: 'id' },
  { title: 'ê¸ˆì•¡', dataIndex: 'amount', key: 'amount',
    render: (val) => `â‚©${val.toLocaleString()}` },
  { title: 'ì‚¬ê¸°í™•ë¥ ', dataIndex: 'fraud_probability', key: 'prob',
    render: (val) => (
      <Tag color={val > 0.5 ? 'red' : 'green'}>
        {(val * 100).toFixed(1)}%
      </Tag>
    )},
  { title: 'ì£¼ìš”ìš”ì¸', dataIndex: 'top_factors', key: 'factors',
    render: (factors) => factors.slice(0, 2).map(f => f.feature).join(', ') },
  { title: 'ì¡°ì¹˜', key: 'action',
    render: () => <a>ìƒì„¸ë³´ê¸°</a> },
];

export default function TransactionTable({ data }) {
  return <Table columns={columns} dataSource={data} rowKey="transaction_id" />;
}
```

### í•µì‹¬ ì½”ë“œ: SHAP ìš”ì¸ í‘œì‹œ

```jsx
// components/TransactionDetail.jsx
import { Card, List, Typography } from 'antd';
const { Text } = Typography;

export default function TransactionDetail({ transaction }) {
  const { fraud_probability, top_factors } = transaction;

  return (
    <Card title={`ì‚¬ê¸° í™•ë¥ : ${(fraud_probability * 100).toFixed(1)}%`}>
      <List
        header={<Text strong>ì£¼ìš” íŒë‹¨ ê·¼ê±°</Text>}
        dataSource={top_factors}
        renderItem={(item) => (
          <List.Item>
            <Text>{item.feature}</Text>
            <Text type={item.impact > 0 ? 'danger' : 'success'}>
              {item.impact > 0 ? '+' : ''}{item.impact.toFixed(3)}
            </Text>
          </List.Item>
        )}
      />
    </Card>
  );
}
```

### í•µì‹¬ ì½”ë“œ: Dockerfile

```dockerfile
# frontend/Dockerfile
FROM node:18-alpine as build
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=build /app/build /usr/share/nginx/html
EXPOSE 80
```

### docker-compose ì¶”ê°€

```yaml
# docker-compose.ymlì— ì¶”ê°€
services:
  api:
    build: .
    ports:
      - "8000:8000"

  frontend:
    build: ./frontend
    ports:
      - "3000:80"
    depends_on:
      - api
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "í”„ë¡ íŠ¸ì—”ë“œëŠ” ì™œ Reactë¥¼ ì„ íƒí–ˆë‚˜ìš”?"
> "ê¸ˆìœµê¶Œì—ì„œ React ì‚¬ìš© ë¹„ìœ¨ì´ ê°€ì¥ ë†’ê³ , ì¸ë ¥ ìˆ˜ê¸‰ì´ ìš©ì´í•©ë‹ˆë‹¤. Ant Designì„ ì‚¬ìš©í•´ í…Œì´ë¸” ì¤‘ì‹¬ì˜ Admin UIë¥¼ ë¹ ë¥´ê²Œ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤."

Q: "SHAP ì‹œê°í™”ëŠ” ì–´ë–»ê²Œ ë³´ì—¬ì£¼ë‚˜ìš”?"
> "í˜„ì—…ì—ì„œëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ ë³µì¡í•œ ê·¸ë˜í”„ë¥¼ ë„ìš°ì§€ ì•ŠìŠµë‹ˆë‹¤. top 3 ìš”ì¸ì„ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œí•˜ê³ , ìƒì„¸ ë¶„ì„ì€ ì˜¤í”„ë¼ì¸ì—ì„œ Jupyterë¡œ í•©ë‹ˆë‹¤. ì´ ë°©ì‹ì´ ì‘ë‹µ ì†ë„ì™€ ê°€ë…ì„± ë©´ì—ì„œ ë” íš¨ìœ¨ì ì…ë‹ˆë‹¤."

---

## 1-9: íŠ¸ë¦¬ ìŠ¤íƒœí‚¹ (Day 9) â­â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import roc_auc_score, f1_score
import numpy as np
import pandas as pd
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì™œ íŠ¸ë¦¬ ìŠ¤íƒœí‚¹ì¸ê°€?**
- 2025 ë²¤ì¹˜ë§ˆí¬ ë…¼ë¬¸ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ ë³´ê³ 
- í˜„ì—…ì—ì„œ ì¦ê°€ ì¤‘ì¸ íŠ¸ë Œë“œ
- ì‹¤ì‹œê°„ ì¶”ë¡  ê°€ëŠ¥ (~12ms)
- ê° ëª¨ë¸ì˜ ì¥ì ì„ ê²°í•©
- **í™•ë¥  ë¶„í¬ ì–‘ê·¹í™”**: ì •ìƒ ê±°ë˜ í™•ë¥  20~30% â†’ 1~2%ë¡œ ë‚®ì•„ì ¸ ìš´ì˜ ë¹„ìš© ì ˆê°

**ì‹¤ì œ ë‹¬ì„± ê²°ê³¼:**
| ì§€í‘œ | ê°’ | ì„¤ëª… |
|------|-----|------|
| AUPRC | 0.5957 | ë¶ˆê· í˜• ë°ì´í„° ì í•© ì§€í‘œ |
| AUC | 0.9205 | ì „ì²´ ì„±ëŠ¥ |
| Recall @5% FPR | 71% | FPR ì œì•½ ì‹œ íƒì§€ìœ¨ |
| Threshold | 0.08 | FPR Constraint ê¸°ì¤€ |

**2. ìŠ¤íƒœí‚¹ êµ¬ì¡°**
```
[Base Models - Level 0]
XGBoost  â”€â”
LightGBM â”€â”¼â†’ [Meta-Learner - Level 1] â†’ ìµœì¢… ì˜ˆì¸¡
CatBoost â”€â”˜

êµì°¨ ê²€ì¦ìœ¼ë¡œ OOF (Out-of-Fold) ì˜ˆì¸¡ ìƒì„± â†’ Meta-Learner í•™ìŠµ
```

**3. ê° ëª¨ë¸ì˜ ê°•ì /ì•½ì **

| ëª¨ë¸ | ê°•ì  | ì•½ì  |
|------|------|------|
| XGBoost | ì •ê·œí™” ìš°ìˆ˜, SHAP ìµœìƒ | ìƒëŒ€ì  ëŠë¦¼ |
| LightGBM | ê°€ì¥ ë¹ ë¦„, ë©”ëª¨ë¦¬ íš¨ìœ¨ | Tail latency |
| CatBoost | ë²”ì£¼í˜• ìë™ ì²˜ë¦¬ | í•™ìŠµ ì‹œê°„ ê¹€ |

**4. Meta-Learner ì„ íƒ**
- Logistic Regression: ê°„ë‹¨, ì˜¤ë²„í”¼íŒ… ë°©ì§€
- XGBoost: ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: LightGBM ë‹¨ë… í•™ìŠµ ë° í‰ê°€
- ì‹¤ìŠµ 2: CatBoost ë‹¨ë… í•™ìŠµ ë° í‰ê°€
- ì‹¤ìŠµ 3: 3ê°œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í‘œ
- ì‹¤ìŠµ 4: StackingClassifierë¡œ ìŠ¤íƒœí‚¹ êµ¬í˜„
- ì‹¤ìŠµ 5: OOF ìˆ˜ë™ êµ¬í˜„ (sklearn ëŒ€ë¹„ ìœ ì—°ì„±)
- ì‹¤ìŠµ 6: ìµœì¢… ì„±ëŠ¥ ë¹„êµ ë° ëª¨ë¸ ì €ì¥

### í•µì‹¬ ì½”ë“œ: sklearn StackingClassifier

```python
# Base models
base_models = [
    ('xgb', XGBClassifier(
        n_estimators=300,
        max_depth=6,
        learning_rate=0.05,
        tree_method='hist',
        device='cuda',
        random_state=42
    )),
    ('lgbm', LGBMClassifier(
        n_estimators=300,
        max_depth=6,
        learning_rate=0.05,
        device='gpu',
        random_state=42,
        verbose=-1
    )),
    ('cat', CatBoostClassifier(
        n_estimators=300,
        max_depth=6,
        learning_rate=0.05,
        task_type='GPU',
        random_state=42,
        verbose=0
    ))
]

# Stacking
stacking_model = StackingClassifier(
    estimators=base_models,
    final_estimator=LogisticRegression(max_iter=1000),
    cv=5,
    passthrough=False,  # Trueë©´ ì›ë³¸ í”¼ì²˜ë„ Meta-Learnerì— ì „ë‹¬
    n_jobs=-1
)

stacking_model.fit(X_train, y_train)
y_prob = stacking_model.predict_proba(X_test)[:, 1]
print(f"Stacking AUC: {roc_auc_score(y_test, y_prob):.4f}")
```

### í•µì‹¬ ì½”ë“œ: ìˆ˜ë™ OOF ìŠ¤íƒœí‚¹

```python
from sklearn.model_selection import StratifiedKFold

def get_oof_predictions(model, X, y, n_splits=5):
    """Out-of-Fold ì˜ˆì¸¡ ìƒì„±"""
    oof_preds = np.zeros(len(X))
    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    for train_idx, val_idx in kfold.split(X, y):
        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_tr = y.iloc[train_idx]

        model_clone = clone(model)
        model_clone.fit(X_tr, y_tr)
        oof_preds[val_idx] = model_clone.predict_proba(X_val)[:, 1]

    return oof_preds

# ê° ëª¨ë¸ì˜ OOF ì˜ˆì¸¡
oof_xgb = get_oof_predictions(xgb_model, X_train, y_train)
oof_lgbm = get_oof_predictions(lgbm_model, X_train, y_train)
oof_cat = get_oof_predictions(cat_model, X_train, y_train)

# Meta-learnerìš© í”¼ì²˜
meta_features = np.column_stack([oof_xgb, oof_lgbm, oof_cat])

# Meta-learner í•™ìŠµ
meta_model = LogisticRegression()
meta_model.fit(meta_features, y_train)

# Test set ì˜ˆì¸¡
test_xgb = xgb_model.predict_proba(X_test)[:, 1]
test_lgbm = lgbm_model.predict_proba(X_test)[:, 1]
test_cat = cat_model.predict_proba(X_test)[:, 1]
test_meta = np.column_stack([test_xgb, test_lgbm, test_cat])

y_final = meta_model.predict_proba(test_meta)[:, 1]
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ íŠ¸ë¦¬ ìŠ¤íƒœí‚¹ì„ ì‚¬ìš©í–ˆë‚˜ìš”?"
> "XGBoost, LightGBM, CatBoost ê°ê°ì˜ ê°•ì ì„ ê²°í•©í•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤. XGBoostëŠ” ì •ê·œí™”, LightGBMì€ ì†ë„, CatBoostëŠ” ë²”ì£¼í˜• ì²˜ë¦¬ì— ê°•í•©ë‹ˆë‹¤. Meta-learner(LogisticRegression)ê°€ ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ìµœì  ì¡°í•©í•˜ì—¬ AUC 0.92, AUPRC 0.60ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤."

Q: "ìŠ¤íƒœí‚¹ ì ìš© í›„ í™•ë¥  ë¶„í¬ê°€ ì–´ë–»ê²Œ ë³€í–ˆë‚˜ìš”?"
> "**í™•ë¥  ë¶„í¬ê°€ ì–‘ê·¹í™”**ë˜ì—ˆìŠµë‹ˆë‹¤. XGBoost ë‹¨ë…ì—ì„œëŠ” ì •ìƒ ê±°ë˜ë„ 20~30% í™•ë¥ ì´ ë‚˜ì™”ëŠ”ë°, ìŠ¤íƒœí‚¹ í›„ì—ëŠ” 1~2%ë¡œ ë‚®ì•„ì¡ŒìŠµë‹ˆë‹¤. Block ë¹„ìœ¨ì€ ìœ ì§€í•˜ë©´ì„œ Hold/Verify ë¹„ìœ¨ì´ ê°ì†Œí•´ì„œ **ìš´ì˜ ë¹„ìš©ì´ ì ˆê°**ë©ë‹ˆë‹¤. 3ê°œ ëª¨ë¸ì´ ë™ì˜í•´ì•¼ ë†’ì€ í™•ë¥ ì´ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."

Q: "ìŠ¤íƒœí‚¹ì˜ ë‹¨ì ì€?"
> "ì¶”ë¡  ì‹œê°„ì´ ë‹¨ì¼ ëª¨ë¸ ëŒ€ë¹„ 2~3ë°° ì¦ê°€í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ 12msë¡œ ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì— ì¶©ë¶„í•©ë‹ˆë‹¤. í•™ìŠµ ì‹œê°„ë„ 3ë°° ì¦ê°€í•˜ì§€ë§Œ, Optuna íŠœë‹ ë•Œë§Œ ë¬¸ì œë  ë¿ ìš´ì˜ì—ëŠ” ì˜í–¥ ì—†ìŠµë‹ˆë‹¤."

Q: "Voting vs Stacking ì°¨ì´ëŠ”?"
> "Votingì€ ë‹¨ìˆœ í‰ê· /ê°€ì¤‘ í‰ê· ì´ê³ , Stackingì€ Meta-learnerê°€ ìµœì ì˜ ê²°í•© ë°©ì‹ì„ í•™ìŠµí•©ë‹ˆë‹¤. ìš°ë¦¬ ë°ì´í„°ì—ì„œ Stackingì´ Votingë³´ë‹¤ AUC +2% ë†’ì•˜ìŠµë‹ˆë‹¤."

Q: "ì™œ F1ì´ ì•„ë‹Œ AUPRC, Recallì„ ì“°ë‚˜ìš”?"
> "FDSì—ì„œëŠ” **Recallì´ í•µì‹¬**ì…ë‹ˆë‹¤. ì‚¬ê¸°ë¥¼ ë†“ì¹˜ë©´(FN) í° ì†ì‹¤ì´ê³ , ì˜¤íƒ(FP)ì€ ì¶”ê°€ ê²€ì¦ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥í•©ë‹ˆë‹¤. F1ì€ Precision-Recall ê· í˜•ì„ ë³´ëŠ”ë°, FDSì—ì„œëŠ” ê· í˜•ë³´ë‹¤ Recall ìš°ì„ ì…ë‹ˆë‹¤. AUPRCëŠ” ë¶ˆê· í˜• ë°ì´í„°(ì‚¬ê¸° 3.5%)ì—ì„œ AUC-ROCë³´ë‹¤ ë” ì •í™•í•œ ì„±ëŠ¥ ì§€í‘œì…ë‹ˆë‹¤."

---

## 1-10: Transformer (Day 10) - ì„ íƒ â­â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
# pip install tab-transformer-pytorch (ì„ íƒ)
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì™œ Transformerì¸ê°€?**
- 2025 ë²¤ì¹˜ë§ˆí¬: F1 0.998 (ì—°êµ¬ ìµœê°•)
- Self-Attentionìœ¼ë¡œ í”¼ì²˜ ê°„ ê´€ê³„ í•™ìŠµ
- ì •í˜• ë°ì´í„°ì—ì„œë„ ê°•ë ¥í•œ ì„±ëŠ¥

**2. TabTransformer êµ¬ì¡°**
```
[ì…ë ¥] ì •í˜• í”¼ì²˜
      â†“
[Embedding Layer]
- ë²”ì£¼í˜• â†’ Embedding
- ìˆ˜ì¹˜í˜• â†’ ê·¸ëŒ€ë¡œ ë˜ëŠ” MLP
      â†“
[Transformer Encoder]
- Multi-Head Self-Attention
- Feed-Forward Network
      â†“
[MLP Head] â†’ ì˜ˆì¸¡
```

**3. Self-Attention ì§ê´€**
- ê° í”¼ì²˜ê°€ ë‹¤ë¥¸ í”¼ì²˜ë“¤ê³¼ì˜ ê´€ê³„ë¥¼ í•™ìŠµ
- ì˜ˆ: "ê³ ì•¡ ê±°ë˜" + "ìƒˆë²½ ì‹œê°„" ì¡°í•© íŒ¨í„´ ìë™ í•™ìŠµ
- XGBoostì˜ ìˆ˜ë™ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ëŒ€ì²´

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: Self-Attention êµ¬í˜„ ì´í•´
- ì‹¤ìŠµ 2: TabTransformer ì§ì ‘ êµ¬í˜„
- ì‹¤ìŠµ 3: í•™ìŠµ ë£¨í”„ ë° Early Stopping
- ì‹¤ìŠµ 4: XGBoostì™€ ì„±ëŠ¥ ë¹„êµ
- ì‹¤ìŠµ 5: ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬

### í•µì‹¬ ì½”ë“œ: TabTransformer êµ¬í˜„

```python
class TabTransformer(nn.Module):
    def __init__(
        self,
        num_continuous: int,
        num_categories: list,  # ê° ë²”ì£¼í˜• í”¼ì²˜ì˜ ì¹´ë””ë„ë¦¬í‹°
        dim: int = 32,
        depth: int = 6,
        heads: int = 8,
        mlp_dim: int = 64,
        dropout: float = 0.1
    ):
        super().__init__()

        # ë²”ì£¼í˜• ì„ë² ë”©
        self.cat_embeddings = nn.ModuleList([
            nn.Embedding(num_cat, dim) for num_cat in num_categories
        ])

        # ìˆ˜ì¹˜í˜• ì²˜ë¦¬
        self.cont_norm = nn.LayerNorm(num_continuous)
        self.cont_proj = nn.Linear(num_continuous, dim)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=dim,
            nhead=heads,
            dim_feedforward=mlp_dim,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)

        # MLP Head
        total_dim = dim * (len(num_categories) + 1)  # +1 for continuous
        self.mlp = nn.Sequential(
            nn.Linear(total_dim, mlp_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, x_cat, x_cont):
        # ë²”ì£¼í˜• ì„ë² ë”©
        cat_embeds = [
            emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeddings)
        ]
        cat_embeds = torch.stack(cat_embeds, dim=1)  # (batch, num_cat, dim)

        # ìˆ˜ì¹˜í˜• ì²˜ë¦¬
        x_cont = self.cont_norm(x_cont)
        cont_embed = self.cont_proj(x_cont).unsqueeze(1)  # (batch, 1, dim)

        # ê²°í•©
        x = torch.cat([cat_embeds, cont_embed], dim=1)  # (batch, num_cat+1, dim)

        # Transformer
        x = self.transformer(x)

        # Flatten + MLP
        x = x.flatten(1)
        return self.mlp(x)
```

### í•µì‹¬ ì½”ë“œ: í•™ìŠµ ë£¨í”„

```python
def train_transformer(model, train_loader, val_loader, epochs=50, patience=5):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.BCELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)

    best_auc = 0
    counter = 0

    for epoch in range(epochs):
        model.train()
        for x_cat, x_cont, y in train_loader:
            x_cat, x_cont, y = x_cat.to(device), x_cont.to(device), y.to(device)

            optimizer.zero_grad()
            output = model(x_cat, x_cont).squeeze()
            loss = criterion(output, y.float())
            loss.backward()
            optimizer.step()

        # Validation
        model.eval()
        val_preds = []
        val_targets = []
        with torch.no_grad():
            for x_cat, x_cont, y in val_loader:
                x_cat, x_cont = x_cat.to(device), x_cont.to(device)
                output = model(x_cat, x_cont).squeeze()
                val_preds.extend(output.cpu().numpy())
                val_targets.extend(y.numpy())

        val_auc = roc_auc_score(val_targets, val_preds)
        scheduler.step()

        if val_auc > best_auc:
            best_auc = val_auc
            torch.save(model.state_dict(), 'models/transformer_best.pt')
            counter = 0
        else:
            counter += 1
            if counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                break

        print(f"Epoch {epoch}: Val AUC = {val_auc:.4f}")

    return best_auc
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ Transformerë¥¼ FDSì— ì ìš©í–ˆë‚˜ìš”?"
> "2025ë…„ ì—°êµ¬ì—ì„œ TabTransformerê°€ ì •í˜• ë°ì´í„°ì—ì„œ F1 0.998ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. Self-Attentionì´ í”¼ì²˜ ê°„ ë³µì¡í•œ ê´€ê³„(ì˜ˆ: ê³ ì•¡+ìƒˆë²½+í•´ì™¸)ë¥¼ ìë™ìœ¼ë¡œ í•™ìŠµí•´ì„œ, ìˆ˜ë™ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì—†ì´ ë†’ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

Q: "í˜„ì—…ì—ì„œ Transformer ì•ˆ ì“°ëŠ” ì´ìœ ëŠ”?"
> "ì¶”ë¡  ì†ë„ê°€ 50-100msë¡œ XGBoost(5ms)ë³´ë‹¤ ëŠë¦½ë‹ˆë‹¤. í•˜ì§€ë§Œ HSBC, Featurespace ë“± ëŒ€í˜• ê¸ˆìœµì‚¬ì—ì„œ ë„ì… ì¤‘ì´ê³ , ë°°ì¹˜ ì¶”ë¡ ì´ë‚˜ ê³ ìœ„í—˜ ê±°ë˜ ì¬ê²€í† ì—ëŠ” ì¶©ë¶„íˆ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."

---

## 1-11: PaySim ê³µì • ë¹„êµ (Day 11) - ì„ íƒ â­â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score, average_precision_score
import time
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì™œ PaySimìœ¼ë¡œ ì¬ì‹¤í—˜?**
- IEEE-CIS: V1~V339ê°€ PCA ë³€í™˜ëœ ì •ì  í”¼ì²˜ â†’ LSTM AUC 0.70 ì‹¤íŒ¨
- PaySim: ì§„ì§œ ì‹œê³„ì—´ (ì‚¬ìš©ìë³„ ê±°ë˜ ìˆœì„œ) â†’ ê³µì •í•œ ML vs DL ë¹„êµ ê°€ëŠ¥
- ì‹œê°„ ìœˆë„ìš° ì§‘ê³„ í”¼ì²˜ ì§ì ‘ êµ¬í˜„ â†’ í˜„ì—… íŒŒì´í”„ë¼ì¸ ê²½í—˜

**2. PaySim ë°ì´í„°ì…‹**

| í•­ëª© | ê°’ |
|------|-----|
| ì´ ê±°ë˜ ìˆ˜ | 6,362,620 |
| ê¸°ê°„ | 30ì¼ (744 steps, 1 step = 1ì‹œê°„) |
| ì‚¬ìš©ì ID | nameOrig |
| ê±°ë˜ íƒ€ì… | CASH_IN, CASH_OUT, TRANSFER, DEBIT, PAYMENT |
| ì‚¬ê¸° ë¹„ìœ¨ | 0.13% (8,213ê±´) |

**3. ì‹œê°„ ìœˆë„ìš° ì§‘ê³„ í”¼ì²˜ (12ê°œ) - í˜„ì—… ìˆ˜ì¤€**
```python
# ì‹œê°„ ìœˆë„ìš°ë³„ ê±°ë˜ ë¹ˆë„ (3ê°œ)
tx_count_1h      # ìµœê·¼ 1ì‹œê°„ ê±°ë˜ ìˆ˜
tx_count_24h     # ìµœê·¼ 24ì‹œê°„ ê±°ë˜ ìˆ˜
tx_count_7d      # ìµœê·¼ 7ì¼ ê±°ë˜ ìˆ˜

# ì‹œê°„ ìœˆë„ìš°ë³„ ê¸ˆì•¡ í•©ê³„ (3ê°œ)
amt_sum_1h       # ìµœê·¼ 1ì‹œê°„ ì´ì•¡
amt_sum_24h      # ìµœê·¼ 24ì‹œê°„ ì´ì•¡
amt_sum_7d       # ìµœê·¼ 7ì¼ ì´ì•¡

# ì‹œê°„ ê°„ê²© (2ê°œ)
time_since_last  # ë§ˆì§€ë§‰ ê±°ë˜ í›„ ê²½ê³¼ ì‹œê°„
avg_time_gap     # í‰ê·  ê±°ë˜ ê°„ê²©

# ì”ì•¡ ê´€ë ¨ (2ê°œ)
balance_ratio    # newBalance / oldBalance
balance_drop_pct # ì”ì•¡ ê°ì†Œìœ¨

# íŒ¨í„´ íƒì§€ (2ê°œ)
same_dest_count  # ê°™ì€ ìˆ˜ì·¨ìì—ê²Œ ë³´ë‚¸ íšŸìˆ˜
is_first_transfer # ì²« ì†¡ê¸ˆ ì—¬ë¶€
```

**4. ë¹„êµ ëª¨ë¸ (4ê°œ)**

| ëª¨ë¸ | ì…ë ¥ | íŠ¹ì§• |
|------|------|------|
| XGBoost | ì§‘ê³„ í”¼ì²˜ + ì›ë³¸ | ë² ì´ìŠ¤ë¼ì¸ |
| íŠ¸ë¦¬ ìŠ¤íƒœí‚¹ | ì§‘ê³„ í”¼ì²˜ + ì›ë³¸ | XGB+LGBM+Cat |
| LSTM | ì‹œí€€ìŠ¤ (seq_len=10) | ì‹œê³„ì—´ íŒ¨í„´ |
| Transformer | ì§‘ê³„ í”¼ì²˜ + ì›ë³¸ | Self-Attention |

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: PaySim ë°ì´í„° ë¡œë“œ ë° EDA
- ì‹¤ìŠµ 2: ì‹œê°„ ìœˆë„ìš° ì§‘ê³„ í”¼ì²˜ êµ¬í˜„ (12ê°œ)
- ì‹¤ìŠµ 3: LSTMìš© ì‹œí€€ìŠ¤ ìƒì„±
- ì‹¤ìŠµ 4: 4ê°œ ëª¨ë¸ í•™ìŠµ (XGBoost, ìŠ¤íƒœí‚¹, LSTM, Transformer)
- ì‹¤ìŠµ 5: ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
- ì‹¤ìŠµ 6: ì„±ëŠ¥ + ì†ë„ ë¹„êµ ë¶„ì„

### í•µì‹¬ ì½”ë“œ: ì‹œê°„ ìœˆë„ìš° ì§‘ê³„ í”¼ì²˜

```python
def create_time_window_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    í˜„ì—… ìˆ˜ì¤€ ì‹œê°„ ìœˆë„ìš° ì§‘ê³„ í”¼ì²˜ ìƒì„± (Vectorized, O(n) ì„±ëŠ¥)

    ê¸°ì¡´ O(nÂ²) ë£¨í”„ ë°©ì‹ ëŒ€ë¹„ ~100ë°° ë¹ ë¦„ (600ë§Œ ê±´ ê¸°ì¤€)
    """
    df = df.sort_values(['nameOrig', 'step']).copy()

    # stepì„ datetimeìœ¼ë¡œ ë³€í™˜ (1 step = 1 hour)
    df['datetime'] = pd.to_datetime(df['step'], unit='h', origin='2020-01-01')
    df = df.set_index('datetime')

    # === 1. ì‹œê°„ ìœˆë„ìš°ë³„ ê±°ë˜ ë¹ˆë„ (rolling count) ===
    # closed='left': í˜„ì¬ ê±°ë˜ ì œì™¸ (ê³¼ê±°ë§Œ)
    df['tx_count_1h'] = df.groupby('nameOrig')['amount'].transform(
        lambda x: x.shift(1).rolling('1H', min_periods=0).count()
    ).fillna(0).astype(int)

    df['tx_count_24h'] = df.groupby('nameOrig')['amount'].transform(
        lambda x: x.shift(1).rolling('24H', min_periods=0).count()
    ).fillna(0).astype(int)

    df['tx_count_7d'] = df.groupby('nameOrig')['amount'].transform(
        lambda x: x.shift(1).rolling('168H', min_periods=0).count()  # 7ì¼ = 168ì‹œê°„
    ).fillna(0).astype(int)

    # === 2. ì‹œê°„ ìœˆë„ìš°ë³„ ê¸ˆì•¡ í•©ê³„ (rolling sum) ===
    df['amt_sum_1h'] = df.groupby('nameOrig')['amount'].transform(
        lambda x: x.shift(1).rolling('1H', min_periods=0).sum()
    ).fillna(0)

    df['amt_sum_24h'] = df.groupby('nameOrig')['amount'].transform(
        lambda x: x.shift(1).rolling('24H', min_periods=0).sum()
    ).fillna(0)

    df['amt_sum_7d'] = df.groupby('nameOrig')['amount'].transform(
        lambda x: x.shift(1).rolling('168H', min_periods=0).sum()
    ).fillna(0)

    # === 3. ì‹œê°„ ê°„ê²© í”¼ì²˜ ===
    df['time_since_last'] = df.groupby('nameOrig')['step'].diff().fillna(0)
    df['avg_time_gap'] = df.groupby('nameOrig')['step'].transform(
        lambda x: x.diff().expanding().mean()
    ).fillna(0)

    # === 4. ì”ì•¡ ê´€ë ¨ í”¼ì²˜ ===
    df['balance_ratio'] = df['newbalanceOrig'] / (df['oldbalanceOrg'] + 1e-6)
    df['balance_drop_pct'] = (df['oldbalanceOrg'] - df['newbalanceOrig']) / (df['oldbalanceOrg'] + 1e-6)

    # === 5. íŒ¨í„´ íƒì§€ í”¼ì²˜ ===
    # ê°™ì€ ìˆ˜ì·¨ì ê±°ë˜ íšŸìˆ˜ (cumcount)
    df['same_dest_count'] = df.groupby(['nameOrig', 'nameDest']).cumcount()

    # ì²« ì†¡ê¸ˆ ì—¬ë¶€
    df['is_transfer'] = (df['type'] == 'TRANSFER').astype(int)
    df['transfer_cumsum'] = df.groupby('nameOrig')['is_transfer'].cumsum() - df['is_transfer']
    df['is_first_transfer'] = ((df['is_transfer'] == 1) & (df['transfer_cumsum'] == 0)).astype(int)

    # ì¸ë±ìŠ¤ ë³µì› ë° ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    df = df.reset_index(drop=True)
    df = df.drop(columns=['is_transfer', 'transfer_cumsum'], errors='ignore')

    feature_cols = [
        'tx_count_1h', 'tx_count_24h', 'tx_count_7d',
        'amt_sum_1h', 'amt_sum_24h', 'amt_sum_7d',
        'time_since_last', 'avg_time_gap',
        'balance_ratio', 'balance_drop_pct',
        'same_dest_count', 'is_first_transfer'
    ]

    return df[feature_cols]
```

> **ì„±ëŠ¥ ë¹„êµ**: 600ë§Œ ê±´ ê¸°ì¤€
> - ê¸°ì¡´ O(nÂ²) ë£¨í”„: ~ìˆ˜ ì‹œê°„
> - Vectorized O(n): ~30ì´ˆ

### í•µì‹¬ ì½”ë“œ: ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬

```python
def benchmark_inference(model, X_sample, n_runs=100):
    """ë‹¨ì¼ ìƒ˜í”Œ ì¶”ë¡  ì†ë„ ì¸¡ì •"""
    times = []
    for _ in range(n_runs):
        start = time.perf_counter()
        _ = model.predict_proba(X_sample.reshape(1, -1))
        times.append((time.perf_counter() - start) * 1000)  # ms
    return np.mean(times), np.std(times)

# ê° ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
results = []
for name, model in models.items():
    mean_ms, std_ms = benchmark_inference(model, X_test[0])
    results.append({'Model': name, 'Latency (ms)': f"{mean_ms:.2f} Â± {std_ms:.2f}"})

print(pd.DataFrame(results).to_markdown(index=False))
```

### ì˜ˆìƒ ì‹¤í—˜ ê²°ê³¼

| ëª¨ë¸ | AUC | AUPRC | ì¶”ë¡  ì†ë„ (ms) |
|------|-----|-------|---------------|
| XGBoost | 0.93+ | 0.60+ | ~0.5 |
| íŠ¸ë¦¬ ìŠ¤íƒœí‚¹ | 0.94+ | 0.62+ | ~1.5 |
| LSTM | 0.90+ | 0.55+ | ~15 |
| Transformer | 0.92+ | 0.58+ | ~30 |

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ PaySimìœ¼ë¡œ ì¬ì‹¤í—˜í–ˆë‚˜ìš”?"
> "IEEE-CISì—ì„œ LSTM AUC 0.70ìœ¼ë¡œ ì‹¤íŒ¨í•œ ì›ì¸ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. V1~V339ê°€ PCA ìµëª…í™”ëœ í”¼ì²˜ë¼ì„œ ì‹œê³„ì—´ íŒ¨í„´ì´ ì—†ì—ˆìŠµë‹ˆë‹¤. PaySimì€ ì‹¤ì œ ê±°ë˜ ì‹œí€€ìŠ¤ê°€ ìˆì–´ì„œ ML vs DL ê³µì • ë¹„êµê°€ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤."

Q: "ì‹œê°„ ìœˆë„ìš° ì§‘ê³„ í”¼ì²˜ëŠ” ì–´ë–»ê²Œ ì„¤ê³„í–ˆë‚˜ìš”?"
> "í˜„ì—… ë…¼ë¬¸ê³¼ ë¸”ë¡œê·¸ë¥¼ ì°¸ê³ í•´ì„œ 12ê°œ í”¼ì²˜ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. 1ì‹œê°„/24ì‹œê°„/7ì¼ ìœˆë„ìš°ë³„ ê±°ë˜ ë¹ˆë„ì™€ ê¸ˆì•¡, ì”ì•¡ ë³€í™”ìœ¨, ê°™ì€ ìˆ˜ì·¨ì ë°˜ë³µ íŒ¨í„´ ë“±ì…ë‹ˆë‹¤. ì´ í”¼ì²˜ë“¤ì´ +200% ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•œë‹¤ëŠ” ì—°êµ¬ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤."

Q: "ì¶”ë¡  ì†ë„ ì¸¡ì • ê²°ê³¼ëŠ”?"
> "XGBoost 0.5ms, LSTM 15msë¡œ 30ë°° ì°¨ì´ë‚¬ìŠµë‹ˆë‹¤. LSTMì´ ì„±ëŠ¥ì€ ì¢‹ì§€ë§Œ ì‹¤ì‹œê°„ ì„œë¹™ì— ë¶€ì í•©í•´ì„œ, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 1-12ì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤."

---

## 1-12: í•˜ì´ë¸Œë¦¬ë“œ ì„œë¹™ (Day 12) - ì„ íƒ â­â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
import torch
import torch.nn as nn
import redis
import pickle
import numpy as np
from xgboost import XGBClassifier
import time
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ë¬¸ì œ ì •ì˜**
- 1-11 ê²°ê³¼: LSTM AUC 0.90+ ë‹¬ì„±í•˜ì§€ë§Œ ì¶”ë¡  ì†ë„ 15ms
- XGBoost: AUC 0.93+, ì¶”ë¡  ì†ë„ 0.5ms
- ëª©í‘œ: DL ì„±ëŠ¥ + XGBoost ì†ë„ ê²°í•©

**2. í•´ê²°ì±…: NVIDIA ë ˆí¼ëŸ°ìŠ¤ ì•„í‚¤í…ì²˜**
```
ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ (1ì‹œê°„ë§ˆë‹¤):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ì „ì²´ ê³ ê° ì‹œí€€ìŠ¤ ë¡œë“œ            â”‚
â”‚ 2. LSTMìœ¼ë¡œ ê³ ê°ë³„ ì„ë² ë”© ê³„ì‚°      â”‚
â”‚ 3. Redisì— ì €ì¥ (key: customer_id)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ (ê±°ë˜ ë°œìƒ ì‹œ):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Redisì—ì„œ ì„ë² ë”© ì¡°íšŒ (0.1ms)    â”‚
â”‚ 2. ì›ë³¸ í”¼ì²˜ + ì„ë² ë”© ê²°í•©          â”‚
â”‚ 3. XGBoost ì¶”ë¡  (0.5ms)             â”‚
â”‚ 4. ì´ < 1ms                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**3. ì™œ Redis?**
- In-memory â†’ ì¡°íšŒ 0.1ms ë¯¸ë§Œ
- í˜„ì—… í‘œì¤€ (Feedzai, Stripe ë“±ì—ì„œ ì‚¬ìš©)
- Dockerë¡œ ì‰½ê²Œ êµ¬ì„±

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: LSTM ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜ êµ¬í˜„
- ì‹¤ìŠµ 2: Redis ì—°ê²° ë° ì„ë² ë”© ì €ì¥/ë¡œë“œ
- ì‹¤ìŠµ 3: í•˜ì´ë¸Œë¦¬ë“œ XGBoost í•™ìŠµ (ì›ë³¸ + ì„ë² ë”©)
- ì‹¤ìŠµ 4: ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ (LSTM ì§ì ‘ vs í•˜ì´ë¸Œë¦¬ë“œ)
- ì‹¤ìŠµ 5: ì„±ëŠ¥ ë¹„êµ (XGBoost ë‹¨ë… vs í•˜ì´ë¸Œë¦¬ë“œ)

### í•µì‹¬ ì½”ë“œ: LSTM ì„ë² ë”© ì¶”ì¶œ

```python
class LSTMEmbedder(nn.Module):
    """LSTMì—ì„œ ì„ë² ë”©ë§Œ ì¶”ì¶œ (ë¶„ë¥˜ í—¤ë“œ ì œì™¸)"""
    def __init__(self, input_size, hidden_size=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                           batch_first=True, dropout=0.2)

    def forward(self, x):
        _, (h_n, _) = self.lstm(x)
        return h_n[-1]  # (batch, hidden_size)

def extract_user_embeddings(model, user_sequences, device):
    """ê³ ê°ë³„ ì„ë² ë”© ì¶”ì¶œ"""
    model.eval()
    embeddings = {}

    with torch.no_grad():
        for user_id, seq in user_sequences.items():
            seq_tensor = torch.FloatTensor(seq).unsqueeze(0).to(device)
            emb = model(seq_tensor).cpu().numpy().flatten()
            embeddings[user_id] = emb

    return embeddings
```

### í•µì‹¬ ì½”ë“œ: Redis ì„ë² ë”© ì €ì¥/ë¡œë“œ

```python
# Redis ì—°ê²°
r = redis.Redis(host='localhost', port=6379, db=0)

EMBEDDING_DIM = 64  # LSTM hidden_size

def save_embeddings_to_redis(embeddings: dict[str, np.ndarray], batch_size: int = 1000):
    """
    ì„ë² ë”©ì„ Redisì— ì €ì¥ (Pipeline + tobytesë¡œ ìµœì í™”)

    - pickle ëŒ€ì‹  tobytes: ì§ë ¬í™” ì˜¤ë²„í—¤ë“œ ì œê±°
    - Pipeline: ë„¤íŠ¸ì›Œí¬ ì™•ë³µ ìµœì†Œí™” (100ë°° ë¹ ë¦„)
    """
    pipe = r.pipeline()
    count = 0

    for user_id, emb in embeddings.items():
        # tobytes()ëŠ” pickleë³´ë‹¤ ~10ë°° ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
        pipe.hset(f"emb:{user_id}", mapping={
            "vector": emb.astype(np.float32).tobytes(),
            "dim": EMBEDDING_DIM
        })
        count += 1

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        if count % batch_size == 0:
            pipe.execute()
            pipe = r.pipeline()

    pipe.execute()  # ë‚¨ì€ ê²ƒ ì²˜ë¦¬
    print(f"Saved {len(embeddings)} embeddings to Redis")

def load_embedding_from_redis(user_id: str) -> np.ndarray:
    """Redisì—ì„œ ì„ë² ë”© ì¡°íšŒ (0.1ms ë¯¸ë§Œ)"""
    data = r.hget(f"emb:{user_id}", "vector")
    if data:
        return np.frombuffer(data, dtype=np.float32)
    return np.zeros(EMBEDDING_DIM, dtype=np.float32)  # fallback (ì‹ ê·œ ê³ ê°)

# ë°°ì¹˜ ì €ì¥
embeddings = extract_user_embeddings(lstm_embedder, user_sequences, device)
save_embeddings_to_redis(embeddings)
```

> **ìµœì í™” í¬ì¸íŠ¸**:
> - `tobytes()` + `frombuffer()`: pickle ëŒ€ë¹„ ~10ë°° ë¹ ë¦„
> - Pipeline: 1ë§Œ ê±´ ì €ì¥ ì‹œ 10ì´ˆ â†’ 0.1ì´ˆ

### í•µì‹¬ ì½”ë“œ: í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ 

```python
def hybrid_predict(user_id, transaction_features, xgb_model, redis_client):
    """í•˜ì´ë¸Œë¦¬ë“œ ì‹¤ì‹œê°„ ì¶”ë¡ """
    start = time.perf_counter()

    # 1. Redisì—ì„œ ì„ë² ë”© ì¡°íšŒ
    embedding = load_embedding_from_redis(user_id)

    # 2. ì›ë³¸ í”¼ì²˜ + ì„ë² ë”© ê²°í•©
    hybrid_features = np.concatenate([transaction_features, embedding])

    # 3. XGBoost ì¶”ë¡ 
    prob = xgb_model.predict_proba(hybrid_features.reshape(1, -1))[0, 1]

    latency = (time.perf_counter() - start) * 1000
    return prob, latency

# ë²¤ì¹˜ë§ˆí¬
latencies = []
for _ in range(100):
    _, latency = hybrid_predict(test_user, test_features, xgb_hybrid, r)
    latencies.append(latency)

print(f"Hybrid latency: {np.mean(latencies):.2f} Â± {np.std(latencies):.2f} ms")
```

### ì‹¤ì œ ì‹¤í—˜ ê²°ê³¼ (5ê°œ ëª¨ë¸ ë¹„êµ)

| ëª¨ë¸ | AUC | Recall@5%FPR | ì¶”ë¡  ì†ë„ |
|------|-----|--------------|-----------|
| XGBoost ë‹¨ë… | 0.9997 | 99.92% | 0.38ms |
| FT-Transformer | 0.9995 | 99.86% | 24.58ms |
| í•˜ì´ë¸Œë¦¬ë“œ (XGB+ì„ë² ë”©) | **0.9997** | **99.95%** | 1.03ms |
| ìŠ¤íƒœí‚¹ (3-Tree) | **0.9998** | 99.92% | 1.63ms |
| í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤íƒœí‚¹ | 0.9992 | 99.89% | 2.35ms |

**í•µì‹¬ ë°œê²¬:**
- **ìŠ¤íƒœí‚¹ì´ AUC ìµœê³ ** (0.9998) - DL ì—†ì´ íŠ¸ë¦¬ ì•™ìƒë¸”ë§Œìœ¼ë¡œ ìµœê³ 
- **í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤íƒœí‚¹ ì„±ëŠ¥ í•˜ë½** (0.9992) - ê³¼ì í•©/ì •ë³´ ì¤‘ë³µ ë¬¸ì œ
- **í•˜ì´ë¸Œë¦¬ë“œê°€ Recall ìµœê³ ** (99.95%) - DL ì„ë² ë”© íš¨ê³¼
- PaySim íŠ¹ì„±: ëª¨ë“  ëª¨ë¸ AUC 0.999+ ìˆ˜ë ´ â†’ ì‹¤ìš©ì„± ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ

### Docker ì„¤ì •

```yaml
# docker-compose.ymlì— ì¶”ê°€
services:
  redis:
    image: redis:latest
    ports:
      - "6379:6379"
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "í•˜ì´ë¸Œë¦¬ë“œë¡œ ì–´ë–»ê²Œ ì†ë„ë¥¼ ê°œì„ í–ˆë‚˜ìš”?"
> "LSTM ì¶”ë¡ ì´ 15msë¡œ ëŠë ¤ì„œ, ë°°ì¹˜ë¡œ ê³ ê°ë³„ ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•´ì„œ Redisì— ìºì‹±í–ˆìŠµë‹ˆë‹¤. ì‹¤ì‹œê°„ì—ëŠ” Redis ì¡°íšŒ(0.1ms) + XGBoost(0.5ms)ë¡œ ì´ 0.6msì— ì¶”ë¡ í•©ë‹ˆë‹¤. LSTM ì§ì ‘ ì‹¤í–‰ ëŒ€ë¹„ 25ë°° ë¹¨ë¼ì¡ŒìŠµë‹ˆë‹¤."

Q: "ì´ ì•„í‚¤í…ì²˜ì˜ ì¥ì ì€?"
> "NVIDIA ë ˆí¼ëŸ°ìŠ¤ ì•„í‚¤í…ì²˜ íŒ¨í„´ì…ë‹ˆë‹¤. DLì˜ íŒ¨í„´ ì¸ì‹ë ¥(ì„ë² ë”©)ê³¼ XGBoostì˜ ì†ë„/ì„¤ëª…ì„±ì„ ê²°í•©í•©ë‹ˆë‹¤. Redis ìºì‹±ìœ¼ë¡œ ì‹¤ì‹œê°„ ì„œë¹™ì´ ê°€ëŠ¥í•˜ê³ , ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì£¼ê¸°(1ì‹œê°„)ë¡œ ì„ë² ë”© ì‹ ì„ ë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤."

Q: "ì„ë² ë”©ì´ ì˜¤ë˜ëœ ê²½ìš°ëŠ”?"
> "ìµœëŒ€ 1ì‹œê°„ ì§€ì—°ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê³ ê° í–‰ë™ íŒ¨í„´ì€ ê¸‰ê²©íˆ ë³€í•˜ì§€ ì•Šê³ , XGBoostê°€ ì‹¤ì‹œê°„ í”¼ì²˜(í˜„ì¬ ê±°ë˜ ì •ë³´)ë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ ì¶©ë¶„íˆ ë³´ì™„ë©ë‹ˆë‹¤. í•„ìš”ì‹œ ì—…ë°ì´íŠ¸ ì£¼ê¸°ë¥¼ 10ë¶„ìœ¼ë¡œ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

Q: "ì™œ 5ê°€ì§€ ëª¨ë¸ì„ ë¹„êµí–ˆë‚˜ìš”?"
> "ë™ì¼ PaySim ë°ì´í„°ì—ì„œ ê³µì • ë¹„êµë¥¼ ìœ„í•´:
> 1. XGBoost ë‹¨ë… - ë² ì´ìŠ¤ë¼ì¸
> 2. FT-Transformer - DL ì§ì ‘ ì¶”ë¡ 
> 3. í•˜ì´ë¸Œë¦¬ë“œ (XGB+ì„ë² ë”©) - NVIDIA Blueprint íŒ¨í„´
> 4. ìŠ¤íƒœí‚¹ (3-Tree) - íŠ¸ë¦¬ ì•™ìƒë¸” ë‹¤ì–‘ì„±
> 5. í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤íƒœí‚¹ - ìŠ¤íƒœí‚¹ + DL ì„ë² ë”© ê²°í•©
> ê²°ë¡ : ìŠ¤íƒœí‚¹ì´ AUC ìµœê³ (0.9998)ì§€ë§Œ, ì‹¤ë¬´ì—ì„œëŠ” ì†ë„ ëŒ€ë¹„ ê°œì„  í­ì´ ì‘ì•„ì„œ XGBoost ë‹¨ë… ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œê°€ ì‹¤ìš©ì ì…ë‹ˆë‹¤."

Q: "í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤íƒœí‚¹ì´ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë‚®ì•„ì§„ ì´ìœ ëŠ”?"
> "í”¼ì²˜ê°€ 35ê°œ(í™•ë¥  3ê°œ + ì„ë² ë”© 32ì°¨ì›)ë¡œ ë§ì•„ì ¸ì„œ LogisticRegressionì´ ê³¼ì í•©ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ ì„ë² ë”©ê³¼ ìŠ¤íƒœí‚¹ í™•ë¥  ê°„ ì •ë³´ ì¤‘ë³µ(redundancy)ì´ ë°œìƒí–ˆê³ , ìŠ¤ì¼€ì¼ ë¶ˆì¼ì¹˜(í™•ë¥  0~1 vs ì„ë² ë”© -2~+2)ë„ ì›ì¸ì…ë‹ˆë‹¤. ì´ë¡ ìƒ 'ë” ë§ì€ ì •ë³´ = ë” ì¢‹ì€ ì„±ëŠ¥'ì´ ì•„ë‹˜ì„ ë³´ì—¬ì£¼ëŠ” ì‚¬ë¡€ì…ë‹ˆë‹¤."

---

## ì „ì²´ ìš”ì•½

| ë…¸íŠ¸ë¶ | ì‹œê°„ | í•µì‹¬ ì‚°ì¶œë¬¼ |
|--------|------|------------|
| 1-1 | 3h | train.csv, test.csv |
| 1-2 | 4h | preprocessing.py, X_tabular, X_sequence |
| 1-3 | 4h | ëª¨ë¸ ë¹„êµ í‘œ, xgb_model.pkl, xgb_importance.csv |
| 1-4 | 4h | src/models/lstm.py, lstm_model.pt |
| 1-5 | 3h | ì•™ìƒë¸” ì„±ëŠ¥ ë¹„êµ í‘œ |
| 1-6 | 3h | shap_explainer.py, ì„¤ëª… ì‹œê°í™” |
| 1-7 | 4h | FastAPI, Docker, í†µí•© í…ŒìŠ¤íŠ¸ |
| 1-8 | 4h | React Admin (ê±°ë˜ ëª©ë¡ + ìƒì„¸) |
| 1-9 | 4h | íŠ¸ë¦¬ ìŠ¤íƒœí‚¹ (XGB+LGBM+Cat), stacking_model.pkl â­â­ |
| 1-10 | 5h | TabTransformer, transformer_model.pt (ì„ íƒ) |
| 1-11 | 4h | í•˜ì´ë¸Œë¦¬ë“œ (DLì„ë² ë”©+XGB), hybrid_model.pkl (ì„ íƒ) |
| 1-12 | 4h | PaySim ì‹œí€€ìŠ¤ ì‹¤í—˜, LSTM ê²€ì¦ (ì„ íƒ) |

**ì´ ì•½ 46ì‹œê°„ (12ì¼, ì „ì²´ ì„ íƒ ì‹œ)**
**í•„ìˆ˜ë§Œ: ~37ì‹œê°„ (1-1~1-9)**

---

## í•µì‹¬ ì‹¤í—˜ ê²°ê³¼ (ë©´ì ‘ìš©)

### 1. ëª¨ë¸ ë¹„êµ (1-3)

| Model | AUC | Time(s) | SHAP |
|-------|-----|---------|------|
| XGBoost | 0.9114 | 45 | âœ… ìµœìƒ |
| LightGBM | 0.91 | 32 | âœ… ì¢‹ìŒ |
| CatBoost | 0.91 | 98 | âš ï¸ ì œí•œ |

### ì‹¤ì œ ë‹¬ì„± ì§€í‘œ (ê²€ì¦ ì™„ë£Œ)

| ì§€í‘œ | ê°’ | í˜„ì—… ê¸°ì¤€ |
|------|-----|----------|
| AUC-ROC | 0.9114 | â‰¥0.90 âœ… |
| Recall | 90.55% | 80-95% âœ… |
| Precision | 9.78% | 5-30% âœ… |
| AUPRC | 0.5313 | â‰¥0.50 âœ… |

### ë‹¤ë‹¨ê³„ ìœ„í—˜ë„ (4ë‹¨ê³„)

```
approve: 0.00 ~ 0.18 (ìŠ¹ì¸) - 67%
verify:  0.18 ~ 0.40 (ì¶”ê°€ì¸ì¦) - 21%
hold:    0.40 ~ 0.65 (ë³´ë¥˜) - 7%
block:   0.65 ì´ìƒ   (ì°¨ë‹¨) - 5%
```

### 2. LSTM ë‹¨ê³„ë³„ ê°œì„  (1-4)

| ë‹¨ê³„ | êµ¬ì„± | AUC |
|------|------|-----|
| ë² ì´ìŠ¤ë¼ì¸ | V1~V20 + ì‹œê³„ì—´ í”¼ì²˜ (35ê°œ) | 0.82 |
| + Optuna íŠœë‹ | í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” | 0.84 |
| + í”¼ì²˜ ì¶”ê°€ | XGBoost importance ê¸°ë°˜ | 0.86 |

### 3. ì•™ìƒë¸” ì‹¤í—˜ ê²°ê³¼ (1-5)

| Model | AUC | ê²°ë¡  |
|-------|-----|------|
| XGBoost | 0.9114 | **ì±„íƒ** |
| LSTM | 0.7054 | ì„±ëŠ¥ ë‚®ìŒ |
| Ensemble (0.9:0.1) | 0.9054 | +0.12% (íš¨ê³¼ ë¯¸ë¯¸) |

âœ… XGBoost ë‹¨ë… AUC 0.91 â†’ ëª©í‘œ ë‹¬ì„±, ì•™ìƒë¸” ë¶ˆí•„ìš”

### 4. ì•™ìƒë¸” ì‹¤í—˜ ê²°ë¡ 

| ë°©ë²• | AUC | ê²°ë¡  |
|------|-----|------|
| XGBoost ë‹¨ë… | 0.9042 | **ì±„íƒ** |
| LSTM ë‹¨ë… | 0.7054 | ì„±ëŠ¥ ë‚®ìŒ |
| ì•™ìƒë¸” (0.9:0.1) | 0.9054 | +0.12% (íš¨ê³¼ ë¯¸ë¯¸) |

**ì´ í‘œê°€ ë©´ì ‘ì—ì„œ "ì™œ XGBoost ë‹¨ë…?"ì— ëŒ€í•œ ê·¼ê±°!**

**í•µì‹¬ ìŠ¤í† ë¦¬:**
> "LSTM ì•™ìƒë¸”ì„ ì‹œë„í–ˆì§€ë§Œ +0.12% í–¥ìƒì— ê·¸ì¹¨ â†’ ë³µì¡ë„ ëŒ€ë¹„ íš¨ê³¼ ë¶„ì„ â†’ XGBoost ë‹¨ë… ì±„íƒ â†’ ë”¥ëŸ¬ë‹ì´ í•­ìƒ ì¢‹ì€ ê±´ ì•„ë‹ˆë‹¤"

---

---

## Lessons Learned: APIì™€ í•™ìŠµ ì½”ë“œ ì¼ê´€ì„±

### ë°œìƒí•œ ë¬¸ì œ

API ì½”ë“œì™€ í•™ìŠµ ì½”ë“œê°€ ë¶„ë¦¬ë˜ì–´ ì‘ì„±ë˜ë©´ì„œ ë‹¤ìŒ ë¬¸ì œ ë°œìƒ:

```
í•™ìŠµ ì½”ë“œ (ë…¸íŠ¸ë¶): LabelEncoder â†’ ProductCD "W" = 4
API ì½”ë“œ (predictor.py): CATEGORY_MAPPINGS â†’ ProductCD "w" = 0
â†’ ì™„ì „íˆ ë‹¤ë¥¸ ê°’! â†’ Recall 0%
```

### í•´ê²° ë°©ë²•

1. **ì „ì²˜ë¦¬ëœ í”¼ì²˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©**: `/samples` APIì—ì„œ 447ê°œ í”¼ì²˜ ì „ì²´ ë°˜í™˜
2. **ìƒˆ ì—”ë“œí¬ì¸íŠ¸**: `/predict/direct/batch` - ì¸ì½”ë”© ë³€í™˜ ì—†ì´ ë°”ë¡œ ì˜ˆì¸¡
3. **Recall ê²€ì¦ ë…¸íŠ¸ë¶ ì¶”ê°€**: `1-3-1_recall_check.ipynb`

### ë©´ì ‘ ì–´í•„ í¬ì¸íŠ¸

> "í•™ìŠµ ì½”ë“œì™€ API ì½”ë“œ ë¶ˆì¼ì¹˜ë¡œ Recall 0% ì´ìŠˆ ë°œìƒ. ì›ì¸ ë¶„ì„ í›„ ì „ì²˜ë¦¬ëœ í”¼ì²˜ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•´ê²°. ì´ ê²½í—˜ìœ¼ë¡œ í•™ìŠµ-ì„œë¹™ ì¼ê´€ì„±ì˜ ì¤‘ìš”ì„±ì„ ê¹Šì´ ì´í•´í•˜ê²Œ ë¨."

---

## ë‹¤ìŒ ë‹¨ê³„: Phase 2

> ìƒì„¸: [docs/roadmap.md](./roadmap.md)

Phase 1 ì™„ë£Œ í›„ Phase 2ì—ì„œ ì¶”ê°€:
- **MLflow**: ì‹¤í—˜ ì¶”ì , ëª¨ë¸ ë²„ì „ ê´€ë¦¬
- **Evidently**: ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§
- **GitHub Actions**: CI/CD
- **ë¹„ìš© ê¸°ë°˜ ìµœì í™”**: ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ê³„ì‚°
