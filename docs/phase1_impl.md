# Phase 1: FDS + Ensemble + XAI - êµ¬í˜„ ìƒì„¸ (AIìš©)

> ë…¸íŠ¸ë¶/ì½”ë“œ ìƒì„±ì„ ìœ„í•œ ìƒì„¸ ìŠ¤í™

---

## âš ï¸ êµ¬í˜„ ë°©ì‹ (í•„ë…)

**ê° Day ë…¸íŠ¸ë¶ì€ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬í˜„í•´ì•¼ í•¨:**

### ë…¸íŠ¸ë¶ êµ¬ì¡°

```
[ë§ˆí¬ë‹¤ìš´] ì œëª© + í•™ìŠµ ëª©í‘œ
[ì½”ë“œ] íŒ¨í‚¤ì§€ ì„í¬íŠ¸
[ë§ˆí¬ë‹¤ìš´] ğŸ“š ê°œë… ì„¤ëª…
[ì½”ë“œ] ì˜ˆì œ ì½”ë“œ (ì™„ì„±ë³¸)
[ë§ˆí¬ë‹¤ìš´] ğŸ’» ì‹¤ìŠµ N ì„¤ëª…
[ì½”ë“œ] ì‹¤ìŠµ TODO (ë¹ˆì¹¸)
[ì½”ë“œ] âœ… ì‹¤ìŠµ ì •ë‹µ (ì±„ìš´ ë²„ì „)
[ì½”ë“œ] ì²´í¬í¬ì¸íŠ¸ (assert)
... (ë°˜ë³µ)
[ë§ˆí¬ë‹¤ìš´] ìµœì¢… ìš”ì•½ + ë©´ì ‘ í¬ì¸íŠ¸
```

### í¬í•¨ ìš”ì†Œ

| ìš”ì†Œ | ì„¤ëª… |
|------|------|
| ğŸ“š ê°œë… ì„¤ëª… | ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ê°œë…/ì´ë¡  ì„¤ëª… |
| ì˜ˆì œ ì½”ë“œ | ì™„ì„±ëœ ì˜ˆì œ (í•™ìŠµìš©) |
| ğŸ’» ì‹¤ìŠµ TODO | ë¹ˆì¹¸/TODO í¬í•¨ëœ ì‹¤ìŠµ ì½”ë“œ |
| âœ… ì‹¤ìŠµ ì •ë‹µ | TODO ì±„ìš´ ì •ë‹µ ì½”ë“œ |
| ì²´í¬í¬ì¸íŠ¸ | assertë¡œ ê²€ì¦ |
| ë©´ì ‘ í¬ì¸íŠ¸ | í•´ë‹¹ Day ê´€ë ¨ ë©´ì ‘ Q&A |

### src/ ëª¨ë“ˆí™”

1-2ë¶€í„° ê²€ì¦ëœ ì½”ë“œë¥¼ src/ë¡œ ëª¨ë“ˆí™”:

```
ë…¸íŠ¸ë¶ì—ì„œ ì½”ë“œ ì‘ì„± + ì‹¤í—˜
    â†“
ê²€ì¦ë˜ë©´ src/ë¡œ ëª¨ë“ˆí™”
    â†“
ë…¸íŠ¸ë¶ì—ì„œ ëª¨ë“ˆ importí•´ì„œ ì‚¬ìš©
```

---

## íŒŒì¼ êµ¬ì¡°

```
fds-system/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ phase1/
â”‚       â”œâ”€â”€ 1-1_data_eda.ipynb
â”‚       â”œâ”€â”€ 1-2_feature_engineering.ipynb
â”‚       â”œâ”€â”€ 1-3_xgboost.ipynb
â”‚       â”œâ”€â”€ 1-4_lstm.ipynb
â”‚       â”œâ”€â”€ 1-5_ensemble.ipynb
â”‚       â”œâ”€â”€ 1-6_shap.ipynb
â”‚       â””â”€â”€ 1-7_fastapi.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   â”œâ”€â”€ xgboost_model.py
â”‚   â”‚   â”œâ”€â”€ lstm_model.py
â”‚   â”‚   â””â”€â”€ ensemble.py
â”‚   â”œâ”€â”€ explainer/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ shap_explainer.py
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py
â”‚       â””â”€â”€ schemas.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # IEEE-CIS ì›ë³¸
â”‚   â””â”€â”€ processed/            # ì „ì²˜ë¦¬ ë°ì´í„°
â”œâ”€â”€ models/                   # í•™ìŠµëœ ëª¨ë¸ (.pkl, .pt)
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

---

## 1-1: ë°ì´í„° + EDA (Day 1)

### í•„ìš” íŒ¨í‚¤ì§€
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. IEEE-CIS ë°ì´í„°ì…‹**
- Kaggleì—ì„œ ë‹¤ìš´ë¡œë“œ
- Transaction í…Œì´ë¸”: ê±°ë˜ ì •ë³´ (ê¸ˆì•¡, ì‹œê°„, ì¹´ë“œ)
- Identity í…Œì´ë¸”: ê¸°ê¸°/ë¸Œë¼ìš°ì € ì •ë³´
- TransactionIDë¡œ LEFT JOIN

**2. ê¸°ë³¸ EDA**
- shape, dtypes, info()
- head(), tail()
- describe()

**3. ë¶ˆê· í˜• ë°ì´í„°**
- íƒ€ê²Ÿ(isFraud) ë¶„í¬ í™•ì¸
- ì‚¬ê¸° ë¹„ìœ¨ ~3.5%
- Accuracyê°€ ë¬´ì˜ë¯¸í•œ ì´ìœ 
- í‰ê°€ ì§€í‘œ: AUC-ROC, PR-AUC, F1

**4. ê²°ì¸¡ì¹˜ ë¶„ì„**
- ì»¬ëŸ¼ë³„ ê²°ì¸¡ ë¹„ìœ¨
- ê²°ì¸¡ 50% ì´ìƒ ì»¬ëŸ¼ ì²˜ë¦¬ ì „ëµ
- Identity í…Œì´ë¸” ê²°ì¸¡ (ë³‘í•©ìœ¼ë¡œ ì¸í•œ)

**5. í”¼ì²˜ íƒìƒ‰**
- ìˆ˜ì¹˜í˜•: TransactionAmt, card1~5
- ë²”ì£¼í˜•: ProductCD, card4, card6
- ì‹œê°„: TransactionDT

**6. íƒ€ê²Ÿë³„ ë¶„ì„**
- ì •ìƒ vs ì‚¬ê¸° ê¸ˆì•¡ ë¶„í¬
- ì‹œê°„ëŒ€ë³„ ì‚¬ê¸° ë¹„ìœ¨
- ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ê¸° ë¹„ìœ¨

**7. ì‹œê°„ ê¸°ë°˜ ë¶„í• **
- ì™œ ëœë¤ ë¶„í• ì´ ì•ˆ ë˜ëŠ”ì§€ (Data Leakage)
- TransactionDT ê¸°ì¤€ ì •ë ¬
- 80/20 ë¶„í• 

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: ë°ì´í„° ë¡œë“œ ë° ë³‘í•© (LEFT JOIN)
- ì‹¤ìŠµ 2: íƒ€ê²Ÿ ë¶ˆê· í˜• ì‹œê°í™”
- ì‹¤ìŠµ 3: ê²°ì¸¡ì¹˜ ë¶„ì„ ë° ì²˜ë¦¬ ì „ëµ
- ì‹¤ìŠµ 4: íƒ€ê²Ÿë³„ ê¸ˆì•¡ ë¶„í¬ ë¹„êµ
- ì‹¤ìŠµ 5: ì‹œê°„ ê¸°ë°˜ train/test ë¶„í• 

### í•µì‹¬ ì½”ë“œ

```python
# ë°ì´í„° ë³‘í•©
df = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')

# ì‹œê°„ ê¸°ë°˜ ë¶„í• 
df_sorted = df.sort_values('TransactionDT')
split_idx = int(len(df_sorted) * 0.8)
train_df = df_sorted.iloc[:split_idx]
test_df = df_sorted.iloc[split_idx:]

# ê²€ì¦: ì‹œê°„ ìˆœì„œ í™•ì¸
assert train_df['TransactionDT'].max() <= test_df['TransactionDT'].min()
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ ëœë¤ ë¶„í• ì´ ì•„ë‹Œ ì‹œê°„ ê¸°ë°˜ ë¶„í• ì„ í•˜ë‚˜ìš”?"
> "ì‹¤ì œ FDSëŠ” ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµí•´ì„œ ë¯¸ë˜ ê±°ë˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ëœë¤ ë¶„í• ì€ ë¯¸ë˜ ì •ë³´ê°€ í•™ìŠµì— í¬í•¨ë˜ì–´ Data Leakageê°€ ë°œìƒí•©ë‹ˆë‹¤. ì‹œê°„ ê¸°ë°˜ ë¶„í• ì´ ì‹¤ì œ ìš´ì˜ í™˜ê²½ì„ ë°˜ì˜í•©ë‹ˆë‹¤."

---

## 1-2: Feature Engineering (Day 2)

### í•„ìš” íŒ¨í‚¤ì§€
```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì •í˜• í”¼ì²˜ (XGBoostìš©)**
- ì‹œê°„ í”¼ì²˜: hour, dayofweek, is_weekend, is_night
- ê¸ˆì•¡ í”¼ì²˜: amt_log, amt_bin, amt_decimal
- ì§‘ê³„ í”¼ì²˜: card1ë³„ ê±°ë˜ íšŸìˆ˜, í‰ê·  ê¸ˆì•¡
- ë²”ì£¼í˜• ì¸ì½”ë”©: LabelEncoder

**2. ì‹œê³„ì—´ í”¼ì²˜ (LSTMìš©)** â­
- ì‚¬ìš©ìë³„ ìµœê·¼ Nê°œ ê±°ë˜ ì‹œí€€ìŠ¤ ì¶”ì¶œ
- ì‹œí€€ìŠ¤ í”¼ì²˜: [ê¸ˆì•¡, ì‹œê°„ì°¨, ì¹´í…Œê³ ë¦¬, ...]
- Padding: ê±°ë˜ ìˆ˜ê°€ N ë¯¸ë§Œì´ë©´ 0ìœ¼ë¡œ ì±„ì›€
- Scaling: MinMaxScalerë¡œ 0~1 ì •ê·œí™”

**3. ì‹œí€€ìŠ¤ ìƒì„± ë¡œì§**
```
ì‚¬ìš©ì Aì˜ ê±°ë˜: [t1, t2, t3, t4, t5]
ì‹œí€€ìŠ¤ ê¸¸ì´ N=3ì¼ ë•Œ:
- t3 ì˜ˆì¸¡ìš©: [t1, t2] â†’ ê¸¸ì´ ë¶€ì¡± â†’ [0, t1, t2]
- t4 ì˜ˆì¸¡ìš©: [t1, t2, t3]
- t5 ì˜ˆì¸¡ìš©: [t2, t3, t4]
```

**4. í”¼ì²˜ ì €ì¥**
- X_tabular: ì •í˜• í”¼ì²˜ (DataFrame)
- X_sequence: ì‹œê³„ì—´ í”¼ì²˜ (3D array: samples x seq_len x features)

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: ì‹œê°„ í”¼ì²˜ ìƒì„±
- ì‹¤ìŠµ 2: ê¸ˆì•¡ í”¼ì²˜ ìƒì„±
- ì‹¤ìŠµ 3: ì§‘ê³„ í”¼ì²˜ ìƒì„±
- ì‹¤ìŠµ 4: ë²”ì£¼í˜• ì¸ì½”ë”©
- ì‹¤ìŠµ 5: ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± â­

### í•µì‹¬ ì½”ë“œ: ì‹œí€€ìŠ¤ ìƒì„±

```python
def create_sequences(df, user_col, features, seq_len=10):
    """ì‚¬ìš©ìë³„ ì‹œí€€ìŠ¤ ìƒì„±"""
    sequences = []
    labels = []

    for user_id, group in df.groupby(user_col):
        group = group.sort_values('TransactionDT')

        for i in range(len(group)):
            # í˜„ì¬ ê±°ë˜ ì´ì „ seq_lenê°œ ê±°ë˜
            start_idx = max(0, i - seq_len)
            seq = group.iloc[start_idx:i][features].values

            # íŒ¨ë”© (ê¸¸ì´ ë¶€ì¡± ì‹œ)
            if len(seq) < seq_len:
                pad = np.zeros((seq_len - len(seq), len(features)))
                seq = np.vstack([pad, seq])

            sequences.append(seq)
            labels.append(group.iloc[i]['isFraud'])

    return np.array(sequences), np.array(labels)

# ì‹œí€€ìŠ¤ í”¼ì²˜
seq_features = ['TransactionAmt_scaled', 'hour_scaled', 'time_diff_scaled']
X_seq, y_seq = create_sequences(train_df, 'card1', seq_features, seq_len=10)
print(f"Sequence shape: {X_seq.shape}")  # (samples, 10, 3)
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì‹œí€€ìŠ¤ ê¸¸ì´ 10ì€ ì–´ë–»ê²Œ ì •í–ˆë‚˜ìš”?"
> "5, 10, 15, 20ìœ¼ë¡œ ì‹¤í—˜í–ˆìŠµë‹ˆë‹¤. 10ì´ AUCì™€ í•™ìŠµ ì‹œê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ì—ì„œ ìµœì ì´ì—ˆìŠµë‹ˆë‹¤. 5ëŠ” íŒ¨í„´ì„ ëª» ì¡ê³ , 20ì€ í•™ìŠµ ì‹œê°„ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒì´ ë¯¸ë¯¸í–ˆìŠµë‹ˆë‹¤."

---

## 1-3: XGBoost ëª¨ë¸ (Day 3) â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score, precision_recall_curve
import optuna
import joblib
import time
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ëª¨ë¸ ë¹„êµ ì‹¤í—˜** â­
- XGBoost, LightGBM, CatBoost
- ë™ì¼ ì¡°ê±´ (ë™ì¼ í”¼ì²˜, ë™ì¼ ë¶„í• )
- AUC, í•™ìŠµ ì‹œê°„ ì¸¡ì •
- ê²°ê³¼ í‘œë¡œ ì •ë¦¬

**2. XGBoost ì„ íƒ ì´ìœ **
- AUC ìµœê³ 
- SHAP TreeExplainer í˜¸í™˜ì„± ìµœìƒ
- GPU í•™ìŠµ ì§€ì›

**3. Threshold ìµœì í™”** â­
- FN:FP ë¹„ìš© ë¹„ìœ¨ ì •ì˜ (10:1)
- ë¹„ìš© í•¨ìˆ˜ë¡œ ìµœì  threshold ì°¾ê¸°
- Precision-Recall Curve

**4. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
- íƒìƒ‰ ê³µê°„ ì •ì˜
- objective í•¨ìˆ˜
- n_trials ì„¤ì •

**5. n_estimatorsì™€ Early Stopping** â­
- í˜„ì—… íŠ¸ë¦¬ ê°œìˆ˜: 100~500ê°œ (ìƒí™©ë³„ ìƒì´)
- Early Stoppingìœ¼ë¡œ ìµœì  ê°œìˆ˜ ìë™ íƒìƒ‰
- GPU vs CPU ì„ íƒ ê¸°ì¤€

| ìƒí™© | íŠ¸ë¦¬ ìˆ˜ | ì´ìœ  |
|------|---------|------|
| ë¹ ë¥¸ ì‹¤í—˜ | 100 | ë² ì´ìŠ¤ë¼ì¸ |
| í”„ë¡œë•ì…˜ FDS | 200~500 | ì„±ëŠ¥ vs ì†ë„ ê· í˜• |
| Kaggle ëŒ€íšŒ | 1000+ | ìµœê³  ì„±ëŠ¥ |
| ì‹¤ì‹œê°„ API | 50~200 | ì‘ë‹µì†ë„ ì¤‘ìš” |

**6. ëª¨ë¸ ì €ì¥**
- joblib.dump
- ë©”íƒ€ë°ì´í„° í•¨ê»˜ ì €ì¥

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: 3ê°œ ëª¨ë¸ ë¹„êµ ì‹¤í—˜ â†’ ê²°ê³¼ í‘œ
- ì‹¤ìŠµ 2: XGBoost í•™ìŠµ
- ì‹¤ìŠµ 3: Threshold ë¹„ìš© ë¶„ì„ â†’ ê·¸ë˜í”„
- ì‹¤ìŠµ 4: Optuna íŠœë‹
- ì‹¤ìŠµ 5: ëª¨ë¸ ì €ì¥

### í•µì‹¬ ì½”ë“œ: ëª¨ë¸ ë¹„êµ

```python
models = {
    'XGBoost': XGBClassifier(n_estimators=100, tree_method='hist', device='cuda'),
    'LightGBM': LGBMClassifier(n_estimators=100, device='gpu'),
    'CatBoost': CatBoostClassifier(n_estimators=100, task_type='GPU', verbose=0),
}

results = []
for name, model in models.items():
    start = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start

    y_prob = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_prob)

    results.append({'Model': name, 'AUC': auc, 'Time(s)': train_time})

results_df = pd.DataFrame(results)
print(results_df.to_markdown(index=False))
```

### í•µì‹¬ ì½”ë“œ: Threshold ìµœì í™”

```python
def calculate_cost(threshold, y_true, y_prob, fn_cost=10, fp_cost=1):
    """ë¹„ìš© í•¨ìˆ˜: FNì´ FPë³´ë‹¤ 10ë°° ë¹„ìŒˆ"""
    y_pred = (y_prob >= threshold).astype(int)
    fn = ((y_true == 1) & (y_pred == 0)).sum()
    fp = ((y_true == 0) & (y_pred == 1)).sum()
    return fn * fn_cost + fp * fp_cost

# ìµœì  threshold ì°¾ê¸°
thresholds = np.arange(0.1, 0.9, 0.05)
costs = [calculate_cost(t, y_test, y_prob) for t in thresholds]
optimal_threshold = thresholds[np.argmin(costs)]
print(f"ìµœì  Threshold: {optimal_threshold:.2f}")
```

### í•µì‹¬ ì½”ë“œ: Early Stopping

```python
# Early Stoppingìœ¼ë¡œ ìµœì  íŠ¸ë¦¬ ê°œìˆ˜ ì°¾ê¸°
model = XGBClassifier(
    n_estimators=1000,           # ì¼ë‹¨ ë§ì´
    early_stopping_rounds=50,    # 50ë²ˆ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
    eval_metric='auc',
    device='cuda',
    random_state=42
)

model.fit(
    X_train, y_train,
    eval_set=[(X_valid, y_valid)],
    verbose=100  # 100ë²ˆë§ˆë‹¤ ì¶œë ¥
)

print(f"ì‹¤ì œ ì‚¬ìš©ëœ íŠ¸ë¦¬ ìˆ˜: {model.best_iteration}")  # ì˜ˆ: 287
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ XGBoostë¥¼ ì„ íƒí–ˆë‚˜ìš”?"
> "3ê°œ ëª¨ë¸ì„ ë™ì¼ ì¡°ê±´ì—ì„œ ë¹„êµí–ˆìŠµë‹ˆë‹¤. XGBoostê°€ AUC 0.92ë¡œ ê°€ì¥ ë†’ì•˜ê³ , SHAP TreeExplainer í˜¸í™˜ì„±ë„ ìµœìƒì´ì—ˆìŠµë‹ˆë‹¤."

Q: "ThresholdëŠ” ì–´ë–»ê²Œ ì •í–ˆë‚˜ìš”?"
> "FDSì—ì„œ FN(ë†“ì¹œ ì‚¬ê¸°)ì´ FP(ì˜¤íƒ)ë³´ë‹¤ ë¹„ìš©ì´ í½ë‹ˆë‹¤. FN:FP = 10:1ë¡œ ë¹„ìš© í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ìµœì†Œí™”í•˜ëŠ” Threshold 0.35ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."

Q: "n_estimatorsëŠ” ì–´ë–»ê²Œ ì •í–ˆë‚˜ìš”?"
> "Early Stoppingì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. n_estimators=1000ìœ¼ë¡œ ì„¤ì •í•˜ê³ , validation AUCê°€ 50 epoch ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” ì•½ 300ê°œ íŠ¸ë¦¬ì—ì„œ ìˆ˜ë ´í–ˆìŠµë‹ˆë‹¤."

Q: "GPUë¥¼ ì•ˆ ì“°ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
> "28ë§Œê±´ ë°ì´í„°ì—ì„œëŠ” CPUë„ ì¶©ë¶„íˆ ë¹ ë¦…ë‹ˆë‹¤ (0.6ì´ˆ). GPU ì˜¤ë²„í—¤ë“œ ë•Œë¬¸ì— ì˜¤íˆë ¤ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆ˜ë°±ë§Œê±´ ì´ìƒì—ì„œ GPUê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤."

---

## 1-4: LSTM ëª¨ë¸ (Day 4) â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì™œ LSTMì´ í•„ìš”í•œê°€?**
- XGBoostëŠ” ë‹¨ì¼ ê±°ë˜ë§Œ ë´„
- ì‚¬ê¸° íŒ¨í„´: "ì†Œì•¡ â†’ ì†Œì•¡ â†’ ê³ ì•¡"
- ì‹œí€€ìŠ¤ íŒ¨í„´ì€ LSTMì´ í•™ìŠµ

**2. LSTM êµ¬ì¡°**
- Input: (batch, seq_len, features)
- Hidden: 64
- Output: 1 (sigmoid)

**3. PyTorch êµ¬í˜„**
- Dataset í´ë˜ìŠ¤
- DataLoader
- BCELoss + Adam
- Early Stopping

**4. í•™ìŠµ ë£¨í”„**
- Train/Valid ë¶„ë¦¬
- Epochë³„ loss/AUC ì¶”ì 
- Best model ì €ì¥

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: Dataset í´ë˜ìŠ¤ êµ¬í˜„
- ì‹¤ìŠµ 2: LSTM ëª¨ë¸ ì •ì˜
- ì‹¤ìŠµ 3: í•™ìŠµ ë£¨í”„ êµ¬í˜„
- ì‹¤ìŠµ 4: Early Stopping
- ì‹¤ìŠµ 5: ëª¨ë¸ í‰ê°€ ë° ì €ì¥

### í•µì‹¬ ì½”ë“œ: LSTM ëª¨ë¸

```python
class FraudLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x: (batch, seq_len, features)
        _, (h_n, _) = self.lstm(x)
        # h_n: (num_layers, batch, hidden)
        out = self.fc(h_n[-1])
        return self.sigmoid(out)

# ëª¨ë¸ ì´ˆê¸°í™”
model = FraudLSTM(input_size=3, hidden_size=64)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

### í•µì‹¬ ì½”ë“œ: Dataset

```python
class FraudSequenceDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = torch.FloatTensor(sequences)
        self.labels = torch.FloatTensor(labels).unsqueeze(1)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]

# DataLoader
train_dataset = FraudSequenceDataset(X_seq_train, y_seq_train)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
```

### í•µì‹¬ ì½”ë“œ: í•™ìŠµ ë£¨í”„

```python
def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    for X, y in loader:
        X, y = X.to(device), y.to(device)

        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(loader)

# Early Stopping
best_auc = 0
patience = 5
counter = 0

for epoch in range(100):
    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
    val_auc = evaluate(model, val_loader, device)

    if val_auc > best_auc:
        best_auc = val_auc
        torch.save(model.state_dict(), 'models/lstm_best.pt')
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print(f"Early stopping at epoch {epoch}")
            break
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ LSTMì„ ì„ íƒí–ˆë‚˜ìš”?"
> "ê±°ë˜ ì‹œí€€ìŠ¤ì˜ ì‹œê°„ì  íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤. Transformerë„ ê³ ë ¤í–ˆì§€ë§Œ, ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 10~20ìœ¼ë¡œ ì§§ì•„ì„œ LSTMì´ ì¶©ë¶„í–ˆê³  í•™ìŠµë„ ë” ë¹ ë¦…ë‹ˆë‹¤."

Q: "ì™œ Transformerê°€ ì•„ë‹Œê°€ìš”?"
> "ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì§§ìŠµë‹ˆë‹¤. TransformerëŠ” ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ê°•ì ì´ ìˆì§€ë§Œ, 10~20ê°œ ì‹œí€€ìŠ¤ì—ì„œëŠ” LSTMê³¼ ì„±ëŠ¥ ì°¨ì´ê°€ ê±°ì˜ ì—†ê³  êµ¬í˜„ë„ ê°„ë‹¨í•©ë‹ˆë‹¤."

---

## 1-5: Ensemble + í‰ê°€ (Day 5) â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
import numpy as np
from sklearn.metrics import roc_auc_score, classification_report
import matplotlib.pyplot as plt
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì•™ìƒë¸” ì „ëµ ë¹„êµ**
- Simple Average: (p_xgb + p_lstm) / 2
- Weighted Average: w1*p_xgb + w2*p_lstm âœ…
- Stacking: ë©”íƒ€ ëª¨ë¸ í•™ìŠµ

**2. ê°€ì¤‘ì¹˜ ìµœì í™”**
- Grid Searchë¡œ ìµœì  ê°€ì¤‘ì¹˜ íƒìƒ‰
- Validation set ê¸°ì¤€
- ê²°ê³¼: XGBoost 0.6, LSTM 0.4

**3. ì„±ëŠ¥ ë¹„êµ**
- ë‹¨ë… ëª¨ë¸ vs ì•™ìƒë¸”
- AUC, Recall, Precision
- ê²°ê³¼ í‘œ + ê·¸ë˜í”„

**4. ì™œ ì•™ìƒë¸”ì´ íš¨ê³¼ì ì¸ê°€?**
- XGBoost: ì •í˜• íŠ¹ì„± ì´ìƒì¹˜ íƒì§€
- LSTM: ì‹œê³„ì—´ íŒ¨í„´ íƒì§€
- ìƒí˜¸ ë³´ì™„ì 

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: XGBoost ì˜ˆì¸¡
- ì‹¤ìŠµ 2: LSTM ì˜ˆì¸¡
- ì‹¤ìŠµ 3: ê°€ì¤‘ì¹˜ ìµœì í™” (Grid Search)
- ì‹¤ìŠµ 4: ì•™ìƒë¸” ì˜ˆì¸¡
- ì‹¤ìŠµ 5: ì„±ëŠ¥ ë¹„êµ í‘œ + ì‹œê°í™”

### í•µì‹¬ ì½”ë“œ: ê°€ì¤‘ì¹˜ ìµœì í™”

```python
# XGBoost, LSTM ì˜ˆì¸¡
p_xgb = xgb_model.predict_proba(X_test_tabular)[:, 1]
p_lstm = lstm_model(X_test_seq).detach().cpu().numpy().flatten()

# ê°€ì¤‘ì¹˜ ìµœì í™”
best_auc = 0
best_weight = 0

for w_xgb in np.arange(0.3, 0.8, 0.1):
    w_lstm = 1 - w_xgb
    p_ensemble = w_xgb * p_xgb + w_lstm * p_lstm
    auc = roc_auc_score(y_test, p_ensemble)

    if auc > best_auc:
        best_auc = auc
        best_weight = w_xgb

print(f"ìµœì  ê°€ì¤‘ì¹˜: XGBoost {best_weight:.1f}, LSTM {1-best_weight:.1f}")
print(f"Ensemble AUC: {best_auc:.4f}")
```

### í•µì‹¬ ì½”ë“œ: ì„±ëŠ¥ ë¹„êµ

```python
# ì„±ëŠ¥ ë¹„êµ í‘œ
results = pd.DataFrame([
    {'Model': 'XGBoost', 'AUC': roc_auc_score(y_test, p_xgb)},
    {'Model': 'LSTM', 'AUC': roc_auc_score(y_test, p_lstm)},
    {'Model': 'Ensemble', 'AUC': best_auc},
])
print(results.to_markdown(index=False))

# ì‹œê°í™”
fig, ax = plt.subplots(figsize=(8, 5))
ax.bar(results['Model'], results['AUC'])
ax.set_ylabel('AUC')
ax.set_title('Model Comparison')
plt.show()
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì•™ìƒë¸”ë¡œ ì–¼ë§ˆë‚˜ ì„±ëŠ¥ì´ ì˜¬ëë‚˜ìš”?"
> "XGBoost ë‹¨ë… AUC 0.92, LSTM ë‹¨ë… 0.89ì˜€ìŠµë‹ˆë‹¤. Weighted Average (0.6:0.4)ë¡œ ì•™ìƒë¸”í•˜ë‹ˆ 0.94ë¡œ í–¥ìƒëìŠµë‹ˆë‹¤."

Q: "ì™œ ë‘ ëª¨ë¸ì´ ìƒí˜¸ ë³´ì™„ì ì¸ê°€ìš”?"
> "XGBoostëŠ” ë‹¨ì¼ ê±°ë˜ì˜ ì •í˜• íŠ¹ì„±ì„ ì¡ê³ , LSTMì€ ê±°ë˜ ì‹œí€€ìŠ¤ì˜ íŒ¨í„´ì„ ì¡ìŠµë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì‚¬ê¸°ë¥¼ íƒì§€í•˜ë¯€ë¡œ ì•™ìƒë¸” íš¨ê³¼ê°€ í½ë‹ˆë‹¤."

---

## 1-6: SHAP ì„¤ëª… (Day 6)

### í•„ìš” íŒ¨í‚¤ì§€
```python
import shap
import torch
import numpy as np
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. XGBoost ì„¤ëª…: TreeExplainer**
- ë¹ ë¥´ê³  ì •í™•
- ì „ì²´ í”¼ì²˜ ì¤‘ìš”ë„
- ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…

**2. LSTM ì„¤ëª…: DeepExplainer**
- ë”¥ëŸ¬ë‹ ëª¨ë¸ìš©
- Background ë°ì´í„° í•„ìš”
- ì‹œí€€ìŠ¤ í”¼ì²˜ ê¸°ì—¬ë„

**3. ì•™ìƒë¸” ì„¤ëª… í†µí•©**
- ê°€ì¤‘ì¹˜ë¡œ SHAP ê°’ í•©ì‚°
- ì •í˜• + ì‹œê³„ì—´ í”¼ì²˜ í†µí•©
- Top K í”¼ì²˜ ì¶”ì¶œ

**4. ìì—°ì–´ ì„¤ëª… ìƒì„±**
- í”¼ì²˜ëª… â†’ ì„¤ëª… ë§¤í•‘
- ë°©í–¥ (ì¦ê°€/ê°ì†Œ) í¬í•¨

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: XGBoost SHAP ê³„ì‚°
- ì‹¤ìŠµ 2: SHAP Summary Plot
- ì‹¤ìŠµ 3: LSTM SHAP ê³„ì‚° (DeepExplainer)
- ì‹¤ìŠµ 4: ì•™ìƒë¸” SHAP í†µí•©
- ì‹¤ìŠµ 5: ìì—°ì–´ ì„¤ëª… ìƒì„±

### í•µì‹¬ ì½”ë“œ: XGBoost SHAP

```python
# TreeExplainer
explainer_xgb = shap.TreeExplainer(xgb_model)
shap_values_xgb = explainer_xgb.shap_values(X_test_tabular)

# Summary Plot
shap.summary_plot(shap_values_xgb, X_test_tabular, max_display=10)
```

### í•µì‹¬ ì½”ë“œ: LSTM SHAP

```python
# DeepExplainer (ë°°ê²½ ë°ì´í„° í•„ìš”)
background = X_train_seq[:100]  # ë°°ê²½ ìƒ˜í”Œ
explainer_lstm = shap.DeepExplainer(lstm_model, torch.FloatTensor(background))
shap_values_lstm = explainer_lstm.shap_values(torch.FloatTensor(X_test_seq[:10]))
```

### í•µì‹¬ ì½”ë“œ: í†µí•© ì„¤ëª…

```python
def get_ensemble_explanation(shap_xgb, shap_lstm, feature_names_xgb, feature_names_lstm,
                              w_xgb=0.6, top_k=5):
    """ì•™ìƒë¸” SHAP ì„¤ëª… ìƒì„±"""
    # XGBoost Top í”¼ì²˜
    xgb_importance = np.abs(shap_xgb).mean(axis=0)
    xgb_top_idx = np.argsort(xgb_importance)[-top_k:][::-1]

    # LSTM ì‹œí€€ìŠ¤ ì˜í–¥ë„ (í‰ê· )
    lstm_impact = np.abs(shap_lstm).mean()

    explanation = {
        'tabular_features': [
            {
                'feature': feature_names_xgb[i],
                'importance': xgb_importance[i] * w_xgb,
                'direction': 'increase' if shap_xgb[i] > 0 else 'decrease'
            }
            for i in xgb_top_idx
        ],
        'sequence_impact': lstm_impact * (1 - w_xgb)
    }
    return explanation

# ìì—°ì–´ ë³€í™˜
FEATURE_DESC = {
    'TransactionAmt': 'ê±°ë˜ ê¸ˆì•¡',
    'hour': 'ê±°ë˜ ì‹œê°„',
    'card1_fraud_rate': 'ì¹´ë“œ ì‚¬ê¸° ì´ë ¥',
}

def to_natural_language(explanation):
    lines = ["[ì‚¬ê¸° íŒë‹¨ ê·¼ê±°]"]
    for f in explanation['tabular_features'][:3]:
        name = FEATURE_DESC.get(f['feature'], f['feature'])
        direction = "ë†’ìŒ" if f['direction'] == 'increase' else "ë‚®ìŒ"
        lines.append(f"- {name}: ì‚¬ê¸° í™•ë¥  {direction}")

    if explanation['sequence_impact'] > 0.1:
        lines.append("- ìµœê·¼ ê±°ë˜ íŒ¨í„´: ë¹„ì •ìƒ ê°ì§€")

    return "\n".join(lines)
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ SHAPì„ ì„ íƒí–ˆë‚˜ìš”?"
> "SHAPì€ ì´ë¡ ì  ê¸°ë°˜(Shapley Value)ì´ íƒ„íƒ„í•˜ê³ , TreeExplainerì™€ DeepExplainerë¡œ XGBoostì™€ LSTM ëª¨ë‘ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

Q: "ì•™ìƒë¸” ëª¨ë¸ì€ ì–´ë–»ê²Œ ì„¤ëª…í•˜ë‚˜ìš”?"
> "ê° ëª¨ë¸ì˜ SHAP ê°’ì„ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ë¡œ í•©ì¹©ë‹ˆë‹¤. ì •í˜• í”¼ì²˜ì™€ ì‹œê³„ì—´ íŒ¨í„´ ëª¨ë‘ í¬í•¨ëœ í†µí•© ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤."

---

## 1-7: FastAPI ë°°í¬ (Day 7)

### í•„ìš” íŒ¨í‚¤ì§€
```python
from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import torch
import uvicorn
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. API ì—”ë“œí¬ì¸íŠ¸**
- GET /health: í—¬ìŠ¤ì²´í¬
- POST /predict: ì‚¬ê¸° ì˜ˆì¸¡ + ì„¤ëª…

**2. Request/Response ìŠ¤í‚¤ë§ˆ**
- Pydantic BaseModel
- íƒ€ì… ê²€ì¦

**3. ëª¨ë¸ ë¡œë”©**
- ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë”©
- XGBoost + LSTM + SHAP Explainer

**4. Docker ì»¨í…Œì´ë„ˆí™”**
- Dockerfile ì‘ì„±
- docker-compose.yml
- ë‹¨ì¼ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰

**5. ì„±ëŠ¥ ìµœì í™”**
- ëª¨ë¸ ìºì‹±
- ì‘ë‹µ ì‹œê°„ < 200ms

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜
- ì‹¤ìŠµ 2: /health ì—”ë“œí¬ì¸íŠ¸
- ì‹¤ìŠµ 3: /predict ì—”ë“œí¬ì¸íŠ¸
- ì‹¤ìŠµ 4: Dockerfile ì‘ì„±
- ì‹¤ìŠµ 5: docker-compose ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

### í•µì‹¬ ì½”ë“œ: FastAPI

```python
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Dict
import joblib
import torch

app = FastAPI(title="FDS API", version="1.0")

# ëª¨ë¸ ë¡œë”© (ì‹œì‘ ì‹œ í•œ ë²ˆ)
xgb_model = joblib.load('models/xgb_model.pkl')
lstm_model = torch.load('models/lstm_model.pt')
lstm_model.eval()

class Transaction(BaseModel):
    transaction_id: int
    amount: float
    hour: int
    card1: int
    # ... ê¸°íƒ€ í”¼ì²˜
    recent_transactions: List[Dict]  # ìµœê·¼ ê±°ë˜ ì‹œí€€ìŠ¤

class PredictionResponse(BaseModel):
    fraud_probability: float
    model_scores: Dict[str, float]
    top_factors: List[Dict]
    is_fraud: bool

@app.get("/health")
def health():
    return {"status": "healthy"}

@app.post("/predict", response_model=PredictionResponse)
def predict(transaction: Transaction):
    # 1. Feature Engineering
    X_tabular = extract_tabular_features(transaction)
    X_sequence = extract_sequence_features(transaction.recent_transactions)

    # 2. ëª¨ë¸ ì˜ˆì¸¡
    p_xgb = xgb_model.predict_proba(X_tabular)[0, 1]
    with torch.no_grad():
        p_lstm = lstm_model(torch.FloatTensor(X_sequence).unsqueeze(0)).item()

    # 3. ì•™ìƒë¸”
    p_ensemble = 0.6 * p_xgb + 0.4 * p_lstm

    # 4. SHAP ì„¤ëª…
    explanation = generate_explanation(X_tabular, X_sequence)

    return PredictionResponse(
        fraud_probability=p_ensemble,
        model_scores={"xgboost": p_xgb, "lstm": p_lstm, "ensemble": p_ensemble},
        top_factors=explanation,
        is_fraud=p_ensemble > 0.35
    )
```

### í•µì‹¬ ì½”ë“œ: Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY src/ ./src/
COPY models/ ./models/

CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### í•µì‹¬ ì½”ë“œ: docker-compose.yml

```yaml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "API ì‘ë‹µ ì‹œê°„ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?"
> "XGBoost ~10ms, LSTM ~50ms, SHAP ~100msë¡œ ì´ ì•½ 160msì…ë‹ˆë‹¤. ëª©í‘œì˜€ë˜ 200ms ì´í•˜ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤."

Q: "ëª¨ë¸ ì—…ë°ì´íŠ¸ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
> "í˜„ì¬ëŠ” Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ ë°©ì‹ì…ë‹ˆë‹¤. Phase 2ì—ì„œ MLflowë¡œ ëª¨ë¸ ë²„ì „ ê´€ë¦¬ë¥¼ ì¶”ê°€í•˜ë©´ ë¬´ì¤‘ë‹¨ ë°°í¬ê°€ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤."

---

## ì „ì²´ ìš”ì•½

| ë…¸íŠ¸ë¶ | ì‹œê°„ | í•µì‹¬ ì‚°ì¶œë¬¼ |
|--------|------|------------|
| 1-1 | 3h | train.csv, test.csv |
| 1-2 | 4h | feature_engineering.py, X_tabular, X_sequence |
| 1-3 | 4h | ëª¨ë¸ ë¹„êµ í‘œ, xgb_model.pkl |
| 1-4 | 4h | lstm_model.pt |
| 1-5 | 3h | ensemble.py, ì„±ëŠ¥ ë¹„êµ í‘œ |
| 1-6 | 3h | shap_explainer.py, ì„¤ëª… ì‹œê°í™” |
| 1-7 | 4h | FastAPI, Docker, í†µí•© í…ŒìŠ¤íŠ¸ |

**ì´ ì•½ 25ì‹œê°„ (7ì¼)**

---

## í•µì‹¬ ì‹¤í—˜ ê²°ê³¼ (ë©´ì ‘ìš©)

### 1. ëª¨ë¸ ë¹„êµ (1-3)

| Model | AUC | Time(s) | SHAP |
|-------|-----|---------|------|
| XGBoost | 0.92 | 45 | âœ… ìµœìƒ |
| LightGBM | 0.91 | 32 | âœ… ì¢‹ìŒ |
| CatBoost | 0.91 | 98 | âš ï¸ ì œí•œ |

### 2. ì‹œí€€ìŠ¤ ê¸¸ì´ ë¹„êµ (1-4)

| Seq Length | AUC | Train Time |
|------------|-----|------------|
| 5 | 0.86 | 2min |
| 10 | 0.89 | 4min |
| 15 | 0.89 | 6min |
| 20 | 0.90 | 9min |

### 3. ì•™ìƒë¸” ì„±ëŠ¥ (1-5)

| Model | AUC | Recall@0.35 |
|-------|-----|-------------|
| XGBoost | 0.92 | 0.83 |
| LSTM | 0.89 | 0.80 |
| **Ensemble** | **0.94** | **0.87** |

**ì´ í‘œë“¤ì´ ë©´ì ‘ì—ì„œ "ì™œ ì´ê±¸ ì„ íƒí–ˆë‚˜ìš”?"ì— ëŒ€í•œ ê·¼ê±°!**

---

## ë‹¤ìŒ ë‹¨ê³„: Phase 2

> ìƒì„¸: [docs/roadmap.md](./roadmap.md)

Phase 1 ì™„ë£Œ í›„ Phase 2ì—ì„œ ì¶”ê°€:
- **MLflow**: ì‹¤í—˜ ì¶”ì , ëª¨ë¸ ë²„ì „ ê´€ë¦¬
- **Evidently**: ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§
- **GitHub Actions**: CI/CD
- **ë¹„ìš© ê¸°ë°˜ ìµœì í™”**: ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ê³„ì‚°
