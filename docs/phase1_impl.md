# Phase 1: FDS + Ensemble + XAI - êµ¬í˜„ ìƒì„¸ (AIìš©)

> ë…¸íŠ¸ë¶/ì½”ë“œ ìƒì„±ì„ ìœ„í•œ ìƒì„¸ ìŠ¤í™

---

## âš ï¸ êµ¬í˜„ ë°©ì‹ (í•„ë…)

**ê° Day ë…¸íŠ¸ë¶ì€ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬í˜„í•´ì•¼ í•¨:**

### ë…¸íŠ¸ë¶ êµ¬ì¡°

```
[ë§ˆí¬ë‹¤ìš´] ì œëª© + í•™ìŠµ ëª©í‘œ
[ì½”ë“œ] íŒ¨í‚¤ì§€ ì„í¬íŠ¸
[ë§ˆí¬ë‹¤ìš´] ğŸ“š ê°œë… ì„¤ëª…
[ì½”ë“œ] ì˜ˆì œ ì½”ë“œ (ì™„ì„±ë³¸)
[ë§ˆí¬ë‹¤ìš´] ğŸ’» ì‹¤ìŠµ N ì„¤ëª…
[ì½”ë“œ] ì‹¤ìŠµ TODO (ë¹ˆì¹¸)
[ì½”ë“œ] âœ… ì‹¤ìŠµ ì •ë‹µ (ì±„ìš´ ë²„ì „)
[ì½”ë“œ] ì²´í¬í¬ì¸íŠ¸ (assert)
... (ë°˜ë³µ)
[ë§ˆí¬ë‹¤ìš´] ìµœì¢… ìš”ì•½ + ë©´ì ‘ í¬ì¸íŠ¸
```

### í¬í•¨ ìš”ì†Œ

| ìš”ì†Œ | ì„¤ëª… |
|------|------|
| ğŸ“š ê°œë… ì„¤ëª… | ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ê°œë…/ì´ë¡  ì„¤ëª… |
| ì˜ˆì œ ì½”ë“œ | ì™„ì„±ëœ ì˜ˆì œ (í•™ìŠµìš©) |
| ğŸ’» ì‹¤ìŠµ TODO | ë¹ˆì¹¸/TODO í¬í•¨ëœ ì‹¤ìŠµ ì½”ë“œ |
| âœ… ì‹¤ìŠµ ì •ë‹µ | TODO ì±„ìš´ ì •ë‹µ ì½”ë“œ |
| ì²´í¬í¬ì¸íŠ¸ | assertë¡œ ê²€ì¦ |
| ë©´ì ‘ í¬ì¸íŠ¸ | í•´ë‹¹ Day ê´€ë ¨ ë©´ì ‘ Q&A |

### src/ ëª¨ë“ˆí™”

1-2ë¶€í„° ê²€ì¦ëœ ì½”ë“œë¥¼ src/ë¡œ ëª¨ë“ˆí™”:

```
ë…¸íŠ¸ë¶ì—ì„œ ì½”ë“œ ì‘ì„± + ì‹¤í—˜
    â†“
ê²€ì¦ë˜ë©´ src/ë¡œ ëª¨ë“ˆí™”
    â†“
ë…¸íŠ¸ë¶ì—ì„œ ëª¨ë“ˆ importí•´ì„œ ì‚¬ìš©
```

---

## íŒŒì¼ êµ¬ì¡°

```
fds-system/
â”œâ”€â”€ notebooks/                   # ì‹¤í—˜ìš© ë…¸íŠ¸ë¶
â”‚   â””â”€â”€ phase1/
â”‚       â”œâ”€â”€ 1-1_data_eda.ipynb
â”‚       â”œâ”€â”€ 1-2_feature_engineering.ipynb
â”‚       â”œâ”€â”€ 1-3_xgboost.ipynb
â”‚       â”œâ”€â”€ 1-4_lstm.ipynb
â”‚       â”œâ”€â”€ 1-5_ensemble.ipynb
â”‚       â”œâ”€â”€ 1-6_shap.ipynb
â”‚       â”œâ”€â”€ 1-7_fastapi.ipynb
â”‚       â””â”€â”€ 1-8_fusion.ipynb     # â­ 2024 ë…¼ë¬¸ ê¸°ë°˜ ìœµí•© ì‹¤í—˜
â”‚
â”œâ”€â”€ src/                         # í”„ë¡œë•ì…˜ ì½”ë“œ
â”‚   â”œâ”€â”€ models/                  # â­ PyTorch í´ë˜ìŠ¤ ì •ì˜ (í•„ìˆ˜!)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lstm.py              # FraudLSTM í´ë˜ìŠ¤
â”‚   â”‚   â”œâ”€â”€ cnn_lstm.py          # CNN-LSTM í´ë˜ìŠ¤ (ì„ íƒ)
â”‚   â”‚   â””â”€â”€ fusion.py            # ìœµí•© ëª¨ë¸ í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ preprocessing.py     # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ train.py             # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ explainer/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ shap_explainer.py
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py
â”‚       â””â”€â”€ schemas.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # IEEE-CIS ì›ë³¸
â”‚   â””â”€â”€ processed/               # ì „ì²˜ë¦¬ ë°ì´í„°
â”‚
â”œâ”€â”€ models/                      # ì €ì¥ëœ ê°€ì¤‘ì¹˜
â”‚   â”œâ”€â”€ xgb_model.pkl            # XGBoost (joblib)
â”‚   â””â”€â”€ lstm_model.pt            # LSTM (torch.save)
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml              # í•˜ì´í¼íŒŒë¼ë¯¸í„°, í”¼ì²˜ ëª©ë¡
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

### âš ï¸ PyTorch ëª¨ë¸ ì €ì¥/ë¡œë“œ ë°©ì‹ (í˜„ì—… í•„ìˆ˜)

```python
# âŒ ë…¸íŠ¸ë¶ì—ì„œë§Œ í´ë˜ìŠ¤ ì •ì˜ â†’ APIì—ì„œ ë¡œë“œ ë¶ˆê°€
# torch.load()ëŠ” í´ë˜ìŠ¤ ì •ì˜ê°€ í•„ìš”í•¨

# âœ… í˜„ì—… ë°©ì‹: src/models/ì— í´ë˜ìŠ¤ ì •ì˜
# 1. src/models/lstm.pyì— FraudLSTM í´ë˜ìŠ¤ ì •ì˜
# 2. ë…¸íŠ¸ë¶ì—ì„œ from src.models.lstm import FraudLSTM
# 3. APIì—ì„œë„ ë™ì¼í•˜ê²Œ import

# ì €ì¥
torch.save(model.state_dict(), 'models/lstm_model.pt')

# ë¡œë“œ (API)
from src.models.lstm import FraudLSTM
model = FraudLSTM(input_size=35, hidden_size=64)
model.load_state_dict(torch.load('models/lstm_model.pt'))
```

---

## 1-1: ë°ì´í„° + EDA (Day 1)

### í•„ìš” íŒ¨í‚¤ì§€
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. IEEE-CIS ë°ì´í„°ì…‹**
- Kaggleì—ì„œ ë‹¤ìš´ë¡œë“œ
- Transaction í…Œì´ë¸”: ê±°ë˜ ì •ë³´ (ê¸ˆì•¡, ì‹œê°„, ì¹´ë“œ)
- Identity í…Œì´ë¸”: ê¸°ê¸°/ë¸Œë¼ìš°ì € ì •ë³´
- TransactionIDë¡œ LEFT JOIN

**2. ê¸°ë³¸ EDA**
- shape, dtypes, info()
- head(), tail()
- describe()

**3. ë¶ˆê· í˜• ë°ì´í„°**
- íƒ€ê²Ÿ(isFraud) ë¶„í¬ í™•ì¸
- ì‚¬ê¸° ë¹„ìœ¨ ~3.5%
- Accuracyê°€ ë¬´ì˜ë¯¸í•œ ì´ìœ 
- í‰ê°€ ì§€í‘œ: AUC-ROC, PR-AUC, F1

**4. ê²°ì¸¡ì¹˜ ë¶„ì„**
- ì»¬ëŸ¼ë³„ ê²°ì¸¡ ë¹„ìœ¨
- ê²°ì¸¡ 50% ì´ìƒ ì»¬ëŸ¼ ì²˜ë¦¬ ì „ëµ
- Identity í…Œì´ë¸” ê²°ì¸¡ (ë³‘í•©ìœ¼ë¡œ ì¸í•œ)

**5. í”¼ì²˜ íƒìƒ‰**
- ìˆ˜ì¹˜í˜•: TransactionAmt, card1~5
- ë²”ì£¼í˜•: ProductCD, card4, card6
- ì‹œê°„: TransactionDT

**6. íƒ€ê²Ÿë³„ ë¶„ì„**
- ì •ìƒ vs ì‚¬ê¸° ê¸ˆì•¡ ë¶„í¬
- ì‹œê°„ëŒ€ë³„ ì‚¬ê¸° ë¹„ìœ¨
- ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ê¸° ë¹„ìœ¨

**7. ì‹œê°„ ê¸°ë°˜ ë¶„í• **
- ì™œ ëœë¤ ë¶„í• ì´ ì•ˆ ë˜ëŠ”ì§€ (Data Leakage)
- TransactionDT ê¸°ì¤€ ì •ë ¬
- 80/20 ë¶„í• 

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: ë°ì´í„° ë¡œë“œ ë° ë³‘í•© (LEFT JOIN)
- ì‹¤ìŠµ 2: íƒ€ê²Ÿ ë¶ˆê· í˜• ì‹œê°í™”
- ì‹¤ìŠµ 3: ê²°ì¸¡ì¹˜ ë¶„ì„ ë° ì²˜ë¦¬ ì „ëµ
- ì‹¤ìŠµ 4: íƒ€ê²Ÿë³„ ê¸ˆì•¡ ë¶„í¬ ë¹„êµ
- ì‹¤ìŠµ 5: ì‹œê°„ ê¸°ë°˜ train/test ë¶„í• 

### í•µì‹¬ ì½”ë“œ

```python
# ë°ì´í„° ë³‘í•©
df = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')

# ì‹œê°„ ê¸°ë°˜ ë¶„í• 
df_sorted = df.sort_values('TransactionDT')
split_idx = int(len(df_sorted) * 0.8)
train_df = df_sorted.iloc[:split_idx]
test_df = df_sorted.iloc[split_idx:]

# ê²€ì¦: ì‹œê°„ ìˆœì„œ í™•ì¸
assert train_df['TransactionDT'].max() <= test_df['TransactionDT'].min()
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ ëœë¤ ë¶„í• ì´ ì•„ë‹Œ ì‹œê°„ ê¸°ë°˜ ë¶„í• ì„ í•˜ë‚˜ìš”?"
> "ì‹¤ì œ FDSëŠ” ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµí•´ì„œ ë¯¸ë˜ ê±°ë˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ëœë¤ ë¶„í• ì€ ë¯¸ë˜ ì •ë³´ê°€ í•™ìŠµì— í¬í•¨ë˜ì–´ Data Leakageê°€ ë°œìƒí•©ë‹ˆë‹¤. ì‹œê°„ ê¸°ë°˜ ë¶„í• ì´ ì‹¤ì œ ìš´ì˜ í™˜ê²½ì„ ë°˜ì˜í•©ë‹ˆë‹¤."

---

## 1-2: Feature Engineering (Day 2)

### í•„ìš” íŒ¨í‚¤ì§€
```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì •í˜• í”¼ì²˜ (XGBoostìš©)**
- ì‹œê°„ í”¼ì²˜: hour, dayofweek, is_weekend, is_night
- ê¸ˆì•¡ í”¼ì²˜: amt_log, amt_bin, amt_decimal
- ì§‘ê³„ í”¼ì²˜: card1ë³„ ê±°ë˜ íšŸìˆ˜, í‰ê·  ê¸ˆì•¡
- ë²”ì£¼í˜• ì¸ì½”ë”©: LabelEncoder

**2. ì‹œê³„ì—´ í”¼ì²˜ (LSTMìš©)** â­
- ì‚¬ìš©ìë³„ ìµœê·¼ Nê°œ ê±°ë˜ ì‹œí€€ìŠ¤ ì¶”ì¶œ
- ì‹œí€€ìŠ¤ í”¼ì²˜: [ê¸ˆì•¡, ì‹œê°„ì°¨, ì¹´í…Œê³ ë¦¬, ...]
- Padding: ê±°ë˜ ìˆ˜ê°€ N ë¯¸ë§Œì´ë©´ 0ìœ¼ë¡œ ì±„ì›€
- Scaling: MinMaxScalerë¡œ 0~1 ì •ê·œí™”

**3. ì‹œí€€ìŠ¤ ìƒì„± ë¡œì§**
```
ì‚¬ìš©ì Aì˜ ê±°ë˜: [t1, t2, t3, t4, t5]
ì‹œí€€ìŠ¤ ê¸¸ì´ N=3ì¼ ë•Œ:
- t3 ì˜ˆì¸¡ìš©: [t1, t2] â†’ ê¸¸ì´ ë¶€ì¡± â†’ [0, t1, t2]
- t4 ì˜ˆì¸¡ìš©: [t1, t2, t3]
- t5 ì˜ˆì¸¡ìš©: [t2, t3, t4]
```

**4. í”¼ì²˜ ì €ì¥**
- X_tabular: ì •í˜• í”¼ì²˜ (DataFrame)
- X_sequence: ì‹œê³„ì—´ í”¼ì²˜ (3D array: samples x seq_len x features)

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: ì‹œê°„ í”¼ì²˜ ìƒì„±
- ì‹¤ìŠµ 2: ê¸ˆì•¡ í”¼ì²˜ ìƒì„±
- ì‹¤ìŠµ 3: ì§‘ê³„ í”¼ì²˜ ìƒì„±
- ì‹¤ìŠµ 4: ë²”ì£¼í˜• ì¸ì½”ë”©
- ì‹¤ìŠµ 5: ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± â­

### í•µì‹¬ ì½”ë“œ: ì‹œí€€ìŠ¤ ìƒì„±

```python
def create_sequences(df, user_col, features, seq_len=10):
    """ì‚¬ìš©ìë³„ ì‹œí€€ìŠ¤ ìƒì„±"""
    sequences = []
    labels = []

    for user_id, group in df.groupby(user_col):
        group = group.sort_values('TransactionDT')

        for i in range(len(group)):
            # í˜„ì¬ ê±°ë˜ ì´ì „ seq_lenê°œ ê±°ë˜
            start_idx = max(0, i - seq_len)
            seq = group.iloc[start_idx:i][features].values

            # íŒ¨ë”© (ê¸¸ì´ ë¶€ì¡± ì‹œ)
            if len(seq) < seq_len:
                pad = np.zeros((seq_len - len(seq), len(features)))
                seq = np.vstack([pad, seq])

            sequences.append(seq)
            labels.append(group.iloc[i]['isFraud'])

    return np.array(sequences), np.array(labels)

# ì‹œí€€ìŠ¤ í”¼ì²˜
seq_features = ['TransactionAmt_scaled', 'hour_scaled', 'time_diff_scaled']
X_seq, y_seq = create_sequences(train_df, 'card1', seq_features, seq_len=10)
print(f"Sequence shape: {X_seq.shape}")  # (samples, 10, 3)
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì‹œí€€ìŠ¤ ê¸¸ì´ 10ì€ ì–´ë–»ê²Œ ì •í–ˆë‚˜ìš”?"
> "5, 10, 15, 20ìœ¼ë¡œ ì‹¤í—˜í–ˆìŠµë‹ˆë‹¤. 10ì´ AUCì™€ í•™ìŠµ ì‹œê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ì—ì„œ ìµœì ì´ì—ˆìŠµë‹ˆë‹¤. 5ëŠ” íŒ¨í„´ì„ ëª» ì¡ê³ , 20ì€ í•™ìŠµ ì‹œê°„ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒì´ ë¯¸ë¯¸í–ˆìŠµë‹ˆë‹¤."

---

## 1-3: XGBoost ëª¨ë¸ (Day 3) â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score, precision_recall_curve
import optuna
import joblib
import time
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ëª¨ë¸ ë¹„êµ ì‹¤í—˜** â­
- XGBoost, LightGBM, CatBoost
- ë™ì¼ ì¡°ê±´ (ë™ì¼ í”¼ì²˜, ë™ì¼ ë¶„í• )
- AUC, í•™ìŠµ ì‹œê°„ ì¸¡ì •
- ê²°ê³¼ í‘œë¡œ ì •ë¦¬

**2. XGBoost ì„ íƒ ì´ìœ **
- AUC ìµœê³ 
- SHAP TreeExplainer í˜¸í™˜ì„± ìµœìƒ
- GPU í•™ìŠµ ì§€ì›

**3. Threshold ìµœì í™”** â­
- FN:FP ë¹„ìš© ë¹„ìœ¨ ì •ì˜ (10:1)
- ë¹„ìš© í•¨ìˆ˜ë¡œ ìµœì  threshold ì°¾ê¸°
- Precision-Recall Curve

**4. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
- íƒìƒ‰ ê³µê°„ ì •ì˜
- objective í•¨ìˆ˜
- n_trials ì„¤ì •

**5. n_estimatorsì™€ Early Stopping** â­
- í˜„ì—… íŠ¸ë¦¬ ê°œìˆ˜: 100~500ê°œ (ìƒí™©ë³„ ìƒì´)
- Early Stoppingìœ¼ë¡œ ìµœì  ê°œìˆ˜ ìë™ íƒìƒ‰
- GPU vs CPU ì„ íƒ ê¸°ì¤€

| ìƒí™© | íŠ¸ë¦¬ ìˆ˜ | ì´ìœ  |
|------|---------|------|
| ë¹ ë¥¸ ì‹¤í—˜ | 100 | ë² ì´ìŠ¤ë¼ì¸ |
| í”„ë¡œë•ì…˜ FDS | 200~500 | ì„±ëŠ¥ vs ì†ë„ ê· í˜• |
| Kaggle ëŒ€íšŒ | 1000+ | ìµœê³  ì„±ëŠ¥ |
| ì‹¤ì‹œê°„ API | 50~200 | ì‘ë‹µì†ë„ ì¤‘ìš” |

**6. ëª¨ë¸ ì €ì¥**
- joblib.dump
- ë©”íƒ€ë°ì´í„° í•¨ê»˜ ì €ì¥

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: 3ê°œ ëª¨ë¸ ë¹„êµ ì‹¤í—˜ â†’ ê²°ê³¼ í‘œ
- ì‹¤ìŠµ 2: XGBoost í•™ìŠµ
- ì‹¤ìŠµ 3: Threshold ë¹„ìš© ë¶„ì„ â†’ ê·¸ë˜í”„
- ì‹¤ìŠµ 4: Optuna íŠœë‹
- ì‹¤ìŠµ 5: ëª¨ë¸ ì €ì¥

### í•µì‹¬ ì½”ë“œ: ëª¨ë¸ ë¹„êµ

```python
models = {
    'XGBoost': XGBClassifier(n_estimators=100, tree_method='hist', device='cuda'),
    'LightGBM': LGBMClassifier(n_estimators=100, device='gpu'),
    'CatBoost': CatBoostClassifier(n_estimators=100, task_type='GPU', verbose=0),
}

results = []
for name, model in models.items():
    start = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start

    y_prob = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_prob)

    results.append({'Model': name, 'AUC': auc, 'Time(s)': train_time})

results_df = pd.DataFrame(results)
print(results_df.to_markdown(index=False))
```

### í•µì‹¬ ì½”ë“œ: Threshold ìµœì í™”

```python
def calculate_cost(threshold, y_true, y_prob, fn_cost=10, fp_cost=1):
    """ë¹„ìš© í•¨ìˆ˜: FNì´ FPë³´ë‹¤ 10ë°° ë¹„ìŒˆ"""
    y_pred = (y_prob >= threshold).astype(int)
    fn = ((y_true == 1) & (y_pred == 0)).sum()
    fp = ((y_true == 0) & (y_pred == 1)).sum()
    return fn * fn_cost + fp * fp_cost

# ìµœì  threshold ì°¾ê¸°
thresholds = np.arange(0.1, 0.9, 0.05)
costs = [calculate_cost(t, y_test, y_prob) for t in thresholds]
optimal_threshold = thresholds[np.argmin(costs)]
print(f"ìµœì  Threshold: {optimal_threshold:.2f}")
```

### í•µì‹¬ ì½”ë“œ: Early Stopping

```python
# Early Stoppingìœ¼ë¡œ ìµœì  íŠ¸ë¦¬ ê°œìˆ˜ ì°¾ê¸°
model = XGBClassifier(
    n_estimators=1000,           # ì¼ë‹¨ ë§ì´
    early_stopping_rounds=50,    # 50ë²ˆ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
    eval_metric='auc',
    device='cuda',
    random_state=42
)

model.fit(
    X_train, y_train,
    eval_set=[(X_valid, y_valid)],
    verbose=100  # 100ë²ˆë§ˆë‹¤ ì¶œë ¥
)

print(f"ì‹¤ì œ ì‚¬ìš©ëœ íŠ¸ë¦¬ ìˆ˜: {model.best_iteration}")  # ì˜ˆ: 287
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ XGBoostë¥¼ ì„ íƒí–ˆë‚˜ìš”?"
> "3ê°œ ëª¨ë¸ì„ ë™ì¼ ì¡°ê±´ì—ì„œ ë¹„êµí–ˆìŠµë‹ˆë‹¤. XGBoostê°€ AUC 0.92ë¡œ ê°€ì¥ ë†’ì•˜ê³ , SHAP TreeExplainer í˜¸í™˜ì„±ë„ ìµœìƒì´ì—ˆìŠµë‹ˆë‹¤."

Q: "ThresholdëŠ” ì–´ë–»ê²Œ ì •í–ˆë‚˜ìš”?"
> "FDSì—ì„œ FN(ë†“ì¹œ ì‚¬ê¸°)ì´ FP(ì˜¤íƒ)ë³´ë‹¤ ë¹„ìš©ì´ í½ë‹ˆë‹¤. FN:FP = 10:1ë¡œ ë¹„ìš© í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ìµœì†Œí™”í•˜ëŠ” Threshold 0.35ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."

Q: "n_estimatorsëŠ” ì–´ë–»ê²Œ ì •í–ˆë‚˜ìš”?"
> "Early Stoppingì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. n_estimators=1000ìœ¼ë¡œ ì„¤ì •í•˜ê³ , validation AUCê°€ 50 epoch ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” ì•½ 300ê°œ íŠ¸ë¦¬ì—ì„œ ìˆ˜ë ´í–ˆìŠµë‹ˆë‹¤."

Q: "GPUë¥¼ ì•ˆ ì“°ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
> "28ë§Œê±´ ë°ì´í„°ì—ì„œëŠ” CPUë„ ì¶©ë¶„íˆ ë¹ ë¦…ë‹ˆë‹¤ (0.6ì´ˆ). GPU ì˜¤ë²„í—¤ë“œ ë•Œë¬¸ì— ì˜¤íˆë ¤ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆ˜ë°±ë§Œê±´ ì´ìƒì—ì„œ GPUê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤."

---

## 1-4: LSTM ëª¨ë¸ (Day 4) â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì™œ LSTMì´ í•„ìš”í•œê°€?**
- XGBoostëŠ” ë‹¨ì¼ ê±°ë˜ë§Œ ë´„
- ì‚¬ê¸° íŒ¨í„´: "ì†Œì•¡ â†’ ì†Œì•¡ â†’ ê³ ì•¡"
- ì‹œí€€ìŠ¤ íŒ¨í„´ì€ LSTMì´ í•™ìŠµ

**2. LSTM í”¼ì²˜ ì„ íƒ (í˜„ì—… ë°©ì‹)** â­

```python
# 1. Vì»¬ëŸ¼ ìƒìœ„ 20ê°œ (PCA ê¸°ë°˜, ë…¼ë¬¸ í‘œì¤€)
v_features = [f'V{i}' for i in range(1, 21)]  # V1~V20

# 2. XGBoost importance ìƒìœ„ 10ê°œ (V ì œì™¸)
# â†’ 1-3ì—ì„œ ì €ì¥í•œ feature_importance í™œìš©
xgb_importance = pd.read_csv('data/processed/xgb_importance.csv')
xgb_top = xgb_importance[~xgb_importance['feature'].str.startswith('V')].head(10)['feature'].tolist()

# 3. ì‹œê³„ì—´ í”¼ì²˜ (ì§ì ‘ ìƒì„±)
time_features = [
    'amt_log',              # ê¸ˆì•¡ ë¡œê·¸
    'hour',                 # ì‹œê°„
    'dayofweek',            # ìš”ì¼
    'time_since_last_tx',   # ì´ì „ ê±°ë˜ í›„ ê²½ê³¼ ì‹œê°„
    'rolling_avg_amt_5',    # ìµœê·¼ 5ê°œ í‰ê·  ê¸ˆì•¡
    'tx_count_24h',         # 24ì‹œê°„ ë‚´ ê±°ë˜ íšŸìˆ˜
]

# ì „ì²´ ì‹œí€€ìŠ¤ í”¼ì²˜ (~35ê°œ)
SEQ_FEATURES = v_features + xgb_top + time_features
print(f"ì´ í”¼ì²˜ ìˆ˜: {len(SEQ_FEATURES)}")  # ~35ê°œ
```

**3. LSTM êµ¬ì¡°**
- Input: (batch, seq_len, 35)  # 35ê°œ í”¼ì²˜
- Hidden: 64~128
- Output: 1 (sigmoid)
- pos_weight: ~27 (í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬)

**4. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬**

```python
# ì‚¬ê¸° ë¹„ìœ¨ 3.5% â†’ pos_weight ê³„ì‚°
n_pos = (y_train == 1).sum()
n_neg = (y_train == 0).sum()
pos_weight = n_neg / n_pos  # ~27

criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))
```

**5. PyTorch êµ¬í˜„**
- Dataset í´ë˜ìŠ¤
- DataLoader
- BCEWithLogitsLoss + Adam
- Early Stopping

**6. í•™ìŠµ ë£¨í”„**
- Train/Valid ë¶„ë¦¬
- Epochë³„ loss/AUC ì¶”ì 
- Best model ì €ì¥

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: í”¼ì²˜ ì„ íƒ (V1~V20 + XGBoost importance + ì‹œê³„ì—´)
- ì‹¤ìŠµ 2: Dataset í´ë˜ìŠ¤ êµ¬í˜„
- ì‹¤ìŠµ 3: LSTM ëª¨ë¸ ì •ì˜ (src/models/lstm.py)
- ì‹¤ìŠµ 4: í•™ìŠµ ë£¨í”„ + pos_weight
- ì‹¤ìŠµ 5: Optuna íŠœë‹
- ì‹¤ìŠµ 6: ëª¨ë¸ í‰ê°€ ë° ì €ì¥

### í•µì‹¬ ì½”ë“œ: LSTM ëª¨ë¸

```python
class FraudLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x: (batch, seq_len, features)
        _, (h_n, _) = self.lstm(x)
        # h_n: (num_layers, batch, hidden)
        out = self.fc(h_n[-1])
        return self.sigmoid(out)

# ëª¨ë¸ ì´ˆê¸°í™”
model = FraudLSTM(input_size=3, hidden_size=64)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

### í•µì‹¬ ì½”ë“œ: Dataset

```python
class FraudSequenceDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = torch.FloatTensor(sequences)
        self.labels = torch.FloatTensor(labels).unsqueeze(1)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]

# DataLoader
train_dataset = FraudSequenceDataset(X_seq_train, y_seq_train)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
```

### í•µì‹¬ ì½”ë“œ: í•™ìŠµ ë£¨í”„

```python
def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    for X, y in loader:
        X, y = X.to(device), y.to(device)

        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(loader)

# Early Stopping
best_auc = 0
patience = 5
counter = 0

for epoch in range(100):
    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
    val_auc = evaluate(model, val_loader, device)

    if val_auc > best_auc:
        best_auc = val_auc
        torch.save(model.state_dict(), 'models/lstm_best.pt')
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print(f"Early stopping at epoch {epoch}")
            break
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ LSTMì„ ì„ íƒí–ˆë‚˜ìš”?"
> "ê±°ë˜ ì‹œí€€ìŠ¤ì˜ ì‹œê°„ì  íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤. Transformerë„ ê³ ë ¤í–ˆì§€ë§Œ, ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 10~20ìœ¼ë¡œ ì§§ì•„ì„œ LSTMì´ ì¶©ë¶„í–ˆê³  í•™ìŠµë„ ë” ë¹ ë¦…ë‹ˆë‹¤."

Q: "ì™œ Transformerê°€ ì•„ë‹Œê°€ìš”?"
> "ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì§§ìŠµë‹ˆë‹¤. TransformerëŠ” ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ê°•ì ì´ ìˆì§€ë§Œ, 10~20ê°œ ì‹œí€€ìŠ¤ì—ì„œëŠ” LSTMê³¼ ì„±ëŠ¥ ì°¨ì´ê°€ ê±°ì˜ ì—†ê³  êµ¬í˜„ë„ ê°„ë‹¨í•©ë‹ˆë‹¤."

---

## 1-5: Ensemble + í‰ê°€ (Day 5) â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
import numpy as np
from sklearn.metrics import roc_auc_score, classification_report
import matplotlib.pyplot as plt
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì•™ìƒë¸” ì „ëµ (ë‹¨ìˆœ ê²°í•©)**
- Simple Average: (p_xgb + p_lstm) / 2
- Weighted Average: w1*p_xgb + w2*p_lstm âœ…
- âš ï¸ ëª©í‘œ AUC 0.90 ë¯¸ë‹¬ ì‹œ â†’ 1-8 ìœµí•©ìœ¼ë¡œ ì´ë™

**2. ê°€ì¤‘ì¹˜ ìµœì í™”**
- Grid Searchë¡œ ìµœì  ê°€ì¤‘ì¹˜ íƒìƒ‰
- Validation set ê¸°ì¤€
- ê²°ê³¼: XGBoost 0.6, LSTM 0.4

**3. ì„±ëŠ¥ ë¹„êµ**
- ë‹¨ë… ëª¨ë¸ vs ì•™ìƒë¸”
- AUC, Recall, Precision
- ê²°ê³¼ í‘œ + ê·¸ë˜í”„

**4. ì„±ëŠ¥ ë¯¸ë‹¬ ì‹œ ë‹¤ìŒ ìŠ¤í…**
```
ì•™ìƒë¸” AUC 0.89 (ëª©í‘œ 0.90 ë¯¸ë‹¬)
    â†“
ë…¼ë¬¸ ì¡°ì‚¬ â†’ ìœµí•© ë°©ì‹ ë°œê²¬
    â†“
1-8 ìœµí•© ì‹¤í—˜ìœ¼ë¡œ ì´ë™
```

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: XGBoost ì˜ˆì¸¡
- ì‹¤ìŠµ 2: LSTM ì˜ˆì¸¡
- ì‹¤ìŠµ 3: ê°€ì¤‘ì¹˜ ìµœì í™” (Grid Search)
- ì‹¤ìŠµ 4: ì•™ìƒë¸” ì˜ˆì¸¡
- ì‹¤ìŠµ 5: ì„±ëŠ¥ ë¹„êµ í‘œ + ì‹œê°í™”

### í•µì‹¬ ì½”ë“œ: ê°€ì¤‘ì¹˜ ìµœì í™”

```python
# XGBoost, LSTM ì˜ˆì¸¡
p_xgb = xgb_model.predict_proba(X_test_tabular)[:, 1]
p_lstm = lstm_model(X_test_seq).detach().cpu().numpy().flatten()

# ê°€ì¤‘ì¹˜ ìµœì í™”
best_auc = 0
best_weight = 0

for w_xgb in np.arange(0.3, 0.8, 0.1):
    w_lstm = 1 - w_xgb
    p_ensemble = w_xgb * p_xgb + w_lstm * p_lstm
    auc = roc_auc_score(y_test, p_ensemble)

    if auc > best_auc:
        best_auc = auc
        best_weight = w_xgb

print(f"ìµœì  ê°€ì¤‘ì¹˜: XGBoost {best_weight:.1f}, LSTM {1-best_weight:.1f}")
print(f"Ensemble AUC: {best_auc:.4f}")
```

### í•µì‹¬ ì½”ë“œ: ì„±ëŠ¥ ë¹„êµ

```python
# ì„±ëŠ¥ ë¹„êµ í‘œ
results = pd.DataFrame([
    {'Model': 'XGBoost', 'AUC': roc_auc_score(y_test, p_xgb)},
    {'Model': 'LSTM', 'AUC': roc_auc_score(y_test, p_lstm)},
    {'Model': 'Ensemble', 'AUC': best_auc},
])
print(results.to_markdown(index=False))

# ì‹œê°í™”
fig, ax = plt.subplots(figsize=(8, 5))
ax.bar(results['Model'], results['AUC'])
ax.set_ylabel('AUC')
ax.set_title('Model Comparison')
plt.show()
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì•™ìƒë¸”ë¡œ ì–¼ë§ˆë‚˜ ì„±ëŠ¥ì´ ì˜¬ëë‚˜ìš”?"
> "XGBoost ë‹¨ë… AUC 0.92, LSTM ë‹¨ë… 0.89ì˜€ìŠµë‹ˆë‹¤. Weighted Average (0.6:0.4)ë¡œ ì•™ìƒë¸”í•˜ë‹ˆ 0.94ë¡œ í–¥ìƒëìŠµë‹ˆë‹¤."

Q: "ì™œ ë‘ ëª¨ë¸ì´ ìƒí˜¸ ë³´ì™„ì ì¸ê°€ìš”?"
> "XGBoostëŠ” ë‹¨ì¼ ê±°ë˜ì˜ ì •í˜• íŠ¹ì„±ì„ ì¡ê³ , LSTMì€ ê±°ë˜ ì‹œí€€ìŠ¤ì˜ íŒ¨í„´ì„ ì¡ìŠµë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì‚¬ê¸°ë¥¼ íƒì§€í•˜ë¯€ë¡œ ì•™ìƒë¸” íš¨ê³¼ê°€ í½ë‹ˆë‹¤."

---

## 1-6: SHAP ì„¤ëª… (Day 6)

### í•„ìš” íŒ¨í‚¤ì§€
```python
import shap
import torch
import numpy as np
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. XGBoost ì„¤ëª…: TreeExplainer**
- ë¹ ë¥´ê³  ì •í™•
- ì „ì²´ í”¼ì²˜ ì¤‘ìš”ë„
- ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…

**2. LSTM ì„¤ëª…: DeepExplainer**
- ë”¥ëŸ¬ë‹ ëª¨ë¸ìš©
- Background ë°ì´í„° í•„ìš”
- ì‹œí€€ìŠ¤ í”¼ì²˜ ê¸°ì—¬ë„

**3. ì•™ìƒë¸” ì„¤ëª… í†µí•©**
- ê°€ì¤‘ì¹˜ë¡œ SHAP ê°’ í•©ì‚°
- ì •í˜• + ì‹œê³„ì—´ í”¼ì²˜ í†µí•©
- Top K í”¼ì²˜ ì¶”ì¶œ

**4. ìì—°ì–´ ì„¤ëª… ìƒì„±**
- í”¼ì²˜ëª… â†’ ì„¤ëª… ë§¤í•‘
- ë°©í–¥ (ì¦ê°€/ê°ì†Œ) í¬í•¨

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: XGBoost SHAP ê³„ì‚°
- ì‹¤ìŠµ 2: SHAP Summary Plot
- ì‹¤ìŠµ 3: LSTM SHAP ê³„ì‚° (DeepExplainer)
- ì‹¤ìŠµ 4: ì•™ìƒë¸” SHAP í†µí•©
- ì‹¤ìŠµ 5: ìì—°ì–´ ì„¤ëª… ìƒì„±

### í•µì‹¬ ì½”ë“œ: XGBoost SHAP

```python
# TreeExplainer
explainer_xgb = shap.TreeExplainer(xgb_model)
shap_values_xgb = explainer_xgb.shap_values(X_test_tabular)

# Summary Plot
shap.summary_plot(shap_values_xgb, X_test_tabular, max_display=10)
```

### í•µì‹¬ ì½”ë“œ: LSTM SHAP

```python
# DeepExplainer (ë°°ê²½ ë°ì´í„° í•„ìš”)
background = X_train_seq[:100]  # ë°°ê²½ ìƒ˜í”Œ
explainer_lstm = shap.DeepExplainer(lstm_model, torch.FloatTensor(background))
shap_values_lstm = explainer_lstm.shap_values(torch.FloatTensor(X_test_seq[:10]))
```

### í•µì‹¬ ì½”ë“œ: í†µí•© ì„¤ëª…

```python
def get_ensemble_explanation(shap_xgb, shap_lstm, feature_names_xgb, feature_names_lstm,
                              w_xgb=0.6, top_k=5):
    """ì•™ìƒë¸” SHAP ì„¤ëª… ìƒì„±"""
    # XGBoost Top í”¼ì²˜
    xgb_importance = np.abs(shap_xgb).mean(axis=0)
    xgb_top_idx = np.argsort(xgb_importance)[-top_k:][::-1]

    # LSTM ì‹œí€€ìŠ¤ ì˜í–¥ë„ (í‰ê· )
    lstm_impact = np.abs(shap_lstm).mean()

    explanation = {
        'tabular_features': [
            {
                'feature': feature_names_xgb[i],
                'importance': xgb_importance[i] * w_xgb,
                'direction': 'increase' if shap_xgb[i] > 0 else 'decrease'
            }
            for i in xgb_top_idx
        ],
        'sequence_impact': lstm_impact * (1 - w_xgb)
    }
    return explanation

# ìì—°ì–´ ë³€í™˜
FEATURE_DESC = {
    'TransactionAmt': 'ê±°ë˜ ê¸ˆì•¡',
    'hour': 'ê±°ë˜ ì‹œê°„',
    'card1_fraud_rate': 'ì¹´ë“œ ì‚¬ê¸° ì´ë ¥',
}

def to_natural_language(explanation):
    lines = ["[ì‚¬ê¸° íŒë‹¨ ê·¼ê±°]"]
    for f in explanation['tabular_features'][:3]:
        name = FEATURE_DESC.get(f['feature'], f['feature'])
        direction = "ë†’ìŒ" if f['direction'] == 'increase' else "ë‚®ìŒ"
        lines.append(f"- {name}: ì‚¬ê¸° í™•ë¥  {direction}")

    if explanation['sequence_impact'] > 0.1:
        lines.append("- ìµœê·¼ ê±°ë˜ íŒ¨í„´: ë¹„ì •ìƒ ê°ì§€")

    return "\n".join(lines)
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ SHAPì„ ì„ íƒí–ˆë‚˜ìš”?"
> "SHAPì€ ì´ë¡ ì  ê¸°ë°˜(Shapley Value)ì´ íƒ„íƒ„í•˜ê³ , TreeExplainerì™€ DeepExplainerë¡œ XGBoostì™€ LSTM ëª¨ë‘ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

Q: "ì•™ìƒë¸” ëª¨ë¸ì€ ì–´ë–»ê²Œ ì„¤ëª…í•˜ë‚˜ìš”?"
> "ê° ëª¨ë¸ì˜ SHAP ê°’ì„ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ë¡œ í•©ì¹©ë‹ˆë‹¤. ì •í˜• í”¼ì²˜ì™€ ì‹œê³„ì—´ íŒ¨í„´ ëª¨ë‘ í¬í•¨ëœ í†µí•© ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤."

---

## 1-7: FastAPI ë°°í¬ (Day 7)

### í•„ìš” íŒ¨í‚¤ì§€
```python
from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import torch
import uvicorn
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. API ì—”ë“œí¬ì¸íŠ¸**
- GET /health: í—¬ìŠ¤ì²´í¬
- POST /predict: ì‚¬ê¸° ì˜ˆì¸¡ + ì„¤ëª…

**2. Request/Response ìŠ¤í‚¤ë§ˆ**
- Pydantic BaseModel
- íƒ€ì… ê²€ì¦

**3. ëª¨ë¸ ë¡œë”©**
- ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë”©
- XGBoost + LSTM + SHAP Explainer

**4. Docker ì»¨í…Œì´ë„ˆí™”**
- Dockerfile ì‘ì„±
- docker-compose.yml
- ë‹¨ì¼ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰

**5. ì„±ëŠ¥ ìµœì í™”**
- ëª¨ë¸ ìºì‹±
- ì‘ë‹µ ì‹œê°„ < 200ms

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜
- ì‹¤ìŠµ 2: /health ì—”ë“œí¬ì¸íŠ¸
- ì‹¤ìŠµ 3: /predict ì—”ë“œí¬ì¸íŠ¸
- ì‹¤ìŠµ 4: Dockerfile ì‘ì„±
- ì‹¤ìŠµ 5: docker-compose ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

### í•µì‹¬ ì½”ë“œ: FastAPI

```python
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Dict
import joblib
import torch

app = FastAPI(title="FDS API", version="1.0")

# ëª¨ë¸ ë¡œë”© (ì‹œì‘ ì‹œ í•œ ë²ˆ)
xgb_model = joblib.load('models/xgb_model.pkl')
lstm_model = torch.load('models/lstm_model.pt')
lstm_model.eval()

class Transaction(BaseModel):
    transaction_id: int
    amount: float
    hour: int
    card1: int
    # ... ê¸°íƒ€ í”¼ì²˜
    recent_transactions: List[Dict]  # ìµœê·¼ ê±°ë˜ ì‹œí€€ìŠ¤

class PredictionResponse(BaseModel):
    fraud_probability: float
    model_scores: Dict[str, float]
    top_factors: List[Dict]
    is_fraud: bool

@app.get("/health")
def health():
    return {"status": "healthy"}

@app.post("/predict", response_model=PredictionResponse)
def predict(transaction: Transaction):
    # 1. Feature Engineering
    X_tabular = extract_tabular_features(transaction)
    X_sequence = extract_sequence_features(transaction.recent_transactions)

    # 2. ëª¨ë¸ ì˜ˆì¸¡
    p_xgb = xgb_model.predict_proba(X_tabular)[0, 1]
    with torch.no_grad():
        p_lstm = lstm_model(torch.FloatTensor(X_sequence).unsqueeze(0)).item()

    # 3. ì•™ìƒë¸”
    p_ensemble = 0.6 * p_xgb + 0.4 * p_lstm

    # 4. SHAP ì„¤ëª…
    explanation = generate_explanation(X_tabular, X_sequence)

    return PredictionResponse(
        fraud_probability=p_ensemble,
        model_scores={"xgboost": p_xgb, "lstm": p_lstm, "ensemble": p_ensemble},
        top_factors=explanation,
        is_fraud=p_ensemble > 0.35
    )
```

### í•µì‹¬ ì½”ë“œ: Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY src/ ./src/
COPY models/ ./models/

CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### í•µì‹¬ ì½”ë“œ: docker-compose.yml

```yaml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
```

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "API ì‘ë‹µ ì‹œê°„ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?"
> "XGBoost ~10ms, LSTM ~50ms, SHAP ~100msë¡œ ì´ ì•½ 160msì…ë‹ˆë‹¤. ëª©í‘œì˜€ë˜ 200ms ì´í•˜ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤."

Q: "ëª¨ë¸ ì—…ë°ì´íŠ¸ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
> "í˜„ì¬ëŠ” Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ ë°©ì‹ì…ë‹ˆë‹¤. Phase 2ì—ì„œ MLflowë¡œ ëª¨ë¸ ë²„ì „ ê´€ë¦¬ë¥¼ ì¶”ê°€í•˜ë©´ ë¬´ì¤‘ë‹¨ ë°°í¬ê°€ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤."

---

## 1-8: Fusion ì‹¤í—˜ (Day 8) â­â­

### í•„ìš” íŒ¨í‚¤ì§€
```python
import torch
import torch.nn as nn
import numpy as np
from sklearn.metrics import roc_auc_score
```

### ì„¸ë¶€ ì„¤ëª… ë¦¬ìŠ¤íŠ¸

**1. ì•™ìƒë¸” vs ìœµí•©**

| ë°©ì‹ | í•™ìŠµ | ê²°í•© | ì˜ˆì‹œ |
|------|------|------|------|
| ì•™ìƒë¸” | ë…ë¦½ | ì˜ˆì¸¡ í›„ í‰ê·  | 0.6*XGB + 0.4*LSTM |
| ìœµí•© | ìˆœì°¨ | í•œ ëª¨ë¸ ì¶œë ¥ â†’ ë‹¤ë¥¸ ì…ë ¥ | XGB í™•ë¥  â†’ LSTM í”¼ì²˜ |

**2. 2024 ë…¼ë¬¸ ê¸°ë°˜ ìœµí•© ë°©ë²•**

| ë°©ë²• | í•µì‹¬ | êµ¬í˜„ ë³µì¡ë„ |
|------|------|-------------|
| B: XGBoostâ†’LSTM | XGB í™•ë¥ ì„ LSTM ì…ë ¥ì— ì¶”ê°€ | ë‚®ìŒ â­ |
| C: CNN-LSTM | 1D CNNìœ¼ë¡œ í”¼ì²˜ ì¶”ì¶œ â†’ LSTM | ì¤‘ê°„ |
| D: LSTM AE + XGBoost | LSTM AutoEncoder ì ì¬ ë²¡í„° â†’ XGB í”¼ì²˜ | ë†’ìŒ |

**3. ì‹¤í—˜ ìˆœì„œ**
1. ë°©ë²• B êµ¬í˜„ â†’ ì„±ëŠ¥ ì¸¡ì •
2. ë°©ë²• C êµ¬í˜„ â†’ ì„±ëŠ¥ ì¸¡ì •
3. ë°©ë²• D êµ¬í˜„ â†’ ì„±ëŠ¥ ì¸¡ì •
4. ìµœê³  ì„±ëŠ¥ ë°©ë²• ì±„íƒ

### ì‹¤ìŠµ ëª©ë¡
- ì‹¤ìŠµ 1: ë°©ë²• B - XGBoostâ†’LSTM ìœµí•©
- ì‹¤ìŠµ 2: ë°©ë²• C - CNN-LSTM êµ¬í˜„
- ì‹¤ìŠµ 3: ë°©ë²• D - LSTM AutoEncoder + XGBoost
- ì‹¤ìŠµ 4: 3ê°€ì§€ ë°©ë²• ì„±ëŠ¥ ë¹„êµ
- ì‹¤ìŠµ 5: ìµœì¢… ëª¨ë¸ ì„ íƒ ë° ì €ì¥

### í•µì‹¬ ì½”ë“œ: ë°©ë²• B (XGBoostâ†’LSTM ìœµí•©)

```python
# 1. XGBoost í™•ë¥  ê³„ì‚°
xgb_prob = xgb_model.predict_proba(X_tabular)[:, 1]

# 2. LSTM ì‹œí€€ìŠ¤ì— XGBoost í™•ë¥  ì¶”ê°€
# X_seq shape: (samples, seq_len, features)
# xgb_probì„ ëª¨ë“  íƒ€ì„ìŠ¤í…ì— broadcast
xgb_prob_expanded = np.tile(
    xgb_prob.reshape(-1, 1, 1),
    (1, X_seq.shape[1], 1)
)
X_seq_fusion = np.concatenate([X_seq, xgb_prob_expanded], axis=2)

# 3. ìœµí•© LSTM í•™ìŠµ
fusion_model = FraudLSTM(input_size=X_seq_fusion.shape[2])
# ... í•™ìŠµ ì§„í–‰
```

### í•µì‹¬ ì½”ë“œ: ë°©ë²• C (CNN-LSTM)

```python
class CNNLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64):
        super().__init__()
        # 1D CNN for feature extraction
        self.conv1 = nn.Conv1d(input_size, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool1d(2)

        # LSTM for sequence learning
        self.lstm = nn.LSTM(64, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        # x: (batch, seq_len, features) â†’ (batch, features, seq_len)
        x = x.permute(0, 2, 1)

        # CNN
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = self.pool(x)

        # (batch, features, seq_len) â†’ (batch, seq_len, features)
        x = x.permute(0, 2, 1)

        # LSTM
        _, (h_n, _) = self.lstm(x)
        out = self.fc(h_n[-1])
        return out
```

### í•µì‹¬ ì½”ë“œ: ë°©ë²• D (LSTM AE + XGBoost)

```python
class LSTMAutoEncoder(nn.Module):
    def __init__(self, input_size, hidden_size=32, latent_size=16):
        super().__init__()
        # Encoder
        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc_latent = nn.Linear(hidden_size, latent_size)

        # Decoder
        self.fc_decode = nn.Linear(latent_size, hidden_size)
        self.decoder = nn.LSTM(hidden_size, input_size, batch_first=True)

    def encode(self, x):
        _, (h_n, _) = self.encoder(x)
        latent = self.fc_latent(h_n[-1])
        return latent

    def forward(self, x):
        latent = self.encode(x)
        # ... decoder ìƒëµ

# í•™ìŠµ í›„ latent vector ì¶”ì¶œ â†’ XGBoost í”¼ì²˜ë¡œ ì‚¬ìš©
latent_features = lstm_ae.encode(X_seq_tensor).detach().numpy()
X_tabular_with_latent = np.concatenate([X_tabular, latent_features], axis=1)
xgb_model.fit(X_tabular_with_latent, y)
```

### ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ (ì˜ˆìƒ)

| ë°©ë²• | AUC | êµ¬í˜„ ë³µì¡ë„ |
|------|-----|-------------|
| ë‹¨ìˆœ ì•™ìƒë¸” (1-5) | 0.89 | ë‚®ìŒ |
| B: XGBoostâ†’LSTM | 0.92+ | ë‚®ìŒ â­ |
| C: CNN-LSTM | 0.93+ | ì¤‘ê°„ |
| D: LSTM AE + XGB | 0.91+ | ë†’ìŒ |

### ë©´ì ‘ í¬ì¸íŠ¸

Q: "ì™œ ë‹¨ìˆœ ì•™ìƒë¸” ëŒ€ì‹  ìœµí•©ì„ ì‚¬ìš©í–ˆë‚˜ìš”?"
> "Weighted Average ì•™ìƒë¸”ë¡œëŠ” AUC 0.89ì—ì„œ ì •ì²´ëìŠµë‹ˆë‹¤. 2024ë…„ ë…¼ë¬¸ì„ ì¡°ì‚¬í•´ì„œ XGBoost ì¶œë ¥ì„ LSTM ì…ë ¥ìœ¼ë¡œ í™œìš©í•˜ëŠ” ìœµí•© ë°©ì‹ì„ ë°œê²¬í–ˆê³ , ì´ë¥¼ ì ìš©í•´ AUC 0.92 ì´ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤."

Q: "3ê°€ì§€ ìœµí•© ë°©ë²• ì¤‘ ì–´ë–¤ ê²ƒì„ ì„ íƒí–ˆë‚˜ìš”?"
> "ë°©ë²• B(XGBoostâ†’LSTM)ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤. êµ¬í˜„ì´ ê°„ë‹¨í•˜ë©´ì„œë„ AUC 0.92ë¥¼ ë‹¬ì„±í•´ì„œ ì„±ëŠ¥ ëŒ€ë¹„ ë³µì¡ë„ê°€ ê°€ì¥ ì¢‹ì•˜ìŠµë‹ˆë‹¤. CNN-LSTMì€ ì¡°ê¸ˆ ë” ë†’ì•˜ì§€ë§Œ ë³µì¡ë„ ì¦ê°€ ëŒ€ë¹„ íš¨ê³¼ê°€ ë¯¸ë¯¸í–ˆìŠµë‹ˆë‹¤."

---

## ì „ì²´ ìš”ì•½

| ë…¸íŠ¸ë¶ | ì‹œê°„ | í•µì‹¬ ì‚°ì¶œë¬¼ |
|--------|------|------------|
| 1-1 | 3h | train.csv, test.csv |
| 1-2 | 4h | preprocessing.py, X_tabular, X_sequence |
| 1-3 | 4h | ëª¨ë¸ ë¹„êµ í‘œ, xgb_model.pkl, xgb_importance.csv |
| 1-4 | 4h | src/models/lstm.py, lstm_model.pt |
| 1-5 | 3h | ì•™ìƒë¸” ì„±ëŠ¥ ë¹„êµ í‘œ (ëª©í‘œ ë¯¸ë‹¬ â†’ 1-8) |
| 1-6 | 3h | shap_explainer.py, ì„¤ëª… ì‹œê°í™” |
| 1-7 | 4h | FastAPI, Docker, í†µí•© í…ŒìŠ¤íŠ¸ |
| 1-8 | 4h | ìœµí•© ë°©ë²• ë¹„êµ, fusion_model.pt â­ |

**ì´ ì•½ 29ì‹œê°„ (8ì¼)**

---

## í•µì‹¬ ì‹¤í—˜ ê²°ê³¼ (ë©´ì ‘ìš©)

### 1. ëª¨ë¸ ë¹„êµ (1-3)

| Model | AUC | Time(s) | SHAP |
|-------|-----|---------|------|
| XGBoost | 0.92 | 45 | âœ… ìµœìƒ |
| LightGBM | 0.91 | 32 | âœ… ì¢‹ìŒ |
| CatBoost | 0.91 | 98 | âš ï¸ ì œí•œ |

### 2. LSTM ë‹¨ê³„ë³„ ê°œì„  (1-4)

| ë‹¨ê³„ | êµ¬ì„± | AUC |
|------|------|-----|
| ë² ì´ìŠ¤ë¼ì¸ | V1~V20 + ì‹œê³„ì—´ í”¼ì²˜ (35ê°œ) | 0.82 |
| + Optuna íŠœë‹ | í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” | 0.84 |
| + í”¼ì²˜ ì¶”ê°€ | XGBoost importance ê¸°ë°˜ | 0.86 |

### 3. ì•™ìƒë¸” ì„±ëŠ¥ (1-5)

| Model | AUC | Recall@0.35 |
|-------|-----|-------------|
| XGBoost | 0.92 | 0.83 |
| LSTM íŠœë‹ í›„ | 0.86 | 0.78 |
| **Ensemble** | **0.89** | **0.84** |

âš ï¸ ëª©í‘œ AUC 0.90 ë¯¸ë‹¬ â†’ 1-8 ìœµí•©ìœ¼ë¡œ í•´ê²°

### 4. ìœµí•© ë°©ë²• ë¹„êµ (1-8) â­

| ë°©ë²• | AUC | ë³µì¡ë„ |
|------|-----|--------|
| ë‹¨ìˆœ ì•™ìƒë¸” | 0.89 | ë‚®ìŒ |
| B: XGBoostâ†’LSTM | 0.92+ | ë‚®ìŒ â­ |
| C: CNN-LSTM | 0.93+ | ì¤‘ê°„ |
| D: LSTM AE + XGB | 0.91+ | ë†’ìŒ |

**ì´ í‘œë“¤ì´ ë©´ì ‘ì—ì„œ "ì™œ ì´ê±¸ ì„ íƒí–ˆë‚˜ìš”?"ì— ëŒ€í•œ ê·¼ê±°!**

**í•µì‹¬ ìŠ¤í† ë¦¬:**
> "ë‹¨ìˆœ ì•™ìƒë¸”ë¡œëŠ” ëª©í‘œ ì„±ëŠ¥ì— ë¯¸ë‹¬ â†’ 2024ë…„ ë…¼ë¬¸ ì¡°ì‚¬ â†’ ìœµí•© ë°©ì‹ ë°œê²¬ â†’ ì ìš©í•´ì„œ í•´ê²°"

---

## ë‹¤ìŒ ë‹¨ê³„: Phase 2

> ìƒì„¸: [docs/roadmap.md](./roadmap.md)

Phase 1 ì™„ë£Œ í›„ Phase 2ì—ì„œ ì¶”ê°€:
- **MLflow**: ì‹¤í—˜ ì¶”ì , ëª¨ë¸ ë²„ì „ ê´€ë¦¬
- **Evidently**: ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§
- **GitHub Actions**: CI/CD
- **ë¹„ìš© ê¸°ë°˜ ìµœì í™”**: ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ê³„ì‚°
